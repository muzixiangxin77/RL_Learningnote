# 计算机视觉——图像分类（一）

本文的内容主要分为两部分：

1.介绍图像分类中常用的评估的数据集及处理方式

2.介绍近几年先进的16种图像分类模型及其原理剖析

## 第一部分：

本节主要介绍在有限计算资源下，能够找到用于分类的数据集：

**1.Kagglecatsanddogs数据集**：这是一个二分类狗和猫的数据集，下载方式如下：

```
!curl -O https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip

!unzip -q kagglecatsanddogs_5340.zip
```

这时咱们的当前目录下呈现的是如下情况：

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-10-31-15-08-38-image.png" alt="" width="294" data-align="center">

这里我们需要注意到在处理大量真实世界的图像数据时，损坏的图像经常发生，所以我们为了确保不对后续的模型造成影响，通常需要以下几个步骤：

①删除损坏/无法读取的图像

②删除或者处理重复的图像

③过滤掉不合适的图像（不符合要求的尺寸，格式，内容等）

④格式转换或者调整图像尺寸

⑤数据增强

```
#1.损坏图像检测
import os
from PIL import Image
import numpy as np

def check_image_integrity(image_path):
    """全面检查图像完整性"""
    try:
        with Image.open(image_path) as img:
            # 验证图像能正常加载和解析
            img.verify()

            # 检查图像模式是否有效
            valid_modes = ['1', 'L', 'P', 'RGB', 'RGBA', 'CMYK', 'YCbCr', 'LAB', 'HSV', 'I', 'F']
            if img.mode not in valid_modes:
                return False, f"Invalid mode: {img.mode}"

            # 检查图像尺寸是否合理
            if img.size[0] == 0 or img.size[1] == 0:
                return False, "Zero dimension image"

            # 检查文件大小是否合理
            file_size = os.path.getsize(image_path)
            if file_size < 1024:  # 小于1KB可能是损坏文件
                return False, "File too small"

            return True, "Valid image"

    except Exception as e:
        return False, f"Corrupted: {str(e)}"

def filter_corrupted_images(data_dir, output_log=None):
    """过滤所有损坏的图像"""
    corrupted_count = 0
    log_entries = []

    for root, dirs, files in os.walk(data_dir):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                file_path = os.path.join(root, file)
                is_valid, message = check_image_integrity(file_path)

                if not is_valid:
                    corrupted_count += 1
                    log_entry = f"DELETE: {file_path} - {message}"
                    log_entries.append(log_entry)
                    print(log_entry)

                    try:
                        os.remove(file_path)
                    except Exception as e:
                        print(f"Failed to delete {file_path}: {e}")

    if output_log and log_entries:
        with open(output_log, 'w') as f:
            f.write('\n'.join(log_entries))

    print(f"Deleted {corrupted_count} corrupted images")
    return corrupted_count
```

```
#2.图像质量评估和过滤
import cv2
from skimage import metrics
from PIL import Image, ImageFilter

def calculate_image_quality(image_path):
    """计算图像质量指标"""
    try:
        # 使用OpenCV读取图像
        img = cv2.imread(image_path)
        if img is None:
            return 0, 0, 0

        # 1. 模糊度检测 (Laplacian方差)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        blur_value = cv2.Laplacian(gray, cv2.CV_64F).var()

        # 2. 亮度检测
        brightness = np.mean(gray)

        # 3. 对比度检测
        contrast = gray.std()

        return blur_value, brightness, contrast

    except Exception as e:
        return 0, 0, 0

def filter_low_quality_images(data_dir, 
                             min_blur=100, 
                             min_brightness=30, 
                             max_brightness=220,
                             min_contrast=25):
    """过滤低质量图像（模糊、过暗、过亮、低对比度）"""
    low_quality_count = 0

    for root, dirs, files in os.walk(data_dir):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                file_path = os.path.join(root, file)
                blur, brightness, contrast = calculate_image_quality(file_path)

                # 应用质量阈值
                if (blur < min_blur or 
                    brightness < min_brightness or 
                    brightness > max_brightness or
                    contrast < min_contrast):

                    low_quality_count += 1
                    print(f"LOW QUALITY: {file_path} - "
                          f"Blur: {blur:.1f}, Brightness: {brightness:.1f}, "
                          f"Contrast: {contrast:.1f}")

                    try:
                        os.remove(file_path)
                    except Exception as e:
                        print(f"Failed to delete {file_path}: {e}")

    print(f"Deleted {low_quality_count} low quality images")
    return low_quality_count
```

```
#3.重复图像检测
import hashlib
from collections import defaultdict

def calculate_image_hash(image_path, hash_size=8):
    """计算图像感知哈希"""
    try:
        with Image.open(image_path) as img:
            # 转换为灰度图并调整大小
            img = img.convert('L').resize((hash_size, hash_size), Image.LANCZOS)

            # 计算像素平均值
            pixels = np.array(img)
            avg = pixels.mean()

            # 生成哈希
            hash_str = ''.join('1' if pixel > avg else '0' for pixel in pixels.flatten())
            return hash_str

    except Exception:
        return None

def find_duplicate_images(data_dir, similarity_threshold=0.95):
    """查找并删除重复/高度相似图像"""
    image_hashes = defaultdict(list)
    duplicate_count = 0

    # 第一阶段：收集所有图像哈希
    for root, dirs, files in os.walk(data_dir):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                file_path = os.path.join(root, file)
                image_hash = calculate_image_hash(file_path)

                if image_hash:
                    image_hashes[image_hash].append(file_path)

    # 第二阶段：删除重复图像（保留每个哈希组中的一个）
    for hash_val, file_list in image_hashes.items():
        if len(file_list) > 1:
            # 保留第一个，删除其余的
            for duplicate_file in file_list[1:]:
                duplicate_count += 1
                print(f"DUPLICATE: {duplicate_file}")
                try:
                    os.remove(duplicate_file)
                except Exception as e:
                    print(f"Failed to delete {duplicate_file}: {e}")

    print(f"Deleted {duplicate_count} duplicate images")
    return duplicate_count
```

```
#4.图像尺寸和比例过滤
def filter_by_size_and_ratio(data_dir, 
                           min_width=, 
                           min_height=,
                           max_width=, 
                           max_height=,
                           min_aspect_ratio=,
                           max_aspect_ratio=):
    """根据尺寸和宽高比过滤图像"""
    invalid_size_count = 0

    for root, dirs, files in os.walk(data_dir):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                file_path = os.path.join(root, file)

                try:
                    with Image.open(file_path) as img:
                        width, height = img.size
                        aspect_ratio = width / height

                        # 检查尺寸和比例是否在合理范围内
                        if (width < min_width or height < min_height or
                            width > max_width or height > max_height or
                            aspect_ratio < min_aspect_ratio or 
                            aspect_ratio > max_aspect_ratio):

                            invalid_size_count += 1
                            print(f"INVALID SIZE: {file_path} - "
                                  f"Size: {width}x{height}, Aspect: {aspect_ratio:.2f}")

                            os.remove(file_path)

                except Exception as e:
                    print(f"Error processing {file_path}: {e}")

    print(f"Deleted {invalid_size_count} invalid size images")
    return invalid_size_count
```

```
def comprehensive_dataset_cleanup(data_dir, 
                                min_blur=,
                                min_brightness=,
                                max_brightness=,
                                min_contrast=,
                                min_size=):
    """完整的数据集清理流程"""

    print("Starting comprehensive dataset cleanup...")

    # 1. 过滤损坏图像
    corrupted_count = filter_corrupted_images(data_dir)

    # 2. 过滤低质量图像
    low_quality_count = filter_low_quality_images(
        data_dir, 
        min_blur=min_blur,
        min_brightness=min_brightness,
        max_brightness=max_brightness,
        min_contrast=min_contrast
    )

    # 3. 删除重复图像
    duplicate_count = find_duplicate_images(data_dir)

    # 4. 过滤不合理尺寸
    size_invalid_count = filter_by_size_and_ratio(data_dir, min_width=min_size, min_height=min_size)

    print(f"\n=== Cleanup Summary ===")
    print(f"Corrupted images removed: {corrupted_count}")
    print(f"Low quality images removed: {low_quality_count}")
    print(f"Duplicate images removed: {duplicate_count}")
    print(f"Invalid size images removed: {size_invalid_count}")

    return {
        'corrupted': corrupted_count,
        'low_quality': low_quality_count,
        'duplicate': duplicate_count,
        'invalid_size': size_invalid_count
    }


if __name__ == "__main__":
    data_directory = "PetImages"  # 数据目录

    results = comprehensive_dataset_cleanup(
        data_directory,
        min_blur=,           # 最小模糊度阈值
        min_brightness=,     # 最小亮度
        max_brightness=,    # 最大亮度
        min_contrast=,       # 最小对比度
        min_size=           # 最小尺寸
    )
```

注：**<u>以上处理代码需要具体问题具体修改，只当作处理方式参考</u>**。其实基本上我们很少用到，因为市面上的图像数据集大部分都是很好的，不需要我们这么麻烦地后期处理。

接下来处理完之后，就是生成数据集，为模型训练做准备了：

```
image_size = (180, 180)
batch_size = 128

train_ds, val_ds = keras.utils.image_dataset_from_directory(
    "PetImages",
    validation_split=0.2,
    subset="both",
    seed=1337,
    image_size=image_size,
    batch_size=batch_size,
)
```

注：这里image_size是根据原本数据集中图片的大小设定的，所以当我们拿到的图片不是同一尺寸时，需要在上述操作前处理成同一大小像素的。也可以往下看第三个数据集中resize函数的应用。

**2.MNIST数据集**：手写数据集，共60000张左右图片28*28，10分类，直接在keras.datasets中直接加载

```
# Load the data and split it between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
```

这里也可以**加载cifar100,cifar10（32*32，50000训练和10000测试）数据集**

```
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data() 
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() 
```

**3.StanfordDogs数据集**：20580张图片，120类狗的品种，也是可以直接加载：

```
dataset_name = "stanford_dogs"/"cifar10"/"cifar100"/"tf_flowers"/"food101"

(ds_train, ds_test), ds_info = tfds.load(
    dataset_name, split=["train", "test"], with_info=True, as_supervised=True
)
NUM_CLASSES = ds_info.features["label"].num_classes


#Determined by specific model choice
size = (IMG_SIZE, IMG_SIZE)
ds_train = ds_train.map(lambda image, label: (tf.image.resize(image, size), label))
ds_test = ds_test.map(lambda image, label: (tf.image.resize(image, size), label))
```

这里dataset_name可以**替换为cifar10,cifar100,food101,tf_flowers等等其他数据集**

## 第二部分：

本节是笔者整理阅读的近几年比较先进的图像分类模型：

### 1.SENet

<img src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-10-31-16-29-20-image.png" title="" alt="" data-align="center">

**论文链接：Squeeze-and-Excitation Networks** https://www.kdocs.cn/l/ccOgClwaiPDK

**1.1 $F_{tr}$ 本质上是一个卷积操作**，用C个卷积核对图像进行特征提取操作:

$$
\begin{align}
U=[u_1,u_2,...,u_C]\in R^{H\times W \times C}\\
u_c=v_c*X=\sum^{C'}_{s=1}v^s_c*x^s \in R^{H \times W}\\
v_c=[v^1_c,v^2_c,...,v^{C'}_c],v^s_c \ is \ a \ 2D\  spatial\  kernel \\
X=[x^1,x^2,...,x^{C'}]\in R^{H'\times W' \times C'}
\end{align}
$$

**1.2 $F_{sq}$ 本质上是将每个通道**的全局空间信息压缩为一个通道描述符：

$$
z_c=F_{sq}(u_c)=\frac{1}{H\cdot W}\sum_{i=1}^{H}\sum_{j=1}^{W}u_c(i,j)
$$

**1.3$F_{ex}$ 本质上是学习通道间的依赖关系**，生成每个通道的权重：

$$
s=F_{ex}(z,W)=\sigma(g(z,W))=\sigma(W_2\delta(W_1z))
$$

这里$\sigma$代表Sigmod函数，$\delta$代表ReLU函数。先通过$W_1z$降维(Bottleneck)，减少参数量和计算复杂度，再经过非线性变换引入非线性增强模型表达能力，然后$W_2\delta(W_1z)$恢复到原来维度，最后激活函数，得到每个通道的重要性权重。

**1.4$F_{scale}$本质上是将学习到的通**道权重应用到原始特征图上

$$
\tilde{x_c}=F_{scale}(u_c,s_c)=s_cu_c\\

\tilde{X}=[\tilde{x_1},\tilde{x_2},...,\tilde{x_C}]
$$

### 2.MobileNet

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-10-31-18-34-08-image.png" alt="" width="246" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-10-31-19-09-01-image.png" alt="" data-align="center" width="411">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-10-31-18-51-46-image.png" alt="" width="273" data-align="center">

**论文链接：MobileNets- Efficient Convolutional Neural Networks for Mobile Vision Applications**
 https://www.kdocs.cn/l/cpheYfgPHnzj

本质上这个网络是为了部署到移动端而设置的**轻量级网络架构**，运用的是深度可分离卷积结构：

设input feature map $F$为$D_F\times D_F\times M$,output feature map $G$为$D_F\times D_F \times N$，即有N个卷积核

**2.1.Standard Convolution计算量:**

首先标准的卷积核大小参数量应该为$D_K\times D_K\times M\times N$,

$$
G_{k,l,n}=\sum_{i,j,m}K_{i,j,m,n}\cdot F_{k+i-1,l+j-1,m}
$$

所以总计算量为

$$
D_K\cdot D_K \cdot M \cdot N \cdot D_F \cdot D_F
$$

**2.2.Depthwise Convolution计算量：**

首先是通过M个深度卷积核$D_K*D_K*1$，然后通过N个$1*1*M$的卷积核，故总计算量为：

$$
D_K\cdot D_K \cdot M \cdot D_F \cdot D_F+ \cdot M \cdot N \cdot D_F \cdot D_F
$$

**2.3.比较两者计算量：**

$$
\frac{D_K\cdot D_K \cdot M \cdot D_F \cdot D_F+ \cdot M \cdot N \cdot D_F \cdot D_F}{D_K\cdot D_K \cdot M \cdot N \cdot D_F \cdot D_F}=\frac{1}{N}+\frac{1}{D^2_K}
$$

足以可见计算量大大缩短，效率提升

### 3.EfficientNet

**论文链接：EfficientNet重新思考卷积神经网络的模型缩放** https://www.kdocs.cn/l/ccsQiwnDOCR4

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-10-31-19-13-06-image.png" alt="" width="494" data-align="center">

**论文核心内容：**

在EfficientNet之前，提升模型性能通常有三种“笨方法”：1.增加深度，让网络层数更深；2.增加宽度，让每一层通道数更多3.增加输入图像分辨率，使用更清晰的图片训练推理。而论文提出的核心思想是，它认为**深度，宽度，分辨率三者相互依赖**的，应该按照一个统一的，科学的比例共同放大，即**复合模型缩放**(Compound coefficient)。

对于一个卷积网络 N，我们可以将其视为一个由多个层 $F_i$组成的函数：$N=F_k​⊙...⊙F_2​⊙F_1​(X)$

其中 ⊙ 表示层与层之间的连接。通常，这些层可以被分组到多个有相似结构的阶段（Stage）。EfficientNet发现，一个设计良好的基线网络（称之为**EfficientNet-B0**）可以被缩放，通过使用一个**复合系数 ϕ** 来统一缩放网络的深度、宽度和分辨率：

- **深度**：$d=\alpha^{\phi}$

- **宽度**：$w=\beta^{\phi}$

- **分辨率**：$r=\gamma^{\phi}$

约束条件：$\alpha\cdot \beta^2 \cdot \gamma^2 \approx{2}$
约束条件的意义：α,β,γ 分别是深度、宽度和分辨率的缩放系数。这个约束意味着，当深度翻倍（α=2）时，计算量大约翻倍；但当宽度或分辨率翻倍（β=2或γ=2）时，计算量会变为原来的四倍（因为特征图尺寸或通道数增大会导致后续卷积计算量平方级增长）。这个约束是为了确保总体的计算量（FLOPS）大约增加 $2^{\phi}$倍。

结论：EfficientNet告诉我们，你不能只疯狂地盖100层楼（只增加深度），而每层楼只有一个小房间（宽度不变），这样结构不稳定（梯度消失/爆炸）。你也不能只把每层楼弄得像迷宫一样布满房间（只增加宽度），而只有3层楼，这样信息流通效率低。你更不能只用巨大的砖块（只增加分辨率），而楼的结构本身很脆弱。

**论文中模型的架构**：

EfficientNetB0:高效的基线模型，如上图所示的结构，其中详细解释一下MBConv块:

3.1 $1\times 1$扩张卷积：输入一个特征图，先通过一个1*1普通卷积提升通道数（通常是输入通道的6倍，所以叫MBConv6）。

3.2 深度可分离卷积：引入MobileNet中架构

3.3 SEBlock：引入SENet中架构

3.4 $1\times 1$投影卷积：将通道维数降回原始输入通道数

3.5 残差连接结构：引入ResNet中架构

**思想迁移**：

如果我们手上有一个高效的基线模型，可以借鉴EfficientNet的复合缩放思想，在**深度**（在论文中是MBConv模块重复次数），**宽度**（所需通道数），**分辨率**（输入图像的分辨率）三个维度对模型进行放大，从而达到性能提升的效果。具体来说，我们可以先进行小网格搜索，在基线模型上固定其中两个维度，微调第三个维度，观察性能变化，找到最佳值，然后应用复合缩放，扩大模型大小。

### 4.ViT

论文链接：An Image is Worth 16x16 Words  Transformers for Image Recognition at Scale https://www.kdocs.cn/l/ctmyPNmRbg2X

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-09-21-56-image.png" alt="" data-align="center" width="555">

**4.1 图像分块嵌入（Patch Embedding）:将2D图像转换为1D序列**

①分块：将一张输入图像$X\in R^{H \times W \times C}$分割成N个固定大小的patches。每个块的大小为$P\times P$,因此总块数为$N=\frac{H\times W}{P^2}$。

②展平与投影：将每个块展平为一个向量$x_p\in R^{P^2\cdot C}$。然后，通过一个可训练的线性投影（即一个全连接层）将这个向量映射到模型的维度D。

$$
z_0=[x_{class};x_p^1E;x_p^2E;...;x_p^NE]+E_{pos}\\
$$

其中$E\in R^{P^2\cdot C \times D}$为投影矩阵，$E_{pos}\in R^{(N+1)\times D}$是位置编码（Transformer本身不具备序列顺序感知能力，所以为了保留图像块之间的空间位置信息，引入位置编码），$x_{class}$是一个可学习的分类令牌，这个令牌作用类似于CNN全局平均池化层后的那个特征向量。经过所有Transformer层后，这个class令牌最终输出状态被用作整个图像的表示。

**4.2 Transformer Encoder的堆叠**

$$
z_l=TransformerEncoder(z_{l-1}),l=1,2,...,L
$$

这里补充一下Transformer Encoder的结构（笔者之前的学习笔记中更详细）：

①多头自注意力层：这里又不得不说自注意力机制，给定一个序列$X=[x_1,x_2,...,x_n]$（每个$x_i$都是一个向量），自注意力通过三个可学习的权重矩阵$(W^Q,W^K,W^V)$,分别对应于查询(query，想要寻找什么)，键(key，能提供什么信息)，值(value，真正内容)，然后计算注意力权重和输出：

$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt d_k})V
$$

直观理解：对于序列中每一个元素，自注意力机制会看一遍序列中所有其他元素，然后决定从每个元素吸取多少信息，这就是与卷积网络不同之处，这是一个全局视野，而卷积网络只是一个局部感受视野。

而多头注意力机制，本质上是让模型能够同时关注来自不同表示子空间的信息：

$$
MultiHead(Q,K,V)=Concat(head_1,head_2,...,head_h)W^O\\
head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)
$$

每个头都有自己的权重矩阵$W_i^Q,W_i^K,W_i^V$，最后将各头输出拼接，通过一个线性层投影

②前馈神经网络：一个全连接层FFN

最后，整个Encoder层的计算流程可以表示为：(注意：这里有个残差连接+层归一化操作)

$$
Z'=LayerNorm(X+Multihead(X))\\
Z=LayerNorm(Z'+FFN(Z'))
$$

**4.3 MLP分类头**：取出最后一层$z_l^0$，即class的对应输出，通过一个多层感知机得到分类结果。

### 5.MLP-Mixer

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-09-53-34-image.png" alt="" data-align="center" width="554">

论文链接：MLP-Mixer An all-MLP Architecture for Vision https://www.kdocs.cn/l/claNRmt1htbQ

MLP-Mixer是一种完全基于多层感知机(MLP)的视觉架构，它不使用卷积或自注意力机制，而是通过两种类型的MLP层来处理图像特征。

**5.1给定输入图**像$X\in R^{H\times W \times C}$，首先将其分割为S个不重叠的patch:

$$
X=[X_1,X_2,...,X_S],X_i\in R^{P\times P\times C}
$$

每个patch被展平并通过一个线性投影：

$$
z^0_i=Linear(Flatten(X_i))+e_i^{pos}\\
Z^0=[z_1^0,z_2^0,...,z_S^0]^T\in R^{S\times D}
$$

**5.2 Token-mixing MLP**:在不同patch之间进行信息交互，设$Z_{l-1}$表示第l-1层的输出

①转置和层归一化：$U=LayerNorm(Z_{l-1})^T\in R^{D\times S}$

②逐通道的MLP处理：即对每个通道通过多层感知机，如图所示

③转置回原形状

④残差连接

**5.3 Channel-mixing MLP**:在通道维度进行信息交互

①层归一化

②逐位置的MLP处理：即对每个patch内部不同通道之间信息交互，如图所示

③残差连接

### 6.FNet

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-10-28-03-image.png" alt="" width="282" data-align="center">

论文链接：FNet Mixing Tokens with Fourier Transforms https://www.kdocs.cn/l/cjFclSO5WauU

FNet是一种基于傅里叶变换的混合模型，它用傅里叶变换替换了Transformer中的自注意力机制。FNet的核心思想是：通过**傅里叶变换在频率域中进行混合**，从而**捕获序列中元素之间的关系**。这种方法的计算效率比自注意力机制高，并且在某些任务上能够达到不错的性能。

这里首先补充一下**离散傅里叶变换DFT**：

**一维的离散傅里叶变换**：对于长度为N的序列x[n],其DFT为：

$$
X[k]=\sum^{N-1}_{n=0}x[n]\cdot e^{-j\frac{2\pi}{N}kn},k=0,1,...,N-1
$$

**二维的离散傅里叶变换**：对于矩阵$Z\in R^{S\times D}$,其2D-DFT为：

$$
F(Z)[k,l]=\sum_{m=0}^{S-1}\sum_{n=0}^{D-1}Z[m,n]\cdot e^{-j2\pi(\frac{km}{S}+\frac{ln}{D})}
$$

**6.1FNet傅里叶混合子层**：

①类似于MLP-Mixer的第一步得到X

②对输入X进行二维离散傅里叶变换：具体计算过程是先沿序列维度（行）进行一维DFT，再沿隐藏维度（列）进行一维DFT

③取傅里叶变换结果的实部(real part)

④残差连接

**6.2前馈网络子层**：与Transformer中前馈网络相同，由两个线性变换和一个激活函数组成，并带有残差连接，参照上述流程图。

### 7.gMLP

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-10-55-35-image.png" alt="" width="637" data-align="center">

论文链接： Pay Attention to MLPs https://www.kdocs.cn/l/csKW7zcbEOEx

gMLP（gated MLP）是一种**无需自注意力机制**的神经网络架构，仅通过**门控多层感知机**在语言和视觉任务中达到与Transformer相当的性能。其核心创新是**空间门控单元**，该单元通过门控机制实现跨token的信息交互。

$$
Z=GELU(LayerNorm(X)\cdot U)\\

\tilde{Z}=s(Z)\\
Y=\tilde{Z}\cdot V+X
$$

**7.1（Input Embeddings） 输入图像**，分割成patches，投影为嵌入向量

**7.2  gMLP块对输入向量进行通道投影+空间门控操作**：具体来说将Z均分为$Z_1,Z_2$，使门控信号（来自$Z_2$）与内容信号（来自$Z_1$）独立

SGU是gMLP的核心，用于实现跨token的信息混合，其数学表达式为：

$s(Z)=Z_1​⊙f_{W,b}​(Z_2​)$

- Z 被沿通道维度分割为两部分：$Z_1​,Z_2​∈R^{n×(d_{ffn}​/2)}$

- ⊙ 表示元素级乘法

- $f_{W,b}​(Z_2​)=W⋅LayerNorm(Z_2​)+b$ 是空间投影
  
  - $W∈R^{n×n}$：空间投影权重矩阵
  
  - $b∈R^n$：偏置向量

**7.3  输出处理**

### 8.SimCLR

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-11-20-47-image.png" alt="" data-align="center" width="342">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-11-21-35-image.png" alt="" width="393" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-11-24-22-dd0b54ade073419b1a4824b1793734f1.jpg" alt="" data-align="center" width="389">

论文链接： A Simple Framework for Contrastive Learning of Visual Representations https://www.kdocs.cn/l/cm3UJektIWs7

SimCLR（A Simple Framework for Contrastive Learning of Visual Representations）是一个简单而有效的**对比学习框架**，用于**无监督视觉表示学习**。其核心思想是通过**数据增强**创造正样本对，并在特征空间中**拉近正样本、推远负样本**。

主要包含四个主要组件：①数据增强模块②编码器网络③投影头网络④对比损失函数

**整个数据流过程**：对于每个输入图像x，SimCLR执行：

①.生成两个随机增强视图（<u>**随机裁剪和调整大小**</u>，**<u>随机色彩失真</u>**，随机高斯模糊，随机灰度化）：$\tilde{x_i},\tilde{x_j}$

②.通过编码器提取特征，例如采用ResNet-50网络架构或者其他任意的网络：$h_i=f(\tilde{x_i}),h_j=f(\tilde{x_j})$

③.通过投影头映射到对比空间:$z_i=g(h_i),z_j=g(h_j)$

④.计算对比损失函数，采用余弦相似度度量，$sim(u,v)=\frac{u^Tv}{\left \| u \right \|  \left \| v \right \| }​$。对于一个batch中的 N 个样本：

- **正样本对**：同一原始样本的两个增强视图 ($z_i,z_j$​)

- **负样本对**：不同原始样本的增强视图

每个batch生成 2N 个增强样本，形成 N 个正样本对。

NT-Xent损失推导:对于正样本对 (i,j)，损失函数为：

$$
l_{i,j}=-log\frac{exp(sim(z_i,z_j)/\tau}{\sum_{k=1}^{2N}1_{[k\ne i]}exp(sim(z_i,z_k)/\tau)}
$$

其中：

- $\tau$ 是温度参数

- $1_{[k\ne i]}$是指示函数，当 k=i 时为1

- 分母包含一个正样本和 2N−2 个负样本

最终损失函数:对batch中所有正样本对计算损失并取平均：

$$
Loss=\frac{1}{2N}\sum_{k=1}^{N}[l_{2k-1,2k}+l_{2k,2k-1}]
$$

这里计算了对称损失，同时考虑 $(z_{2k-1},z_{2k}),(z_{2k},z_{2k-1})$ 两个方向。

### 9.MobileViT

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-13-05-18-image.png" alt="" width="601" data-align="center">

论文链接：MobileViT Light-weight, General-purpose, and Mobile-friendly Vision Transformer
 https://www.kdocs.cn/l/cfH5YhGtw1eO

这里MV2表示MobileNetV2 block，参考之前分析的MobileNet的架构，$\downarrow 2$表示对图像下采样

补充一下**下采样**和**上采样**的知识：

<u>下采样</u>（把图像变小）：①最大池化②平均池化③步长卷积（采用大于1的步长）

<u>上采样</u>（把图像变大）：①最近邻上采样②双线性插值③转置卷积（填充+步长调整）

**下面我们着重分析MobileViT block**:

**9.1 输入图像**张量$X\in R^{H\times W \times C}$,采用标准$n*n$卷积提取局部特征，再用$1*1$卷积进行通道投影，投影到更高维度，最终得到$X_L\in R^{H \times W\times d}$

**9.2Unfold操作**（类似于ViT中前期操作）：

1. 将 $\mathbf{X}_L$ 划分为 $N$ 个不重叠的 $h \times w$ patches，$P=h\cdot w$;

2. 将每个 patch 展平为向量：$\mathbf{p}_{i,j} \in \mathbb{R}^{P \times d}$，其中 $i=1,\ldots,\frac{H}{h}$, $j=1,\ldots,\frac{W}{w}$;

3. 重组为序列：对于每个空间位置 $p \in {1,\ldots,P}$，收集所有 patches 在该位置的特征;

$$
\mathbf{X}_U(p,:,:) = \left[ \mathbf{p}_{1,1}[p], \mathbf{p}_{1,2}[p], \ldots, \mathbf{p}_{\frac{H}{h},\frac{W}{w}}[p] \right]^T
$$

**9.3 Transformer全局建模**：结构就是整个Transformer架构；对每一个$p\in R^{hw\times d}$输入到Transformer中

$$
\mathbf{X}_G(p,:,:) = \text{Transformer}(\mathbf{X}_U(p,:,:))
$$

**9.4Fold操作（序列到空间变换）**：

$$
\mathbf{X}_F = \text{Fold}(\mathbf{X}_G) \in \mathbb{R}^{H \times W \times d}
$$

1. 将 $\mathbf{X}_G \in \mathbb{R}^{P \times N \times d}$ 重组为 $N$ 个 $h \times w \times d$ 的 patches

2. 将 patches 重新排列到原始空间位置

3. 组合成完整的特征图

**9.5特征融合**

9.5.1通道维度调整：

$$
\mathbf{X}_F' = \text{Conv2D}_{1 \times 1}(\mathbf{X}_F) \in \mathbb{R}^{H \times W \times C}
$$

9.5.2跳跃连接与拼接：

$$
\mathbf{X}_{\text{concat}} = \text{Concat}(\mathbf{X}, \mathbf{X}_F') \in \mathbb{R}^{H \times W \times 2C}
$$

9.5.3最终特征融合：

$$
\mathbf{Y} = \text{Conv2D}_{n \times n}(\mathbf{X}_{\text{concat}}) \in \mathbb{R}^{H \times W \times C}
$$

### 10.Compact Convolutional Transformers

论文链接： Escaping the Big Data Paradigm with Compact Transformers https://www.kdocs.cn/l/cs5CPUmVDWXD

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-14-00-06-image.png" alt="" width="538" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-14-14-57-image.png" alt="" width="621" data-align="center">

这篇论文**提出的背景很具思考性**：

1.Transformers的数据饥渴：传统Vision Transformer（ViT）需要在大规模数据集（如JFT-300M）上预训练才能达到良好性能，这限制了其在数据稀缺领域的应用。

2.移动设备限制：大型ViT模型参数过多（如ViT-Base有85M参数），无法在资源受限的设备上部署。

3.科学医学领域限制：许多科学和医学领域数据集规模小（几千样本），且与ImageNet等通用数据集差异大，迁移学习效果有限。

下面是**CCT的完整数据流程**：

| 步骤             | 公式                                         | 说明          |
| -------------- | ------------------------------------------ |:-----------:|
| 输入             | $X∈R^{H×W×C}$                              | 原始图像        |
| 卷积 Tokenizer   | $F=ReLU(Conv2d(X))$                        | 提取局部特征      |
| 池化（可选）         | $F^′=MaxPool(F)$                           | 降维          |
| 展平             | $z_0​=Flatten(F^′)$                        | 得到 token 序列 |
| 位置编码           | $z_0​←z_0​+E_{pos}​$                       | 可选          |
| Transformer 编码 | $z_L​=TransformerEncoder(z0​)$             | L 层编码       |
| SeqPool        | $s=z_L​w$<br>$a=softmax(s)$<br>$z=a^Tz_L​$ | 加权池化        |
| 分类             | $\hat{y​}=Softmax(MLP(z))$                 | 输出预测        |

具体来说，

CCT 的核心创新在于：

1. **卷积 Tokenizer**

2. **SeqPool 序列池化**

3. **轻量化 Transformer 编码器**

①卷积 Tokenizer（Convolutional Tokenizer）

**目标：** 将图像转换为 token 序列，同时保留局部结构信息。增强局部特征提取能力，并减少对位置编码的依赖。

**步骤：**

1. **卷积层**：
   
   $F=ReLU(Conv2d(X))$
   
   - 输入：$X∈R^{H×W×C}$
   
   - 卷积核：$K∈R^{k×k×C×d}$
   
   - 输出特征图：$F∈R^{H^′×W^′×d}$

2. **最大池化**（可选，用于降维）：
   
   $F^′=MaxPool(F)$

3. **展平为序列**：
   
   $z_0​=Flatten(F^′)∈R^{n×d}$
   
   其中 n=H′′×W′′ 是 token 数量。

②位置编码（可选）

CCT 对位置编码的依赖较低，但仍可添加：

$z_0​←z_0​+E_{pos}​$

③Transformer 编码器

与 ViT 类似，但层数更少、头数更少、隐藏维度更小：

$z_ℓ​=TransformerLayer(z_{ℓ−1}​),ℓ=1,…,L$

④<u>**SeqPool 序列池化**</u>

**目标：** 将所有输出 token 池化为一个全局表示向量。

**步骤：**

1. 输出 token 序列：$z_L​∈R^{n×d}$

2. 通过一个线性层计算每个 token 的重要性分数：
   
   $s=z_L​w$其中$w∈R^{d×1}$

3. 对分数进行 softmax 归一化：
   
   $a=softmax(s)∈R^{n×1}$

4. 加权求和得到全局表示：
   
   $z=a^Tz_L​∈R^{1×d}$

⑤分类头

$\hat{y}​=Softmax(MLP(z))$

### 11.ConvMixer

论文链接：patches_are_all_you_need_https://www.kdocs.cn/l/cgmqpNAPvtcq

<img src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-14-53-10-image.png" title="" alt="" data-align="center">

与相关模型对比:

| 模型            | 空间混合           | 通道混合          | 核心操作     |
| ------------- | -------------- | ------------- |:--------:|
| ViT           | 自注意力           | MLP           | 注意力机制    |
| MLP-Mixer     | MLP(空间)        | MLP(通道)       | 全连接层     |
| **ConvMixer** | **深度卷积(大卷积核)** | **逐点卷积(1*1)** | **标准卷积** |

1. **输入层**：
   
   $X∈R^{H×W×C}$

2. **Patch Embedding**：
   
   $z_0​=BN(σ(Conv2D(X;W_{patch}​)))$
   
   其中 $W_{patch}​∈R^{p×p×C×h}$

3. **ConvMixer Layers**（重复d次）：
   
   对于 l=1 到 d：
   
   $z_l^′​=BN(σ(DepthwiseConv(z_{l−1}​;W_l^{depth}​)))+z_{l−1}$
   
   $​z_l​=BN(σ(PointwiseConv(z_l^′​;W_l^{point}​)))$
   
   其中：
   
   - $W_l^{depth}​∈R^{k×k×h×1}$（深度卷积权重）
   
   - $W_l^{point}​∈R^{1×1×h×h}$（逐点卷积权重）

4. **全局池化**：
   
   $z_{global}​=\frac{1}{H'\cdot W'}\sum_{i=1}^{H'}\sum_{j=1}^{W'}z_d(i,j,:)$
   
   其中 $H^′=p/H​,W^′=p/W​$

5. **分类层**：
   
   $\hat{y}=Softmax(W_{class}​⋅z_{global}​+b_{class}​)$
   
   其中 $W_{class}​∈R^{n_{classes}​×h}$

（emmm,笔者感觉就是patches+Mobilenet的组和...）

### 12.EANet

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-15-17-27-image.png" alt="" width="564" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-15-40-05-image.png" alt="" width="483" data-align="center">

论文链接：Beyond Self-attention External Attention using Two Linear Layers for Visual Tasks
 https://www.kdocs.cn/l/cqYBTVU2Y3FZ

EANet 的核心是提出了 **External Attention（外部注意力）** 机制，用于替代传统的 Self-Attention（自注意力）。其动机是：

- **Self-Attention 的问题**：
  
  - 计算复杂度高：O(N2)，其中 N 是像素/令牌数。
  
  - 只关注单个样本内部的关系，忽略了**跨样本的潜在关联**。

- **External Attention 的优势**：
  
  - 使用两个**外部、可学习、共享的记忆单元**（Memory）作为键和值。
  
  - 计算复杂度降为 O(N)。
  
  - 隐式地利用了整个训练集中的信息，具有**正则化**效果。

对于输入特征$F\in R^{N\times d}$,**self-attention计算**为$A=softmax(QK^T),F_{out}=AV$,其中Q,K,V都是F通过线性变换得到，是可以学习的权重矩阵

而**对于Etxternal attention**,使用两个外部记忆单元$M_k\in R^{S\times d},M_v\in R^{S\times d}$分别作为key memory 和 value memory。计算公式为：

$$
A=Norm(F\cdot M_k^T)\\
F_{out}=AM_v
$$

同时论文中提到，为了增强注意力图的稳定性，他们采取对注意力矩阵进行**行列双重归一化(按列softmax,按行归一化)**：

$$
\hat{A}=FM_k^T\\
\hat{A_{i,j}}=\frac{exp(\hat{A_{i,j}})}{\sum_{k}exp(\hat{A_{k,j}})}\\
A_{i,j}=\frac{\hat{A_{i,j}}}{\sum_{k}\hat{A_{k,j}}}
$$



### 13.Involutional neural networks

论文链接：Involution Inverting the Inherence of Convolution for Visual Recognition
 https://www.kdocs.cn/l/craKkhpQjrvz

1. **卷积的固有局限性**

- **空间无关性**：卷积核在空间上是共享的，无法适应不同位置的视觉模式。

- **通道特异性**：每个通道有独立的卷积核，导致通道间冗余。

- **局部性限制**：小卷积核难以捕获长距离依赖。

 2. **Involution 的设计理念**

- **空间特异性**：每个空间位置有自己独特的核。

- **通道无关性**：同一位置的所有通道共享同一个核。

- **动态核生成**：核由输入特征图动态生成，适应性强。

- **大感受野**：支持大核尺寸，捕获长距离依赖。

3. **与自注意力的关系**

- 自注意力是 Involution 的一种复杂实例。

- Involution 简化了关系建模，仅基于单个像素生成核，避免了复杂的像素对关系计算。

<u>**13.1标准卷积的详细推导**</u>

1. <u>符号定义</u>
- 输入特征图：$\mathbf{X} \in \mathbb{R}^{H \times W \times C_i}$

- 卷积核：$\mathcal{F} \in \mathbb{R}^{C_o \times C_i \times K \times K}$

- 输出特征图：$\mathbf{Y} \in \mathbb{R}^{H \times W \times C_o}$
2. <u>卷积操作公式推导</u>

对于输出位置 $(i,j)$ 和输出通道 $k$：

$Y_{i,j,k}​=\sum_{c=1}^{C_i}\sum_{(u,v)\in \Delta_K }F_{k,c,u+\left \lfloor K/2 \right \rfloor ,v+\left \lfloor K/2 \right \rfloor }\cdot X_{i+u,j+v,c}​$

其中：

- $\Delta_K = [- \lfloor K/2 \rfloor, \cdots, \lfloor K/2 \rfloor] \times [- \lfloor K/2 \rfloor, \cdots, \lfloor K/2 \rfloor]$ 是 $K \times K$ 邻域偏移

- $\mathcal{F}_{k,c,:,:}$ 是第 $k$ 个输出通道、第 $c$ 个输入通道的卷积核
3. <u>卷积的核心特性分析</u>

**空间维度**：

- 对于不同的空间位置 $(i_1,j_1)$ 和 $(i_2,j_2)$，使用**相同的**卷积核 $\mathcal{F}_{k,c,:,:}$

- 这就是"空间无关性"（spatial-agnostic）

**通道维度**：

- 对于不同的输出通道 $k_1$ 和 $k_2$，使用**不同的**卷积核 $\mathcal{F}*{k_1,:,:,:}$ 和 $\mathcal{F}*{k_2,:,:,:}$

- 这就是"通道特异性"（channel-specific）

**参数量**：$C_o \times C_i \times K \times K$

<u>**13.2Involution的详细推导**</u>

1.<u>符号定义</u>

- 输入特征图：$\mathbf{X} \in \mathbb{R}^{H \times W \times C}$

- Involution核：$\mathcal{H} \in \mathbb{R}^{H \times W \times K \times K \times G}$

- 输出特征图：$\mathbf{Y} \in \mathbb{R}^{H \times W \times C}$

2.<u>Involution操作公式推导</u>

对于输出位置 $(i,j)$ 和输出通道 $k$：

$$
Y_{i,j,k}=\sum_{(u,v)\in \Delta_K}H_{i,j,u+\left \lfloor K/2 \right \rfloor ,v+\left \lfloor K/2 \right \rfloor ,\left \lceil kG/C \right \rceil }\cdot X_{i+u,j+v,k}
$$

详细解释这个公式：

 核的索引 $\mathcal{H}_{i,j,:,:,g}$

- $i,j$：**空间位置**，每个位置有自己独特的核

- $:,:$：$K \times K$ 的空间核权重

- $g = \lceil kG/C \rceil$：**分组索引**，控制通道间的共享程度

与卷积的关键差异：

①**空间维度**：

- 位置 $(i_1,j_1)$ 使用核 $\mathcal{H}_{i_1,j_1,:,:,:}$

- 位置 $(i_2,j_2)$ 使用核 $\mathcal{H}_{i_2,j_2,:,:,:}$

- **每个位置有独特的核** → "空间特异性"

②**通道维度**：

- 通道 $k_1$ 和 $k_2$ 如果属于同一组（$\lceil k_1G/C \rceil = \lceil k_2G/C \rceil$），则使用**相同的**核切片

- **通道间共享核** → "通道无关性"

3.<u>核生成函数推导</u>（这里可以思考，可以替代的核函数生成方式，进一步研究达到性能边界）

由于每个位置都需要独特的核，如果直接存储 $\mathcal{H}$，参数量会爆炸：$H \times W \times K \times K \times G$

解决方案：**动态生成核**

$$
H_{i,j}=\phi(X_{i,j})=W_1\cdot \sigma(W_0\cdot X_{i,j})
$$

详细展开：

1. 输入：$\mathbf{X}_{i,j} \in \mathbb{R}^C$（单个位置的C维特征向量）

2. 第一个线性变换：  
   $\mathbf{z} = W_0\cdot X_{i,j}$，其中 $\mathbf{W}_0 \in \mathbb{R}^{\frac{C}{r} \times C}$
   
   - 输出维度：$\frac{C}{r}$，$r$ 是压缩比

3. 非线性激活：  
   $\mathbf{a} = \sigma(\mathbf{z})$，包含BN和ReLU

4. 第二个线性变换：  
   $\mathcal{H}_{i,j} = \mathbf{W}_1 \cdot \mathbf{a}$，其中 $\mathbf{W}_1 \in \mathbb{R}^{(K \times K \times G) \times \frac{C}{r}}$
   
   - 输出维度：$K \times K \times G$，重塑为核的形状

**参数量**：仅需存储 $\mathbf{W}_0$ 和 $\mathbf{W}_1$，与输入尺寸 $H,W$ 无关！

### 14.Perceiver

论文链接： Perceiver General Perception with Iterative Attention https://www.kdocs.cn/l/cgOBQWaw1X9i

<img src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-18-41-25-image.png" title="" alt="" data-align="center">

**<u>直观解释：</u>**

Perceiver 的秘诀在于：**不要直接在庞大的输入数据上做自注意力，而是先把输入“概括”或“压缩”到一个固定大小的“工作记忆区”（潜在空间），然后在这个小得多的记忆区里进行复杂的计算。**

这个过程的比喻：

- **输入数据**：一本厚厚的百科全书（海量信息）。

- **潜在数组（Latent Array）**：你手中的一小叠空白卡片（固定大小的记忆区）。

- **交叉注意力（Cross-Attention）**：你**从书中提取**关键信息，并**写**到卡片上。

- **潜在自注意力（Latent Self-Attention）**：你**反复阅读、思考和整理**手中的卡片，让卡片上的信息之间产生关联，形成更深的理解。

- **迭代**：整理完一轮卡片后，你可以**再次翻阅百科全书**，根据你当前的理解，把更相关、更细致的信息补充到卡片上。

这个过程可以重复多次，每次都在迭代地“蒸馏”信息。

**14.1 定义输入和潜在数组**：**输入字节数组(Byte Array)**:图像、音频、点云等，表示为字节数组$X\in R^{M\times C}$,M是输入元素数量，C为每个元素的通道数，**位置编码**：使用**傅里叶特征或学习的位置编码**，与输入特征拼接；**潜在数组(Latent  Array)**:$L\in R^{N\times D}$，N是潜在单元数量，是一个超参数，通常N相对于M很小（256，512），且L是随机初始化的，可以看作一组可学习的查询。

**14.2 交叉注意力(Cross Attention)——从输入中汲取信息**：

这是**降低计算复杂度的关键**。

- **标准自注意力**：Query,Key, Value都来自同一个输入 X。复杂度 O(M²)。

- **Perceiver 的交叉注意力**：
  
  - Q来自**潜在数组** L。
  
  - K,V来自**输入字节数组** X。

**数学过程**：

1. 线性投影：
   
   - $Q=LW^Q∈R^{N×D_k}​$
   
   - $K=XW^K∈R^{M×D_k}​$
   
   - $V=XW^V∈R^{M×D}​$

2. 注意力计算：
   
   - $Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}}​)V∈R^{N×D}​$

**为什么复杂度降低了？**

- 在 $QK^T$ 这一步，我们计算的是一个 N×M 的矩阵，而不是 M×M。

- 因此，复杂度从 O(M²) 降到了 **O(M x N)**。因为 N 是固定的且远小于 M，这使得处理大规模输入成为可能。

**这一步的作用**：潜在数组 L 中的每一个单元，都根据输入 X 的所有信息进行了更新。相当于每个潜在单元都“看了一遍”整个输入，并提取了与自己最相关的部分。

**14.3 潜在Transformer(Latent Transformer)——内部思考加工：**

现在，我们有了一个更新后的潜在数组 L‘，它包含了从输入中提取的“摘要”信息。

- 我们将 L′ 输入到一个标准的 Transformer 模块中。

- 这个 Transformer 在潜在数组内部进行**自注意力**：L′′=Transformer(L′)。

- 因为 N 很小（例如512），这个自注意力的复杂度 **O(N²)** 是非常低的，我们可以堆叠很多层（比如48层）来构建一个非常深的、表达能力强的模型，而计算代价依然可控。

**这一步的作用**：让潜在单元之间进行充分的“交流”，融合信息，建立起全局的、高层次的理解。

**14.4 迭代(Iteration)——多次查阅与精炼**

<u>步骤2和步骤3可以重复多次，</u>

在每次迭代中，潜在数组都会基于当前已经形成的理解，**再次去“查阅”原始输入** X，提取更精细或之前被忽略的信息。**迭代注意力**，多次从输入中蒸馏信息。

为了节省参数，除了第一次，后续迭代的交叉注意力层和Transformer层可以共享参数。这让模型更像一个在深度上展开的循环神经网络（RNN）。

**14.5 输出(Output)**

经过若干次迭代后，我们得到最终的潜在数组，对其在N个维度上全局平均池化，得到D维向量，接一个简单的线性分类器，得到最终分类结果。

注：笔者的一个误解，一开始以为这个架构能够处理很复杂的高维数据，实则参考了一些网上大神的观点，Perceiver 的核心创新不是"直接处理高维数据"，而是**建立一个固定大小的"信息摘要系统"**：

1. **不问结构**：将所有输入视为无结构的字节序列

2. **位置告知**：通过位置编码告诉模型元素间的关系

3. **工作记忆**：用固定大小的潜在数组作为信息蒸馏器

4. **迭代查阅**：多次从原始输入中提取相关信息到工作记忆

5. **深度思考**：在小型工作记忆中进行复杂计算

这种设计让 Perceiver 能够以**固定的计算预算**处理**任意维度、任意规模**的输入，这才是它真正的威力所在！

### 15.Swin Transformers

论文链接：Swin Transformer Hierarchical Vision Transformer using Shifted Windows
 https://www.kdocs.cn/l/coGgq7o8HORY

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-19-45-21-image.png" alt="" width="444" data-align="center">

<img src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-11-01-19-52-23-image.png" title="" alt="" data-align="center">

想象一下，你要处理一张高清图片。

1. **老方法（ViT）**：把整张图片一下子给一个“超级大脑”（全局自注意力）去分析。这个大脑很厉害，但计算量巨大（计算量与图片尺寸的平方成正比），图片一大就算不动了。**而且它只输出一种分辨率（人为选定的patch_size）的理解，不适合做需要多尺度信息的任务**（比如在目标检测中，你需要同时看清小物体和大物体）。

2. **Swin Transformer 的方法**：不像“超级大脑”那样一次性看全图，而是像我们人眼一样，**先看局部，再逐步整合**。
   
   - **局部看**：把图片划分成一个个不重叠的小窗口（比如 7x7 的补丁），只在每个小窗口内部进行“自我分析”（自注意力）。这极大地降低了计算量。
   
   - **逐步整合**：随着网络变深，把相邻的小窗口“粘合”起来，形成更大的窗口，从而让模型的“视野”逐渐变大，最终理解整个图片的全局信息。

我们以经典的Swin-T模型为例，输入一张$224\times 224\times 3$的RGB图片：

| 阶段               | 操作                              | 输出分辨率       | 输出通道数      | Swin Block 数量 | 功能比喻                |
|:----------------:|:-------------------------------:|:-----------:|:----------:|:-------------:|:-------------------:|
| **Patch Split**  | 切分成 4x4 小块                      | 56 x 56     | 48 (4x4x3) | -             | 把原材料（图片）切成标准砖块      |
| **Linear Embed** | 线性变换                            | 56 x 56     | **C=96**   | -             | 给砖块贴上特征标签           |
| **Stage 1**      | Swin Blocks                     | 56 x 56     | 96         | x2            | 第一层，在小范围内精细加工砖块     |
| **Stage 2**      | **Patch Merging** + Swin Blocks | **28 x 28** | **192**    | x2            | 把 4 块砖粘成 1 块大砖，视野变大 |
| **Stage 3**      | Patch Merging + Swin Blocks     | **14 x 14** | **384**    | x6            | 继续粘合，视野再次翻倍，理解更复杂结构 |
| **Stage 4**      | Patch Merging + Swin Blocks     | **7 x 7**   | **768**    | x2            | 最终层，拥有整个图片的全局视野     |

1. **Patch Merging（补丁合并）**：
   
   - **作用**：降低分辨率，增加通道数，形成金字塔式的层次结构。
   
   - **做法**：将 2x2 的相邻小窗口拼接起来（通道数变为 4倍），然后通过一个线性层将通道数减半（例如从 4C 变为 2C）。这实现了 2倍下采样。

2. **Swin Transformer Block**：
   
   由两个连续子块**常规窗口自注意力（W-MSA）和偏移窗口自注意力（SW-MSA）组**成：
   
   第一个子块W-MSA：将输入特征图均匀划分M*M的非重叠窗口，只在每个窗口内部计算自注意力（**论文中在计算时引入带相对位置偏置的参数矩阵B,可以参考下面GCViT**，说是可以提高性能，不理解但尊重），窗口与窗口之间没有任何交流；
   
   第二个子块SW-MSA：将窗口的划分起点向右下角偏移（M/2,M/2）个像素。这样一来，新的窗口就由上一层的不同窗口的部分组成。但是会带来**窗口数量增加和窗口大小不一**的问题，论文中提到的方法是**循环移位+注意力掩码**（移位后，一个窗口可能包含原本不相邻的区域，通过掩码矩阵，限制注意力只在"原本属于同一连续区域"的token间计算）
   
   具体的举例可以参考<u>https://blog.csdn.net/qq_39478403/article/details/120042232</u>和原论文中插图Figure4，理解起来还是比较困难的，这里笔者就不详细解释了，有点说不清......

虽然中间处理的细节有点复杂，但是思想还是可以借鉴的。

### 16.GCViT

论文链接：Global Context Vision Transformers https://www.kdocs.cn/l/cdWXtOnMJNt1

![](C:\Users\Lenovo\AppData\Roaming\marktext\images\2025-11-01-21-04-04-image.png)

![](C:\Users\Lenovo\AppData\Roaming\marktext\images\2025-11-01-21-33-13-image.png)

背景问题：

- **ViT的计算瓶颈**：标准Vision Transformer的自注意力机制具有O(N²)计算复杂度，难以处理高分辨率图像

- **局部注意力限制**：Swin Transformer等采用局部窗口注意力，但感受野受限，难以建模长程依赖

- **缺乏归纳偏置**：纯Transformer架构缺乏CNN固有的平移不变性和局部性偏置

模型架构：

**1.DownSampler**:下采样操作,输入$X\in R^{B\times H\times W\times C}$,B是批次大小

$$
\begin{aligned}
&\text{深度可分离卷积} \\
&\hat{X} = \text{DW-Conv}_{3×3}(X) \\
&\text{其中：} \text{DW-Conv}_{3×3}(X)[b,i,j,c] = \sum_{m,n=-1}^{1} W[m,n,c] \cdot X[b,i+m,j+n,c] \\
\\
&\text{GELU激活函数} \\
&\hat{X} = \text{GELU}(\hat{X}) = X \cdot \Phi(X) \\
&\text{其中：} \Phi(X) = \frac{1}{2}[1 + \text{erf}(\frac{X}{\sqrt{2}})] \\
\\
&\text{压缩激励注意力（SE Block）} \\
&\text{(a) 全局平均池化：} z_c = \frac{1}{H\times W}\sum_{i=1}^{H}\sum_{j=1}^{W} \hat{X}[b,i,j,c] \\
&\text{(b) 两个全连接层：} s = \sigma(W_2 \cdot \delta(W_1 \cdot z)) \\
&\text{(c) 通道重标定：} \hat{X}_{\text{se}}[b,i,j,c] = s_c \cdot \hat{X}[b,i,j,c] \\
\\
&\text{1×1卷积 + 残差连接} \\
&X_{\text{out}} = \text{Conv}_{1×1}(\hat{X}_{\text{se}}) + X\\
\\
&\text{3×3最大池化，stride=2} \\
&X_{\text{down}}[b,i,j,c] = \max_{m,n=0}^{2} X_{\text{out}}[b, 2i+m, 2j+n, c] \\
\\
&\text{最终输出尺寸：} (B, \frac{H}{2}, \frac{W}{2}, 2C)
\end{aligned}
$$

**2.LocalMSA(局部多头自注意力)**：输入$X\in R^{B\times H\times W\times C}$,B是批次大小

$$
\begin{aligned}
&\text{将特征图划分为 } N_w = \frac{H}{h} \times \frac{W}{w} \text{ 个不重叠窗口} \\
&X_{\text{windows}} ∈ ℝ^{(B×N_w)×(h×w)×C}\\
\\
&Q = X_{\text{windows}}W^Q \quad ∈ ℝ^{(B×N_w)×(h×w)×d} \\
&K = X_{\text{windows}}W^K \quad ∈ ℝ^{(B×N_w)×(h×w)×d} \\
&V = X_{\text{windows}}W^V \quad ∈ ℝ^{(B×N_w)×(h×w)×d} \\
&\text{其中：} d = \frac{C}{N_{\text{heads}}}, \quad W^Q, W^K, W^V ∈ ℝ^{C×d}\\
\\
&Q_{\text{heads}} = \text{reshape}(Q, (B×N_w, h×w, N_{\text{heads}}, d)) \quad ∈ ℝ^{(B×N_w)×N_{\text{heads}}×(h×w)×d} \\
&K_{\text{heads}} = \text{reshape}(K, (B×N_w, h×w, N_{\text{heads}}, d)) \\
&V_{\text{heads}} = \text{reshape}(V, (B×N_w, h×w, N_{\text{heads}}, d))\\
\\
&\text{ 计算注意力分数矩阵} \\
&A = \frac{Q_{\text{heads}}K_{\text{heads}}^⊤}{\sqrt{d}} \quad ∈ ℝ^{(B×N_w)×N_{\text{heads}}×(h×w)×(h×w)} \\
\\
&\text{ 详细展开：} \\
&A[b,n,i,j] = \frac{1}{\sqrt{d}} \sum_{k=1}^{d} Q_{\text{heads}}[b,n,i,k] \cdot K_{\text{heads}}[b,n,j,k]\\
\\
&\text{相对位置计算} \\
&\Delta x = x_i - x_j, \quad \Delta y = y_i - y_j \\
&\text{其中：} x_i = i \mod w, \quad y_i = \lfloor i / w \rfloor \\
\\
&\text{ 偏置查找} \\
&B[i,j] = \hat{B}[\Delta x + (p-1), \quad \Delta y + (p-1)] \\
&\text{其中：} \hat{B} ∈ ℝ^{(2p-1)×(2p-1)} \text{ 是可学习参数表}\\
\\
&\text{Attention} = \text{Softmax}(A + B) \cdot V_{\text{heads}} \\
&\text{详细推导：} \\
&\text{Output}[b,n,i,k] = \sum_{j=1}^{h×w} \frac{\exp(A[b,n,i,j] + B[i,j])}{\sum_{m=1}^{h×w} \exp(A[b,n,i,m] + B[i,m])} \cdot V_{\text{heads}}[b,n,j,k]\\
\\
&\text{Output}_{\text{merged}} = \text{reshape}(\text{Output}, (B×N_w, h×w, C)) \\
&\text{Output}_{\text{final}} = \text{Linear}(\text{Output}_{\text{merged}}) \\
&\text{Output}_{\text{final}} = \text{reshape}(\text{Output}_{\text{final}}, (B, H, W, C))
\end{aligned}
$$

**3.GlobalMSA(全局多头自注意力)**：输入$X\in R^{B\times H\times W\times C}$,B是批次大小

先生成全局查询生成器：

$$
\begin{aligned}
&\text{应用 } L = \log_2(\frac{H}{h}) \text{ 次Fused-MBConv + downsample} \\
&X^{(0)} = X \\
&\text{对于 } l = 1 \text{ 到 } L: \\
&X^{(l)} = \text{DownSampler}(X^{(l-1)}) \\
&\text{最终得到：} X^{(L)} ∈ ℝ^{(B×h×w×C)}\\
\\
&Q_g = \text{Linear}(X^{(L)}) \quad ∈ ℝ^{B×C×h×w} \\
\\
&\text{重复操作匹配窗口数量} \\
&N_w = \frac{H}{h} \times \frac{W}{w} \\
&Q_g^{\text{repeat}} = \text{repeat}(Q_g, \text{dim}=0, \text{repeats}=N_w) \quad ∈ ℝ^{(B×N_w)×C×h×w}
\end{aligned}
$$

再计算全局注意力：

$$
\begin{aligned}
&\text{局部K,V生成:}\\
&K = \text{Linear}(X_{\text{windows}}) \quad ∈ ℝ^{(B×N_w)×(h×w)×d} \\
&V = \text{Linear}(X_{\text{windows}}) \quad ∈ ℝ^{(B×N_w)×(h×w)×d}  \\
\\
&\text{全局Q处理:}\\
&Q_g^{\text{repeat}} ∈ ℝ^{(B×N_w)×C×h×w} \\
&\text{重塑为：} Q_g^{\text{reshape}} = \text{reshape}(Q_g^{\text{repeat}}, (B×N_w, h×w, C)) \\
&\text{投影为：} Q_g^{\text{projected}} = \text{Linear}(Q_g^{\text{reshape}}) \quad ∈ ℝ^{(B×N_w)×(h×w)×d}\\
\\
&\text{使用全局Q，局部K,V:} \\
&A = \frac{Q_g^{\text{projected}}K^⊤}{\sqrt{d}} + B \quad ∈ ℝ^{(B×N_w)×(h×w)×(h×w)} \\
\\
&\text{详细解释：} \\
&\text{对于每个位置 } i \text{ 在全局查询中：} \\
&A[b,i,j] = \frac{1}{\sqrt{d}} \sum_{k=1}^{d} Q_g^{\text{projected}}[b,i,k] \cdot K[b,j,k] + B[i,j]
\\
\\
&\text{Output} = \text{Softmax}(A) \cdot V \\
&\text{Output} = \text{Linear}(\text{Output}) \quad ∈ ℝ^{(B×N_w)×(h×w)×C} \\
&\text{Output}_{\text{final}} = \text{reshape}(\text{Output}, (B, H, W, C))
\end{aligned}
$$




