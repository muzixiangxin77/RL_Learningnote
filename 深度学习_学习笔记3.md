# **<mark>CNN的初步</mark>**

1.卷积神经网络起源，基本结构，如何用Keras实现

2.一些最佳的卷积神经网络架构及在Keras中使用预训练的模型

3.视觉任务：物体检测（在图像中分类多个物体并在其周围放置边界框），语义分割（根据物体所属的类别对每个像素进行分类）

## **一.卷积层**

第一层卷积层中神经元与它们感受视野内的像素相连，在隐藏层中处理较小的低级特征；第二层卷积层中每个神经元又与第一层中一个小矩形区域内的神经元相连，将低级特征组合成更大的高级特征...以此类推，如图所示：

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-14-14-14-31-image.png" alt="" width="438" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-14-14-25-07-image.png" alt="" width="396" data-align="center">

将图像投影到一个轴上，看上去是二维的

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-14-14-25-32-image.png" alt="" width="393" data-align="center">

实际上是如下形式：（我们一般默认当前层的深度方向维度都是前一层的的深度，只考虑空间维度上的改变和特征提取）

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-14-14-41-15-image.png" alt="" width="441" data-align="center">

实际上每个图像（彩色）包含三个颜色通道（RGB），同时还有比如说m*n像素，那么我们的卷积核实际上并不是一个二维矩阵，而是一个三维矩阵(3,w,h)的一个权重矩阵。然后每个卷积层，含有很多滤波器（对应着有多少个不同的你设置的卷积核），卷积核在底层图像中平移移动，作运算得到一个数值，然后不断移动得到一层（Map），每个Map即为一个特征图。所以每个滤波器输出一个特征图。最终导致其实每个卷积层也是一个三维的。这里一个特征图（Map）中的神经元共享相同权重偏置参数。

一般来说：

彩色图像是三个通道：红，绿，蓝；

灰度图像是一个通道

卫星图像捕获额外光频（如红外线等）:更多通道

**数学公式：（计算从第l-1的卷积层到第l的卷积层的推导值）**

$$
z_{i,j,k}=b_k+\sum_{u=0}^{f_h-1}\sum_{v=0}^{f_w-1}\sum_{k'=0}^{f_{n'}-1}x_{i',j',k'}\cdot w_{u,v,k',k}

$$

这里，

$z_{i,j,k}$代表位与第l的卷积层的第k个特征图的第i行第j列的神经元的结果值

$f_h,f_w,f_{n'}$分别代表卷积核（过滤器）的宽度，长度，高度（也即在第l-1的卷积层卷过的层数）

$x_{i',j',k'}$代表位与第l-1的卷积层的第k'个特征图的第i'行第j'列的神经元结果值

$i'=i\cdot s_h+u,s_h$代表沿长度方向的步长

$j'=j\cdot s_w+v,s_w$代表沿宽度方向的步长

$b_k$代表第l的卷积层第k个特征图的偏置

**tensorflow中数据预处理函数：**

| 特性       | Resizing层 | CenterCrop层    |
| -------- | --------- | -------------- |
| **操作类型** | 缩放        | 裁剪             |
| **宽高比**  | 默认改变      | 保持原比例          |
| **内容保留** | 全部内容（但变形） | 中心内容（边缘丢失）     |
| **填充行为** | 无填充       | 自动填充（当图像小于目标时） |
| **插值方法** | 支持多种插值算法  | 无插值（直接裁剪）      |
| **适用场景** | 通用尺寸统一    | 主体居中的图像        |
| **数据分布** | 改变像素值分布   | 保持原始像素值        |
| **计算开销** | 中等（需插值计算） | 低（仅索引操作）       |

**1.CenterCrop**:从图像中心裁剪指定尺寸的区域，height=70,width=120表示裁剪后目标高度70像素，宽度120像素，如果原始图像尺寸小于目标尺寸，会自动填充黑边（padding）

**2.Rescaling**：对图像像素值进行线性缩放，scale=1/255表示每个像素值乘以1/255

注：后面我们需要预训练和复用一些经典神经网络时都是用Resizing函数将原始数据改变使其适合复用的神经网络的输入形式

```
from sklearn.datasets import load_sample_images
import tensorflow as tf

images = load_sample_images()["images"]
images = tf.keras.layers.CenterCrop(height=70, width=120)(images)
images = tf.keras.layers.Rescaling(scale=1 / 255)(images)

images.shape # 输出结果为(num_samples,70,120,3)


tf.random.set_seed(42)
conv_layer=tf.keras.layers.Conv2D(filters=32,kernel_size=7)
fmaps=conv_layer(images) #输出结果为(num_samples,64,114,32)
```

这里解释一下为什么输出结果形状为这种形式：因为首先filters=32，说明一共是32个过滤器，32个卷积核，对应32个特征图，所以卷完之后得到32层特征图。其次，卷积核在二维上是7*7的，在没指定步长和没考虑边界值的时候，默认步长为1而且不会有边界填充0的情况，所以每次卷积都只是在内部进行，所以70-6=64,120-6=114。样本数不变。

**设置步长及边界情况：**

```
if we set padding="same"

conv_layer=tf.keras.layers.Con2D(filters=32,kernel_size=7,padding="same")
fmaps=conv_layer(images)

fmaps.shape #输出结果为(num_samples,70,120,32)
```

```
if we set stride strides=2 and set padding="same"

conv_layer=tf.keras.layers.Conv2D(filters=32,kernel_size=7,padding="same",strides=2)
fmaps=conv_layer(images)

fmaps.shape #输出结果为(num_samples,35,60,32)
```

如果需要**查看卷积核和偏置项参数**：

```
kernels,biases=conv_layer.get_weights()

#output_channel等于过滤器个数，即特征图层数

kernels.shape #(7,7,3,32)[height,input_width,input_channel,output_channel]
biases.shape #(32,)[output_channel]
```

**注意：当然我们需要设置在卷积层设置相应的激活函数，比如ReLU函数及权重初始化He initialization，与Dense层一样。（直接在Conv2D后面加上相应参数即可）**

## **二.池化层**

**主要类型：<mark>最大池化</mark>（在一个小矩形中选择最大的元素）通常选择；<mark>平均值池化</mark>（小矩形求平均值）**

主要作用：

1.减少计算负载，内存使用量和参数数量

2.对小位移的一定程度不变性，如平移不变性，旋转不变性，尺度不变性等

3.在某些应用中不变性往往不需要，以语义分割（即根据像素所属的对象对图像中的每个像素进行分类的任务）为例：如果图像右移一个像素，输出理应向右平移一个像素，但是在一些情况下目标是不变性的。

```
max_pool=tf.keras.layers.MaxPool2D(pool_size=2,padding="same",strides=2)
output=max_pool(images)


average_pool=tf.keras.layers.AvgPool2D(...)
output=average_pool(images)
```

<mark>**传统池化**</mark>**（MaxPooling2D)：在空间维度上操作，只是减小了特征图的尺寸，实际通道数未发生改变**

**<mark>深度池化</mark>：在深度通道上操作，减少了通道数，保持空间尺寸不变（将输入的张量的通道分成若干组，对每组通道进行最大池化操作，每组输出一个通道，即一个最大值）**

**<mark>全局平均池化</mark>：计算每个完整特征图的平均值（就像一个使用与输入相同空间维度的池化核的平均池化层，所有空间位置：高度和宽度，压缩为一个单一值）。为每个特征图和每个实例输出一个数字，只保留通道维度**

#### 与传统方法的对比(B:batch size;H:height;C:channel;W:width)

| 特性          | 全局平均池化 (GAP) | 全连接层 (FC)    | 展平层 (Flatten) |
| ----------- | ------------ | ------------ | ------------- |
| **输出维度**    | [B, C]       | [B, D] (自定义) | [B, H×W×C]    |
| **参数数量**    | 0            | 大量 (H×W×C×D) | 0             |
| **过拟合风险**   | 低            | 高            | 中             |
| **空间信息**    | 聚合           | 丢失           | 保留            |
| **输入尺寸灵活性** | 支持任意尺寸       | 固定尺寸         | 固定尺寸          |
| **计算复杂度**   | 极低           | 高            | 低             |

#### 与传统池化的对比

| 特性       | DepthPool (深度池化)       | MaxPooling2D (空间池化)    |
| -------- | ---------------------- | ---------------------- |
| **操作维度** | 通道维度 (C)               | 空间维度 (H, W)            |
| **输出尺寸** | [B, H, W, C/pool_size] | [B, H/pool, W/pool, C] |
| **信息保留** | 保留空间细节                 | 保留通道完整信息               |
| **典型用途** | 通道降维、轻量化网络             | 空间下采样、特征抽象化            |

```
#深度池化操作：
class DepthPool(tf.keras.layers.Layer):
    def __init__(self, pool_size=2, **kwargs):
        super().__init__(**kwargs)
        self.pool_size = pool_size  # 每组通道数

    def call(self, inputs):
        # 1. 获取输入张量动态形状 [batch, height, width, channels]
        shape = tf.shape(inputs)  

        # 2. 计算通道分组数量
        groups = shape[-1] // self.pool_size  # 总通道数 / 每组通道数

        # 3. 重塑张量形状：添加分组维度
        # 新形状：[batch, height, width, groups, pool_size]
        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)

        # 4. 执行最大池化操作
        # - 先重塑张量
        # - 在最后一个维度(pool_size)上取最大值
        # - 输出形状：[batch, height, width, groups]
        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)  


#全局平均池化：
global_avg_pool = tf.keras.layers.GlobalAvgPool2D() 
global_avg_pool(images)
```

```
#实战：
import tensorflow as tf
from tensorflow.keras import layers, models, datasets

# 1. 定义自定义深度池化层
class DepthPool(layers.Layer):
    def __init__(self, pool_size=2, **kwargs):
        super().__init__(**kwargs)
        self.pool_size = pool_size

    def call(self, inputs):
        shape = tf.shape(inputs)
        groups = shape[-1] // self.pool_size
        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)
        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)

# 2. 加载并预处理 CIFAR-10 数据集
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0  # 归一化

# 3. 构建包含 DepthPool 的模型
model = models.Sequential([
    # 输入层 (32x32 RGB 图像)
    layers.Input(shape=(32, 32, 3)),

    # 卷积块1
    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),

    # 深度池化层 (替代传统空间池化)
    DepthPool(pool_size=2),  # 通道数减半 (64->32)，空间尺寸不变

    # 卷积块2
    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),

    # 结合深度池化和空间池化
    DepthPool(pool_size=2),  # 通道数减半 (128->64)
    layers.MaxPooling2D((2, 2)),  # 空间尺寸减半

    # 卷积块3
    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),

    # 分类头
    layers.GlobalAveragePooling2D(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')  # CIFAR-10 有10个类别
])

# 4. 编译模型
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 5. 训练模型
history = model.fit(train_images, train_labels,
                    epochs=20,
                    batch_size=64,
                    validation_split=0.2)

# 6. 评估模型
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f'测试准确率: {test_acc:.4f}')
```

## **三.CNN架构**

**常规CNN架构**：多个卷积层（每个卷积层后面跟个激活函数ReLU），然后是池化层，再是卷积层，再是池化层，以此类推。最后添加一个，常规的前馈神经网络由几个全连接层（+ReLU）组成，输出层输出预测结果（sigmoid,softmax,...）。

卷积核不宜过大，基本上是(3,3)/(2,2)，使用更少参数并减少计算量，且性能更好。

但是对于第一个卷积层例外：通常可以(5,5)，步长为2或更大，减少图像空间维度而不会丢失太多信息。

```
from functools import partial


DefaultConv2D=partial(tf.keras.layers.Conv2D,kernel_size=3,padding="same",
                       activation="relu",kernel_initializer="he_normal")


model=tf.keras.Sequential([
           DefaultCon2D(filters=64,kernel_size=7,input_shape[28,28,1]),
           tf.keras.layers.MaxPool2D((2,2)),
           DefaultCon2D(filters=128),
           DefaultCon2D(filters=128),
           tf.keras.layers.MaxPool2D((2,2)),
           DefaultCon2D(filters=256),
           DefaultCon2D(filters=256),
           tf.keras.layers.MaxPool2D((2,2)),
           tf.keras.layers.Flatten(),
           tf.keras.layers.Dense(units=128,activation="relu",
                                 kernel_initializer="he_normal"),
           tf.keras.layers.Dropout(0.5),
           tf.keras.layers.Dense(units=64,activation="relu",
                                 kernel_initializer="he_normal"),
           tf.keras.layers.Dropout(0.5),
           tf.keras.layers.Dense(units=10,activation="softmax")])


model.compile(loss="sparse_categorical_crossentropy", optimizer="adam",
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))
score = model.evaluate(X_test, y_test)
```

<mark>**前辈大神高级的东西来了~~~~接下来会有好几种常规CNN架构的变体：**</mark>

**1.LeNet-5**

**2.AlexNet**

**3.GoogLeNet**

**4.VGGNet**

**5.ResNet**

**6.Xception**

**7.SENet**

8.DenseNet

9.MobileNet

10.CSPNet

11.EfficientNet

12.ResNeXt...

#### **1.LeNet-5:**

我们现在主要是用ReLU替代tanh,softmax代替RBF

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-14-19-45-07-image.png" alt="" width="534" data-align="center">

```
#经典LeNet-5实现：
from functools import partial

DefaultConv2D=partial(tf.keras.layers.Conv2D,kernel_size=5,padding="same",
                       activation="relu",kernel_initializer="he_normal")

model=tf.keras.Sequential([
           DefaultConv2D(filters=6,kernel_size=5,input_shape[32,32,3]),
           tf.keras.layers.MaxPool2D((2,2),strides=2),
           DefaultConv2D(filters=16,kernel_size=5),
           tf.keras.layers.MaxPool2D((2,2),strides=2),
           DefaultConv2D(filters=120,kernel_size=5),
           tf.keras.layers.Dense(units=84,activation="relu",
                                 kernel_initializer="he_normal"),
           tf.keras.layers.Dense(units=10,activation="softmax")])

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam",
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))
score = model.evaluate(X_test, y_test)
```

#### **2.AlexNet:**

为了减少过拟合，作者采用了两种正则化技术。首先，他们在训练过程中对第F9层和第F10层的输出应用了dropout技术，dropout率为50%。其次，他们通过随机对训练图像进行不同偏移量的平移、水平翻转以及改变光照条件等操作，对数据进行了增强处理。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-14-19-49-13-image.png" alt="" width="464" data-align="center">

```
from functools import partial


DefaultConv2D=partial(tf.keras.layers.Conv2D,kernel_size=3,padding="same",
                       activation="relu",kernel_initializer="he_normal")

model=tf.keras.Sequential([
       DefaultConv2D(filters=96,kernel_size=11,input_shape[227,227,3]),
       tf.keras.layers.MaxPool2D((3,3)),
       DefaultConv2D(filters=256,kernel_size=5),
       tf.keras.layers.MaxPool2D((3,3)),
       DefaultConv2D(filters=384),
       DefaultConv2D(filters=384),
       DefaultConv2D(filters=256),
       tf.keras.layers.MaxPool2D((3,3)),
       tf.keras.layers.Dense(units=4096,activation="relu",
                                 kernel_initializer="he_normal"),
       tf.keras.layers.Dropout(0.5),
       tf.keras.layers.Dense(units=4096,activation="relu",
                                  kernel_initializer="he_normal"),
       tf.keras.layers.Dropout(0.5),
       tf.keras.layers.Dense(units=1000,activation="softmax")])
])
```

#### **<mark>数据增强</mark>**：通过人工方式增加训练集规模，生成每个训练样本的多个真实变体实现，有助于减少过拟合，也是一种正则化技术

在keras中有如下增强方式：

<mark>1.随机翻转层</mark>：随机水平，垂直，或者同时翻转

```
tf.keras.layers.RandomFlip(
    mode='horizontal_and_vertical',  # 可选: 'horizontal', 'vertical'
    seed=None
)
```

<mark>2.随机旋转层</mark>：随机旋转图像，角度由factor控制，处理旋转后空白区域（通过填充）

- `factor=0.2`：旋转角度范围 ±0.2×360° = ±72°

- `fill_mode`：
  
  - `'constant'`：用固定值填充（fill_value）
  
  - `'reflect'`：镜像反射填充（推荐）
  
  - `'wrap'`：重复边缘像素

```
tf.keras.layers.RandomRotation(
    factor,  # 旋转范围 [-factor*2pi, factor*2pi]
    fill_mode='reflect',  # 填充模式: 'constant', 'reflect', 'wrap'
    interpolation='bilinear',
    seed=None,
    fill_value=0.0  # 常量填充时的值
)
```

<mark>3.随机缩放层</mark>：随机放大或缩小图像，处理缩放后空白区域

**参数示例**：

- `height_factor=(0.8, 1.2)`：随机缩放至80%-120%大小

- `height_factor=0.2`：缩放范围80%-120%（等价于(0.8, 1.2)）

```
tf.keras.layers.RandomZoom(
    height_factor,  # 高度缩放范围
    width_factor=None,  # 宽度缩放范围（默认同高度）
    fill_mode='reflect',
    interpolation='bilinear',
    seed=None,
    fill_value=0.0
)
```

<mark>4.随机裁剪层</mark>：随机裁剪指定大小的图像区域，强制模型关注局部特征

```
tf.keras.layers.RandomCrop(
    height,  # 裁剪高度
    width,   # 裁剪宽度
    seed=None
)
```

<mark>5.随机亮度/对比度调整</mark>：

```
# 随机亮度
tf.keras.layers.RandomBrightness(
    factor,  # 亮度调整范围 [-factor, factor]
    value_range=(0, 255),  # 输入值范围
    seed=None
)

# 随机对比度
tf.keras.layers.RandomContrast(
    factor,  # 对比度调整因子
    seed=None
)
```

<mark>6.随机平移层：</mark>随机上下左右平移图像

```
tf.keras.layers.RandomTranslation(
    height_factor,  # 高度平移范围
    width_factor,   # 宽度平移范围
    fill_mode='reflect',
    interpolation='bilinear',
    seed=None,
    fill_value=0.0
)
```

<mark>应用数据增强层：</mark>

```
from tensorflow.keras import layers, models

# 创建包含数据增强的模型
model = models.Sequential([
    # 数据增强层
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomContrast(0.3),

    # 特征提取层
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    # 分类层
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# 编译和训练模型
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=10)
```

**数据增强在处理不平衡数据集时也非常有用：可以利用它生成更少出现类别的更多样本，这种手法也称为SMOTE。**

#### **3.GoogLeNet:**

引入<mark>inception模块（增强版卷积层）</mark>：“3 × 3 + 1(S)”表示该层使用3 × 3核，步长1，以及“same”填充。(**所有卷积层均采用ReLU激活函数**),由于都是"same"填充，所以得到的特征图宽度和高度一致，在最后四个层进行合并时，能够沿着深度方向拼接，使用<mark>Concatenate()</mark>层即可

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-09-29-53-image.png" alt="" width="399" data-align="center">

**为什么要用这些1x1的层？**

作用：

1.对于1 × 1的卷积核，虽然无法捕捉空间模式上的特征，但是捕获深度维度上的模式（即跨通道的）

2.它们被配置为输出比输入更少的特征图，作为<mark>瓶颈层</mark>，减少维度，降低训练成本

3.[1x1,3x3],[1x1,5x5]这样的一对卷积层，能够捕捉更复杂的模式，相当于将两层神经网络扫过图像。所以，综上所述，可以把<mark>inception层当作增强版的卷积层</mark>。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-09-42-48-image.png" alt="" width="358" data-align="center">

```
import tensorflow as tf
from tensorflow.keras import layers, Model

class InceptionModule(layers.Layer):
    def __init__(self, filters_1x1, filters_3x3_reduce, filters_3x3, 
                 filters_5x5_reduce, filters_5x5, filters_pool, **kwargs):
        super(InceptionModule, self).__init__(**kwargs)

        # 1×1 卷积分支
        self.branch1 = tf.keras.Sequential([
            layers.Conv2D(filters_1x1, (1, 1), padding='same', activation='relu')
        ])

        # 1×1 + 3×3 卷积分支
        self.branch2 = tf.keras.Sequential([
            layers.Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu'),
            layers.Conv2D(filters_3x3, (3, 3), padding='same', activation='relu')
        ])

        # 1×1 + 5×5 卷积分支
        self.branch3 = tf.keras.Sequential([
            layers.Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu'),
            layers.Conv2D(filters_5x5, (5, 5), padding='same', activation='relu')
        ])

        # 3×3 最大池化 + 1×1 卷积分支
        self.branch4 = tf.keras.Sequential([
            layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same'),
            layers.Conv2D(filters_pool, (1, 1), padding='same', activation='relu')
        ])

    def call(self, inputs):
        branch1 = self.branch1(inputs)
        branch2 = self.branch2(inputs)
        branch3 = self.branch3(inputs)
        branch4 = self.branch4(inputs)

        # 在通道维度上拼接所有分支的输出
        return tf.concat([branch1, branch2, branch3, branch4], axis=-1)

def LRN(inputs):
    """局部响应归一化层 (Local Response Normalization)"""
    return tf.nn.local_response_normalization(
        inputs, depth_radius=5, bias=2, alpha=1e-4, beta=0.75
    )

def create_googlenet(input_shape=(224, 224, 3), num_classes=1000):
    """创建 GoogleNet (Inception v1) 模型"""
    inputs = layers.Input(shape=input_shape)

    # =============== 初始卷积层 ===============
    # 输入: 224x224x3
    x = layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu')(inputs)
    # 输出: 112x112x64

    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    # 输出: 56x56x64

    x = LRN(x)  # 局部响应归一化

    # =============== 1×1 卷积降维 ===============
    x = layers.Conv2D(64, (1, 1), padding='same', activation='relu')(x)
    # 输出: 56x56x64

    x = layers.Conv2D(192, (3, 3), padding='same', activation='relu')(x)
    # 输出: 56x56x192

    x = LRN(x)  # 局部响应归一化

    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    # 输出: 28x28x192

    # =============== Inception 模块组 1 ===============
    # Inception 3a
    x = InceptionModule(
        filters_1x1=64, 
        filters_3x3_reduce=96, filters_3x3=128,
        filters_5x5_reduce=16, filters_5x5=32,
        filters_pool=32
    )(x)
    # 输出: 28x28x(64+128+32+32)=256

    # Inception 3b
    x = InceptionModule(
        filters_1x1=128, 
        filters_3x3_reduce=128, filters_3x3=192,
        filters_5x5_reduce=32, filters_5x5=96,
        filters_pool=64
    )(x)
    # 输出: 28x28x(128+192+96+64)=480

    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    # 输出: 14x14x480

    # =============== Inception 模块组 2 ===============
    # Inception 4a
    x = InceptionModule(
        filters_1x1=192, 
        filters_3x3_reduce=96, filters_3x3=208,
        filters_5x5_reduce=16, filters_5x5=48,
        filters_pool=64
    )(x)
    # 输出: 14x14x(192+208+48+64)=512

    # Inception 4b
    x = InceptionModule(
        filters_1x1=160, 
        filters_3x3_reduce=112, filters_3x3=224,
        filters_5x5_reduce=24, filters_5x5=64,
        filters_pool=64
    )(x)
    # 输出: 14x14x(160+224+64+64)=512

    # Inception 4c
    x = InceptionModule(
        filters_1x1=128, 
        filters_3x3_reduce=128, filters_3x3=256,
        filters_5x5_reduce=24, filters_5x5=64,
        filters_pool=64
    )(x)
    # 输出: 14x14x(128+256+64+64)=512

    # Inception 4d
    x = InceptionModule(
        filters_1x1=112, 
        filters_3x3_reduce=144, filters_3x3=288,
        filters_5x5_reduce=32, filters_5x5=64,
        filters_pool=64
    )(x)
    # 输出: 14x14x(112+288+64+64)=528

    # Inception 4e
    x = InceptionModule(
        filters_1x1=256, 
        filters_3x3_reduce=160, filters_3x3=320,
        filters_5x5_reduce=32, filters_5x5=128,
        filters_pool=128
    )(x)
    # 输出: 14x14x(256+320+128+128)=832

    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
    # 输出: 7x7x832

    # =============== Inception 模块组 3 ===============
    # Inception 5a
    x = InceptionModule(
        filters_1x1=256, 
        filters_3x3_reduce=160, filters_3x3=320,
        filters_5x5_reduce=32, filters_5x5=128,
        filters_pool=128
    )(x)
    # 输出: 7x7x(256+320+128+128)=832

    # Inception 5b
    x = InceptionModule(
        filters_1x1=384, 
        filters_3x3_reduce=192, filters_3x3=384,
        filters_5x5_reduce=48, filters_5x5=128,
        filters_pool=128
    )(x)
    # 输出: 7x7x(384+384+128+128)=1024

    # =============== 分类头 ===============
    # 全局平均池化
    x = layers.GlobalAveragePooling2D()(x)
    # 输出: 1024

    # Dropout
    x = layers.Dropout(0.4)(x)

    # 全连接层
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    return Model(inputs, outputs, name='GoogleNet')

# 创建模型
model = create_googlenet()
model.summary()

# 编译模型
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
```

#### **4.VGGNet:**

它包含2或3个卷积层和一个池化层，随后再次包含2或3个卷积层和一个池化层，依此类推（根据VGG变体不同，总共包含16或19个卷积层），最后是一个包含2个隐藏层和输出层的全连接网络。它使用3×3的小滤波器，但数量众多。

#### **5.ResNet:**

引入<mark>残差网络</mark>：输入到某一层的信号也会被添加到堆栈中更高层的输出中。在训练神经网络时，目标是使其拟合目标函数h(x)。若将输入x与网络输出相加（即<mark>添加跳跃连接</mark>），网络将被迫拟合f(x) = h(x) – x而非h(x)。这被称为<mark>残差学习</mark>。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-09-56-22-image.png" alt="" width="430" data-align="center">

当初始化一个常规神经网络时，其权重接近于零，因此网络输出的值也接近于零。如果你添加一个跳跃连接，生成的网络会输出其输入的副本；换句话说，它最初模仿了恒等函数。如果目标函数与恒等函数相当接近（这种情况很常见），这将大大加快训练速度。

此外，如果你添加许多跳跃连接，网络即使在几个层尚未开始学习的情况下也能开始取得进展。由于跳跃连接，信号可以轻松地穿过整个网络。<mark>深度残差网络可以被视为一堆残差单元（RUs）</mark>，其中每个残差单元是<mark>一个带有跳跃连接的小型神经网络</mark>。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-10-01-07-image.png" alt="" width="440" data-align="center">

这里跳跃连接时会出现问题，为什么呢？因为假设到第四个蓝色卷积块处理完是一个32x32,通道数为64的特征图层，到下面第一个紫色卷积块之后，宽度和高度都减半了，后面处理又不改变宽度高度，所以当我们想跳跃连接到第二个紫色块的后面，会出现宽度高度不匹配的现象出现，所以就需要有如下操作，引入1x1卷积块+步长为2的卷积块：

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-10-06-13-image.png" alt="" width="419" data-align="center">

接下来手搓一个ResNet-34（包含3个64特征图层的残差单元，4个128特征图层的残差单元，6个256特征图层的残差单元，3个512特征图层的残差单元）

```
DefaultConv2D=partial(tf.keras.layers.Conv2D,kernel_size=3,strides=1,
                      padding="same",kernel_initializer="he_normal",
                      use_bias=False)
class ResidualUnit(tf.keras.layers.Layer):
      def __init__(self,filters,strides=1,activation="relu",**kwargs):
          super().__init__(**kwargs)
          self.activation=tf.keras.activations.get(activation)
          self.main_layers=[
             DefaultConv2D(filters,strides=strides),
             tf.keras.layers.BatchNormalization(),
             self.activation,
             DefaultConv2D(filters),
             tf.keras.layers.BatchNormalization()
                            ]
          self.skip_layers=[]
          if strides>1:
             self.skip_layers=[
                      DefaultConv2D(filters,kernel_size=1,strides=strides),
                      tf.keras.layers.BatchNormalization()     
                               ]
       def call(self,inputs):
           Z=inputs
           for layer in self.main_layers:
               Z=layer(Z)
           skip_Z=inputs
           for layer in self.skip_layers:
               skip_Z=layer(skip_Z)
           return self.activation(Z+skip_Z)  


model = tf.keras.Sequential([
    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation("relu"),
    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding="same"),
                           ])
prev_filters = 64
for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:
    strides = 1 if filters == prev_filters else 2
    model.add(ResidualUnit(filters, strides=strides))
    prev_filters = filters

model.add(tf.keras.layers.GlobalAvgPool2D())
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(10, activation="softmax"))
```

#### **6.Xception**

引入深度可分离卷积层。常规卷积层使用滤波器试图同时捕获空间模式和跨通道模式，而分离式卷积层则假设空间模式和跨通道模式可以分别建模：第一部分对每个输入特征图应用单个空间滤波器，然后第二部分仅搜索跨通道模式——是一个1x1滤波器。

### 第一步：深度卷积 (Depthwise Convolution)

- **目标**：只学习**空间模式**。

- **操作**：
  
  - 输入张量形状：`(Height, Width, Channels_in)` 或 `(H, W, M)`。
  
  - 使用 `M` 个独立的卷积核。**关键点：每个卷积核只负责处理输入张量的一个且仅一个通道！**
  
  - 每个卷积核的大小是 `(kernel_size, kernel_size, 1)`。例如，对于 3x3 卷积，核大小是 `(3, 3, 1)`。这里的 `1` 表示它只作用于单个通道。
  
  - 每个卷积核在其对应的**单个输入通道**上进行滑动卷积操作（计算点积），生成该通道对应的**单个输出特征图**。
  
  - **输出**：`M` 个特征图，每个特征图是单个输入通道的空间滤波结果。输出张量形状：`(H_out, W_out, M)`。注意，**通道数 `M` 没有改变！** 这一步没有混合任何通道信息。

- **计算量**：对于一个 `K x K` 的深度卷积核，处理一个 `H x W x M` 的输入，计算量约为 `K * K * M * H_out * W_out`。

### 第二步：逐点卷积 (Pointwise Convolution)

- **目标**：只学习**跨通道模式**。

- **操作**：
  
  - 输入张量形状：来自深度卷积的输出 `(H_out, W_out, M)`。
  
  - 使用 `N` 个标准的 `1x1` 卷积核。每个卷积核的大小是 `(1, 1, M)`。
  
  - 每个 `1x1` 卷积核在输入张量的**所有 `M` 个通道上**进行卷积操作（计算点积）。`1x1` 卷积核会遍历输入张量在空间位置 `(i, j)` 上的所有 `M` 个通道的值，将它们加权求和，生成输出张量在位置 `(i, j)` 的一个值。
  
  - **输出**：`N` 个特征图。输出张量形状：`(H_out, W_out, N)`。`1x1` 卷积有效地将 `M` 个通道的信息混合（线性组合）成 `N` 个新的通道。它不改变空间分辨率（因为核是 1x1，步长通常为 1）。

- **计算量**：处理一个 `H_out x W_out x M` 的输入，输出 `N` 个通道，计算量约为 `1 * 1 * M * N * H_out * W_out` = `M * N * H_out * W_out`。

**深度可分离卷积的总计算量** = `(K*K*M + M*N) * H_out * W_out`

深度卷积的输出 **直接作为** 逐点卷积的输入。这两个操作是**串联**的，中间没有额外的“合并”操作（如相加或拼接）。

```
from tensorflow.keras.layers import SeparableConv2D, Input

# 定义一个输入张量 (例如 32x32 RGB 图像)
inputs = Input(shape=(32, 32, 3))

# 使用深度可分离卷积层替代 Conv2D
# filters: 输出通道数 (N)
# kernel_size: 深度卷积的空间卷积核大小 (K)
x = SeparableConv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)

# 输出形状: (32, 32, 64) (假设 padding='same', strides=1)  

#上下等价！！！

# 第一步: Depthwise Convolution
x_depthwise = DepthwiseConv2D(kernel_size=3, strides=1, padding='same', use_bias=False)(inputs)
# 输出形状: (32, 32, 3) (M=3 个通道，每个通道做了独立的空间滤波)

# 第二步: Pointwise Convolution (1x1 Convolution)
x_pointwise = Conv2D(filters=64, kernel_size=1, strides=1, padding='valid', activation='relu')(x_depthwise)
```

#### **7.SENet:**

在inception和residual的基础上由增加了一个SE Block。SE 块分析其连接的单元输出，仅关注深度维度（不寻找任何空间模式），并学习哪些特征通常会一起活跃。然后，它使用这些信息来重新校准特征图。例如，一个 SE 块可能学习到嘴巴、鼻子和眼睛通常在图片中一起出现：如果你看到嘴巴和鼻子，你应该也看到眼睛。因此，如果块看到嘴巴和鼻子特征图的强激活，但眼睛特征图的激活较弱，它将增强眼睛特征图（更准确地说，它将减少无关特征图）。如果眼睛与其他物体有些混淆，这种特征图重新校准将有助于消除模糊性。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-11-11-26-image.png" alt="" width="418" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-11-16-24-image.png" alt="" width="349" data-align="center">

SE块由三层组成：全局平均池化层+使用ReLU激活函数的隐藏Dense层，以及使用sigmoid激活函数的密集输出层

### SE块核心原理

SE块是一种**通道注意力机制**，通过学习各特征通道的重要性权重，动态调整通道特征响应值。其工作流程分为三个关键步骤：

1. **Squeeze**（压缩）：
   
   - 通过全局平均池化（GAP）将每个通道的H×W空间特征压缩为单个标量
   
   - 公式：$z_c = \frac{1}{H \times W} \sum_{i=1}^H \sum_{j=1}^W x_c(i,j)$

2. **Excitation**（激励）：
   
   - 使用两个全连接层学习通道间依赖关系
   
   - 第一个FC降维（压缩比r），第二个FC恢复原始通道数
   
   - 公式：$s = \sigma(W_2\delta(W_1z))$  
     （$\sigma$: sigmoid, $\delta$: ReLU, $W_1 \in \mathbb{R}^{C/r \times C}$）

3. **Scale**（缩放）：
   
   - 将学习到的通道权重$s_c$与原始特征图逐通道相乘
   
   - 公式：$\tilde{x}_c = s_c \cdot x_c$

```
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Multiply, Reshape

def se_block(input_tensor, ratio=16):
    """Squeeze-and-Excitation块实现"""
    channels = input_tensor.shape[-1]

    # Squeeze: 全局平均池化
    x = GlobalAveragePooling2D()(input_tensor)
    x = Reshape((1, 1, channels))(x)

    # Excitation: 两个全连接层
    x = Dense(channels // ratio, activation='relu')(x)
    x = Dense(channels, activation='sigmoid')(x)

    # Scale: 通道加权
    return Multiply()([input_tensor, x]) 
```

```
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add

def residual_block_se(x, filters, kernel_size=3, stride=1, se_ratio=16):
    """带SE块的残差单元"""
    shortcut = x

    # 主路径
    x = Conv2D(filters, kernel_size, strides=stride, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2D(filters, kernel_size, padding='same')(x)
    x = BatchNormalization()(x)

    # 插入SE块 (在残差连接之前)
    x = se_block(x, ratio=se_ratio)

    # 残差连接
    if stride > 1 or shortcut.shape[-1] != filters:
        shortcut = Conv2D(filters, 1, strides=stride)(shortcut)
        shortcut = BatchNormalization()(shortcut)

    x = Add()([x, shortcut])
    return Activation('relu')(x)

# 使用示例
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input

inputs = Input(shape=(224, 224, 3))
x = residual_block_se(inputs, filters=64)
model = Model(inputs, x)
```

```
from tensorflow.keras.layers import Concatenate, MaxPooling2D

def inception_block_se(x, filters, se_ratio=16):
    """带SE块的Inception模块"""
    # 1x1 分支
    branch1x1 = Conv2D(filters[0], 1, activation='relu')(x)

    # 3x3 分支
    branch3x3 = Conv2D(filters[1], 1, activation='relu')(x)
    branch3x3 = Conv2D(filters[2], 3, padding='same', activation='relu')(branch3x3)

    # 5x5 分支
    branch5x5 = Conv2D(filters[3], 1, activation='relu')(x)
    branch5x5 = Conv2D(filters[4], 5, padding='same', activation='relu')(branch5x5)

    # 池化分支
    branch_pool = MaxPooling2D(3, strides=1, padding='same')(x)
    branch_pool = Conv2D(filters[5], 1, activation='relu')(branch_pool)

    # 合并分支
    x = Concatenate()([branch1x1, branch3x3, branch5x5, branch_pool])

    # 插入SE块 (在分支合并后)
    return se_block(x, ratio=se_ratio)

# 使用示例
inputs = Input(shape=(224, 224, 256))
x = inception_block_se(inputs, filters=[64, 96, 128, 16, 32, 32])
model = Model(inputs, x)
```

#### **8.其他架构：**

ResNeXt22：
ResNeXt对ResNet中的残差单元进行了改进。在最佳ResNet模型中，每个残差单元仅包含3个卷积层，而ResNeXt的残差单元由多个并行堆栈（例如32个堆栈）组成，每个堆栈包含3个卷积层ResNeXt22通过将每个堆栈的卷积层数量增加到16个，进一步优化了残差单元。然而，每个堆栈的前两层仅使用少量滤波器（例如仅四个），因此整体参数数量与ResNet保持一致。随后将所有堆栈的输出相加，并将结果传递给下一个残差单元（同时包含跳跃连接）。

DenseNet23：
DenseNet 由多个密集块组成，每个密集块包含几个密集连接的卷积层。这种架构在使用相对较少参数的情况下实现了卓越的准确性。什么是“密集连接”？每个层的输出会被作为输入传递给同一块内后续的所有层。例如，块中的第 4 层将第 1、2 和 3 层输出进行深度拼接后作为输入。密集块之间由几个过渡层分隔。

MobileNet24：
MobileNet 是一类经过优化的轻量级快速模型，因此在移动和网络应用中广受欢迎。它们基于深度可分离卷积层，与 Xception 类似。作者提出了几种变体，通过牺牲部分准确性来换取更快速和
更小的模型。

CSPNet25
跨阶段部分网络（CSPNet）与DenseNet类似，但每个密集块的输入的一部分直接与该块的输出进行拼接，而无需经过该块。

<mark>EfficientNet26</mark>
EfficientNet可能是本列表中最重要的模型。作者提出了一种高效扩展任何卷积神经网络的方法，通过以系统性方式同时增加深度（层数）、宽度（每层滤波器数量）和分辨率（输入图像大小）。这被称为复合缩放。他们使用神经网络架构搜索来寻找一个适合缩小版 ImageNet（图像更小且数量更少）的良好架构，然后使用复合缩放来创建越来越大的该架构版本。

## **四.Keras中预训练的模型**

可用的预训练模型参考下面的网站，都有详细说明~~~~

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-13-02-47-image.png" alt="" width="695" data-align="center">

以ResNet50为例：

```
#直接利用ImageNet上ResNet50的模型和权重
model=tf.keras.applications.ResNet50(weights="imagenet")

#调整输入的数据的大小
K=tf.keras.backend
images=K.constant(load_sample_images()["images"])
images_resized=tf.keras.layers.Resizing(height=224,width=224,
              crop_to_aspect_ratio=True)(images)

#每个API模型都有一个属于自己的preprocess_input()，可以直接用来加工数据
inputs=tf.keras.applications.resnet50.preprocess_input(images_resized)


Y_proba=model.predict(inputs)
Y_proba.shape

#输出数据属于某一类的概率，且是前三个最有可能的类别：
top_K=tf.keras.applications.resnet50.decode_predictions(Y_proba,top=3)
for image_index in range(len(images)):
    print(f"image {image_index}")
    for class_id,name,y_proba in top_K[image_index]:
        print(f"{class_id}-{name:12s} {y_proba:.2%}")
```

这里有个问题，就是我们利用ResNet50预测数据的两张图片，一张是ImageNet里的图像，一张不是，所以结果显示为一张预测正确，但是另一张预测错误，不属于ImageNet中1000个类别中的一个，所以肯定会预测错误（超出类别范围了...）。所以就需要用到下面的迁移学习了。

## **五.预训练模型——迁移学习**

这里直接上笔者在kaggle上运行的代码实例（没有GPU的儿~~~~）

```
import numpy as np 
import pandas as pd 

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import tensorflow_datasets as tfds

dataset,info=tfds.load("tf_flowers",as_supervised=True,with_info=True)
dataset_size=info.splits["train"].num_examples
class_names=info.features["label"].names
n_classes=info.features["label"].num_classes 


dataset_size

class_names

n_classes

import tensorflow as tf
test_set_raw,valid_set_raw,train_set_raw=tfds.load("tf_flowers",
                                                   split=["train[:10%]","train[10%:25%]","train[25%:]"],
                                                   as_supervised=True)
batch_size = 32
preprocess = tf.keras.Sequential([
    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),
    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)
])
train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))
train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)
valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)
test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)



data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip(mode="horizontal", seed=42),
    tf.keras.layers.RandomRotation(factor=0.05, seed=42),
    tf.keras.layers.RandomContrast(factor=0.2, seed=42)
])


base_model = tf.keras.applications.xception.Xception(weights="imagenet",
                                                      include_top=False)

# avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)
# output = tf.keras.layers.Dense(n_classes, activation="softmax")(avg)

full_model=tf.keras.Sequential([
    tf.keras.layers.Input(shape=(224,224,3)),
    data_augmentation,
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(n_classes,activation="softmax")
                      ])

# model = tf.keras.Model(inputs=base_model.input, outputs=output)

for layer in base_model.layers:
    layer.trainable = False


optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)
full_model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = full_model.fit(train_set, validation_data=valid_set, epochs=5)
```

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-18-51-34-image.png" alt="" width="610" data-align="center">

```
for layer in base_model.layers[56:]:
    layer.trainable = True

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)
full_model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = full_model.fit(train_set, validation_data=valid_set, epochs=20)
```

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-18-52-06-image.png" alt="" width="678" data-align="center">

```
full_model.evaluate(test_set)
```

<img src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-18-57-20-image.png" title="" alt="" width="690">

**<mark>迁移学习的结果一般般，差不多最好的是92%在验证集上，在测试集上达到94%的准确率。</mark>**

## **六.目标分类+定位（回归）：检测**

定位一个物体，其实只需要知道四个参数，一般来说矩形的四个角嘛，所以我们只需增加Dense层输出4个值即可

```
base_model = tf.keras.applications.xception.Xception(weights="imagenet",
                                                     include_top=False)
avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)

class_output = tf.keras.layers.Dense(n_classes, activation="softmax")(avg)

loc_output = tf.keras.layers.Dense(4)(avg)

model = tf.keras.Model(inputs=base_model.input,
                       outputs=[class_output, loc_output])

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)  

model.compile(loss=["sparse_categorical_crossentropy", "mse"],
              loss_weights=[0.8, 0.2],  # depends on what you care most about
              optimizer=optimizer, metrics=["accuracy", "mse"])
```

其实没有那么简单，首先需要解决的问题是：我们的flowers dataset并没有标记方格状的边界在花的周围，所以需要人工添加标签，可以使用的**开源图像标注工具**有：

1.VGG Image Annotator 

2.Labelimg

3.OpenLabeler

4.ImgLab

创建的数据集中每个项包含预处理后的图像标签及边界框的元组形式：（图像，类标签，边界框），当然边界框最好进行归一化处理，确保水平和垂直坐标以及高度宽度在0-1之间。

这里有个**评估模型边界框准确性的理想指标：IoU**，预测边界框与目标边界框重叠面积除以两者联合面积，在 Keras 中，IoU 由 tf.keras.metrics.MeanIoU 类实现。

**<mark>图像中多个物体的分类与定位任务被称为物体检测</mark>**。直到几年前，常见的做法是使用一个经过训练可对图像中**大致居中的单一物体进行分类和定位的卷积神经网络（CNN）**，然后将该CNN在图像上滑动，并在每个步骤进行预测。该CNN通常被训练为不仅预测类别概率和边界框，还预测一个对象存在分数：这是估计图像中确实包含一个位于中间附近的对象的概率。这是一个二元分类输出；它可以通过一个具有单个单元的密集输出层生成，使用sigmoid激活函数，并使用二元交叉熵损失进行训练。

**主要现在流行的是YOLO及其变体**，这里不再讨论了，感兴趣的可以自己去找找，对笔者这种菜菜，没到那种层次......

## **七.总结**

我们在构建自定义CNN架构时，选择用哪种网络，哪种架构（Inception,Resnet,SEblock...)取决于我们具体实际问题，模型的目标（精度，速度，大小）以及计算资源**残差模块是构建深层稳定网络的基石，Inception模块是高效捕获多尺度特征的利器，而SEBlock则是以极低成本显著提升特征质量的神奇插件**

残差模块：

  ①核心思想是解决深层网络训练中的梯度消失/爆炸/退化问题，通过引入跳跃连接，能够让网络学习输入与输出间的残差；

  ②优势是在训练极深的神经网络时能够提升模型性能上限，缓解梯度消失，通用性强

  ③应用场景当基础网络深度超过20层左右，需要构建很深的网络获得高精度时选择，优先选择作为构建时基础框架

Inception模块：

  ①核心思想是在同一层并行使用多种尺寸的卷积核和池化操作，以高效捕捉多尺度特征，利用1x1卷积核进行降维（减少计算量）和升维（增加非线性）

  ②优势是多尺度特征提取，计算效率高，参数利用率高

  ③应用场景是需要高效利用计算资源的深度网络，对特征尺度敏感的任务（如包含不同大小物体的图像分类）

  ④优先选择：计算资源为主要瓶颈，构建网络适中，希望单层内有效融合不同尺度信息，任务中物体尺度变化很大

SEBlock模块：

  ①核心思想是引入通道注意力机制

  ②优势是显著提升模型性能，即插即用（方便集成到现有基础模块），提高特征表示能力

  ③应用场景是几乎任何CNN架构的增强插件，需要进一步提升现有模型精度

  ④有一个基础网络，希望以最小代价提升性能，计算资源相对紧张

| 特性        | 残差模块 (ResBlock)                                       | Inception模块                                            | SEBlock                                                   |
| --------- | ----------------------------------------------------- | ------------------------------------------------------ | --------------------------------------------------------- |
| **核心目标**  | **解决深度网络的训练难题**                                       | **高效捕获多尺度特征**                                          | **通道注意力（特征校准）**                                           |
| **主要优势**  | 训练极深网络，缓解梯度消失，通用                                      | 多尺度、高计算效率、参数高效                                         | **即插即用、显著提点、开销极小**                                        |
| **主要开销**  | 增加少量参数和计算（跳跃连接）                                       | 并行路径增加宽度，但1x1卷积优化                                      | **增加极少参数和计算(<1%)**                                        |
| **典型应用**  | ResNet及变种，几乎所有现代CNN骨架                                 | GoogLeNet/Inception系列                                  | SEResNet, MobileNetV3, **插件**                             |
| **优先选择当** | 1. 网络**很深** (>20层)<br>2. 训练困难<br>3. 追求**最高精度** (资源允许) | 1. 需要**单层多尺度融合**<br>2. **计算效率**是关键约束<br>3. 物体**尺度变化大** | 1. 需要**最小代价提升**现有模型<br>2. **资源受限**但需更好精度<br>3. 作为**增强插件** |

## 组合使用

在实践中，这些模块常常被**组合使用**以获得最佳效果，这体现了现代CNN架构设计的模块化思想：

1. **Residual Inception (Inception-ResNet)：** 将Inception模块中的并行卷积路径的输出与原始输入相加，结合了多尺度特征提取和残差学习的优点。Google的Inception-ResNet系列是典型代表。

2. **SE-ResNet / SE-Inception：** 在ResBlock或Inception模块内部或后面添加SEBlock，让网络在利用残差或多尺度能力的同时，还能自适应地校准通道特征的重要性。SENet及其变种（如SEResNeXt）展示了这种组合的强大效果。

3. **MobileNetV3：** 同时使用了基于深度可分离卷积的类Inception结构、残差连接以及SEBlock（称为NL，Non-Linearities with SE），是轻量级网络设计的典范。

## 决策流程建议

1. **确定深度需求：** 如果任务复杂需要非常深的网络 -> **ResBlock是基础骨架**。

2. **评估计算预算：**
   
   - 预算充足：在ResBlock骨架上，可以添加**Inception结构**（形成Res-Inception）增强多尺度能力，或/和添加**SEBlock**提升特征质量。
   
   - 预算紧张：考虑以**高效Inception结构**（或类似思想如MobileNet的倒残差块）为基础骨架 -> 再考虑是否添加**SEBlock**（通常值得加，开销小收益大）。

3. **任务特性：**
   
   - 物体尺度变化大 -> 考虑引入**Inception结构**。
   
   - 特征通道重要性差异可能大 -> 添加**SEBlock**。

4. **即插即用提升：** 如果你已经有一个不错的基线模型（无论基于ResBlock还是Inception），尝试在关键位置插入**SEBlock**通常是提升精度最快捷且代价最低的方法。

5. **实验验证：** 最终选择哪个或哪些模块，最可靠的方法是在你的**具体任务和数据集上进行消融实验**，比较不同组合在精度、速度和模型大小上的trade-off。
