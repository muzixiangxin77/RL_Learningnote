# **RNNs:循环神经网络**

1.RNN基本概念以及如何用时间反向传播算法训练

2.预测时间序列，ARMA家族，两者比较

3.RNN面临问题：不稳定梯度（递归式dropout,递归式归一化）和有限的短期记忆（LSTM,GRU拓展）

4.WaveNet（超强处理数万个时间序列的CNN架构）

## **一.循环神经网络的神经元和层：**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-19-45-36-image.png" alt="" width="542" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-19-47-10-image.png" alt="" width="560" data-align="center">

每个神经元有两个权重：一个是来自x(t)的输入，一个是来自上个时间的输出y(t-1)；权重分别为$w_x,w_y$ ，公式如下：

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-19-50-29-image.png" alt="" data-align="center" width="367">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-20-02-28-image.png" alt="" width="545" data-align="center">

由于递归神经元在时间步 t 的输出是所有先前时间步输入的函数，可以说它具有某种记忆形式。神经网络中在不同时间步之间保持某些状态的部分被称为记忆单元（或简称为单元）。单个递归神经元或一层递归神经元是最基本的单元，仅能学习短时序列模式（通常约10个时间步长，但具体取决于任务）。

细胞在时间步 t 时的状态，记为 h(t)（“h”代表“隐藏”），是该时间步某些输入及其前一时间步状态的函数：h(t)= f(x(t),h(t–1))。其在时间步 t 时的输出，记为 ŷ(t)，也是前一状态和当前输入的函数。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-20-04-56-image.png" alt="" width="481" data-align="center">

循环神经网络（RNN）可以同时处理一组输入序列并生成一组输出序列（参见图左上角的网络）。这种**序列到序列的网络**对于预测时间序列非常有用，

1.例如，你家每天的电力消耗：你向它输入过去N天的数据，并训练它输出向未来推移一天的电力消耗（即从N-1天前到明天）。

2.例如，你可以向网络输入序列数据，并忽略所有输出，只保留最后一个输出（参见图右上角的网络）。这是**序列到向量网络**。

3.例如，你可以向网络输入对应于电影评论的词序列，网络将输出情感评分（例如从0[讨厌]到1[喜欢]）。相反，你可以让网络在每个时间步重复输入相同的向量，并让其输出一个序列（参见图左下角的网络）。这是**向量到序列**网络。

4.例如，输入可以是一张图像（或CNN的输出），输出可以是该图像的字幕。最后，你可以**使用一个序列到向量网络（称为编码器）你向网络输入一种语言的句子，编码器会将该句子转换为单一向量表示，然后解码器会将该向量解码为另一种语言的句子。这种两步模型，称为编码器-解码器**。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-20-10-23-image.png" alt="" width="482" data-align="center">

**训练 RNN 的关键**<mark>在于将其展开到时间维度（就像我们刚才所做的那样），然后使用常规的反向传播。这种策略被称为时间反向传播（BPTT）</mark>。与常规反向传播类似，首先对展开后的网络进行一次正向传播（用虚线箭头表示）。然后<mark>使用损失函数 ℒ(Y(0), Y(1), …, Y(T); Ŷ(0), Ŷ(1),…, Ŷ(T)) </mark>对输出序列进行评估（其中 Y (i)是第i个目标，Ŷ(i)是第i个预测，T是最大时间步长）。注意，**该损失函数可能忽略某些输出**。例如，在序列到向量RNN中，除了最后一个输出外，所有输出均被忽略。在图中，损失函数仅基于最后三个输出计算。该损失函数的梯度随后通过展开的网络向后传播（由实心箭头表示）。在此示例中，**由于输出 Ŷ(0) 和 Ŷ(1) 未用于计算损失，梯度不会通过它们向后传播**；它们仅通过 Ŷ(2)、Ŷ(3) 和 Ŷ(4) 传播。此外，由于每个时间步使用的参数W 和 b 相同，它们的梯度将在反向传播过程中被多次调整。一旦反向传播阶段完成且所有梯度计算完毕，BPTT 可以执行梯度下降步骤来更新参数（这与常规反向传播并无不同）。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-20-17-55-image.png" alt="" width="405" data-align="center">

## **二.预测时间序列——基础**

以一个实例入手：

```
filepath = tf.keras.utils.get_file(
    "ridership.tgz",
    "https://github.com/ageron/data/raw/main/ridership.tgz",
    cache_dir=".",
    extract=True
)
if "_extracted" in filepath:
    ridership_path = Path(filepath) / "ridership"
else:
    ridership_path = Path(filepath).with_name("ridership")  


import pandas as pd
from pathlib import Path

path = Path("datasets/ridership/CTA_-_Ridership_-_Daily_Boarding_Totals.csv")
df = pd.read_csv(path, parse_dates=["service_date"])
df.columns = ["date", "day_type", "bus", "rail", "total"]  # shorter names

#这一步很重要！！！需要把时间作为索引，很多时间序列预测项目处理都需要做的一步：
df = df.sort_values("date").set_index("date")

df = df.drop("total", axis=1)  # no need for total, it's just bus + rail
df = df.drop_duplicates()  # remove duplicated months (2011-10 and 2014-07) 
```

```
df.head()
```

```
import matplotlib.pyplot as plt

df["2019-03":"2019-05"].plot(grid=True, marker=".", figsize=(8, 3.5))

plt.show()
```

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-20-24-56-image.png" alt="" width="624" data-align="center">

从这里我们能够清晰的观察到周期性变化，每周的变化基本一致。这时出现了一种极其简单的预测方式就是**简单预测（naive forecasting）**：只需复制一周前的值来预测明天的客流量，就能得到相当好的结果了。

接下去我们来看看是否达到不错的效果：将两个时间序列（公交和铁路）以及向右移动偏移一周的序列显示一下（差分）：

```
diff_7=df[["bus","rail"]].diff(7)["2019-03":"2019-05"]


fig,axs=plt.subplots(2,1,sharex=True,figsize=(8,5))

df.plot(ax=axs[0],legend=False,marker=".")

df.shift(7).plot(ax=axs[0],legend=True,marker=":")

diff_7.plot(ax=axs[1],grid=True,marker=".")

plt.show()
```

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-15-20-32-30-image.png" alt="" width="570" data-align="center">

**<mark>当时间序列与其滞后版本相关时，我们称该时间序列具有自相关性</mark>**

```
#MAE(平均绝对误差)
diff_7.abs().mean()


#MAPE(平均绝对百分比误差)
targets=df[["bus","rail"]]["2019-03":"2019-05"]
(diff_7/targets).abs().mean()
```

根据书上所说，我们对于时间序列，**需要先看是否有周期性，因为有周期性能够让我们时间序列预测更加精确，周期性分为：周，月，年**，从上面的显示来看，数据有明显的周 季节性 ，但是没有月度季节性，现在查看是否有年度季节性。

```
period=slice("2001","2019")

df_monthly=df.select_dtypes(include="number").resample("ME").mean()#计算每月的平均值
rolling_average_12_months=df_months.loc[period].rolling(window=12).mean()

fig,ax=plt.subplots(figsize=(8,4))
df_monthly[period].plot(ax=ax,marker=".")
rolling_average_12_months.plot(ax=ax,grid=True,legend=False)

plt.show()
```

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-19-10-29-00-image.png" alt="" width="563" data-align="center">

确实存在一定的年度季节性，尽管它比周季节性更不稳定，且在铁路系列中比公交系列更明显：我们可以看到每年大约在同一时间出现峰值和谷值。让我们看看如果绘制12个月的差值会得到什么结果（见图）

```
df_monthly.diff(12)[period].plot(grid=True, marker=".", figsize=(8, 3))
plt.show()
```

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-19-10-30-09-image.png" alt="" data-align="center" width="618">

**差分不仅消除了年度季节性，还消除了长期趋势**。例如，2016年至2019年时间序列中存在的线性下降趋势在差分时间序列中变成了一个大致恒定的负值。事实上，差分是一种常用的技术，用于去除时间序列中的趋势和季节性：研究一个平稳的时间序列更容易，即一个其统计属性随时间保持恒定的时间序列，没有季节性或趋势。一旦你能够对差分时间序列进行准确的预测，只需将之前减去的过去值加回去，就可以轻松地将它们转换为对实际时间序列的预测。

## **三.ARMA家族预测时间序列**

① P阶AR：我们的**AR过程在ACF图中显示出逐渐减少的趋势，因为作为一个AR过程，其当前与过去的滞后项具有良好的相关性**。我们期望**PACF在滞后项阶数后会急剧下降，因为这些接近当前项的滞后项可以很好地捕获变化，因此我们不需要很多过去的滞后项来预测当前项**

$$
\hat{y}_{(t)}=\sum_{i=1}^{p}\alpha_{i}y_{t-i}+c+\epsilon_t
$$

② q阶MA：**ACF 图能够画出相邻的滞后项之间的良好的相关关系，并且在阶数q 之后迅速下降（因为这不是一个AR 过程，因此和过去的滞后项没有相关关系）**。同样，我们也可以看到PACF 逐渐下降，临近的滞后项并不能预测当前项（不同于AR 过程）；而更远的滞后项有可能有良好的相关关系。

$$
\hat{y}_{(t)}=\sum_{i=1}^{q}\theta_{i}\epsilon_{t-i}+c+\epsilon_t
$$

③ ARMA：模型假设时间序列是平稳的，如果不是那么可以尝试差分

$$
\hat{y}_{(t)}=\sum_{i=1}^{p}\alpha_{i}y_{t-i}+\sum_{i=1}^{q}\theta_{i}\epsilon_{t-i}
,\epsilon_{t}=y_{(t)}-\hat{y}_{t}
$$

 $\hat{y}_{(t)}$是时间t的预测值，$y_{(t)}$是时间t的实际值，第一个求和项是时间序列过去P个值得加权和，使用的权重$\alpha_{i}$，参数P是超参数，决定模型回溯到过去多远的时间点，整个求和项是模型的**自回归成分**：基于过去的回归

第二个和是过去q个预测误差的加权和,q是超参数，整个和是模型的**移动平均成分**

**对单个时间步长进行差分将产生时间序列导数的近似值**：事实上，它将给出序列在每个时间步长的斜率。这意味着**它将消除任何线性趋势**，将其转化为常数值。例如，若对序列 [3, 5, 7, 9, 11] 应用一步差分，将得到差分序列 [2, 2, 2, 2]。若原始时间序列具有二次趋势而非线性趋势，则单次差分处理将不足以消除趋势。例如，序列[1, 4, 9, 16, 25, 36] 在进行一次差分后变为 [3, 5, 7, 9, 11]，但如果进行第二次差分，则得到 [2, 2, 2, 2]。因此，进行两次差分将消除二次趋势。更一般地，**连续进行d轮差分运算相当于计算时间序列的d阶导数近似值，因此可消除最高至d阶的多项式趋势。该超参数d被称为积分阶数。**

<mark>**差分运算是自回归积分移动平均（ARIMA）模型的核心贡献**</mark>，该模型由乔治·博克斯（George Box）和吉尔姆·詹金斯（Gwilym Jenkins）于1970年在吉尔姆·詹金斯在其著作《时间序列分析》（Wiley）中提出：**该模型通过进行d轮差分使时间序列更具平稳性，随后应用常规ARMA模型**。在进行预测时，它使用该ARMA模型，并恢复差分过程中被扣除的项。

ARMA家族的最后一个成员是**季节性ARIMA（SARIMA）模型**：它与ARIMA模型以相同方式建模时间序列，**但额外建模给定频率（如每周）的季节性成分**，采用完全相同的ARIMA方法。它共有七个超参数：与ARIMA相同的p、d和q超参数，以及**额外的P、 D 和 Q 超参数用于建模季节性模式**，以及**季节性模式的周期 s**。超参数 P、D 和 Q 与 p、d 和 q类似，但用于建模时间序列在t – s、t – 2s、t – 3s 等时刻的值。
④ **SARIMA 模型**认为时间序列包含**两种模式**：

1. **短期模式**：相邻时间点之间的关系（用 `p,d,q` 建模）

2. **周期性模式**：固定周期重复出现的模式（用 `P,D,Q,s` 建模）

| 参数        | 非季节性参数                     | 季节性参数                                                                    | 关键区别                                       |
| --------- | -------------------------- | ------------------------------------------------------------------------ | ------------------------------------------ |
| **p / P** | 自回归阶数<br>(用过去 `p` 天的值预测今天) | **季节性自回归阶数**<br>(用过去 `P` **个周期**的值预测今天)<br>例：`P=1` 表示用 *去年同期的值* 预测今年     | 时间尺度不同：<br>- `p` 看前几天<br>- `P` 看前几个 *完整周期* |
| **d / D** | 普通差分次数<br>(消除短期趋势)         | **季节性差分次数**<br>(消除周期性趋势)<br>例：`D=1` 时：<br>`y_t' = y_t - y_{t-s}`         | 差分对象不同：<br>- `d` 消除短期波动<br>- `D` 消除季节效应    |
| **q / Q** | 移动平均阶数<br>(用过去 `q` 天的误差修正) | **季节性移动平均阶数**<br>(用过去 `Q` **个周期**的误差修正)<br>例：`Q=1` 表示用 *去年同期预测误差* 修正当前预测 | 误差修正尺度不同                                   |
| **s**     | -                          | **季节周期长度**<br>数据重复的基本单位                                                  | 定义季节性的“节奏”                                 |

让我们看看如何将 SARIMA 模型拟合到铁路时间序列，并使用它来预测明天的客流量。假设今天是2019年5月最后一天，我们希望预测“明天”即2019年6月1日的铁路客流量。为此，我们可以使用包含多种**统计模型的statsmodels库**，其中包括ARMA模型及其变体，由ARIMA类实现：

```
from statsmodels.tsa.arima.model import ARIMA

origin,today="2019-01-01","2019-05-31"
rail_series=df.loc[origin:today]["rail"].asfreq("D")#并使用asfreq(“D”)将
                                                   时间序列的频率设置为每日
model=ARIMA(rail_series,order=(1,0,0),seasonal_order=(0,1,1,7)) 

#statsmodels 接口与 Scikit-Learn 接口略有不同，因为我们在构造模型时传递数据，
                                           而非在 fit() 方法中传递。
model=model.fit()
y_pred=model.forecast()
y_pred[0]


#如果需要预测多天（例如未来7天），可显式指定步数
y_pred_multi = model.forecast(steps=7)  # 预测2019-06-01至2019-06-07
```

- **s=7**：数据以 **7天为周期**（周循环）

- **D=1**：进行一阶季节性差分 → 计算 **本周与上周的差值**  
  `差分序列 = y_t - y_{t-7}`

- **Q=1**：使用 **前一个周期（7天前）的预测误差** 来修正当前预测

- **P=0**：不考虑季节性自回归项,P=1时代表，包含y_{t-7}的项
  
  滚动预测2019-03-01到2019-5-31的每一天的预测

```
origin,start_date,end_date="2019-01-01","2019-03-01","2019-05-31"
time_period=pd.date_range(start_date,end_date)#预测的目标日期
rail_series=df.loc[origin:end_date]["rail"].asfreq("D")
y_preds=[]
for today in time_period.shift(1):#将日期序列向后移动一天
    model=ARIMA(rail_series[origin:today],
                            order=(1,0,0),
                            seasonal_order=(0,1,1,7))
    model=model.fit()
    y_pred=model.forest()[0]
    y_preds.append(y_pred)
y_preds=pd.Series(y_preds,index=time_period)
MAE=(y_preds-rail_series[time_period]).abs().mean()


fig, ax = plt.subplots(figsize=(8, 3))
rail_series.loc[time_period].plot(label="True", ax=ax, marker=".", grid=True)
ax.plot(y_preds, color="r", marker=".", label="SARIMA Forecasts")
plt.legend()
plt.show()
```

为SARIMA模型选取合适的超参数：**网格搜索，随机搜索；良好的p、q、P和Q值通常相当小（通常为0到2，有时可达5或6），而d和D通常为0或1，有时为2。至于s，它只是主要季节性模式的周期：在我们的案例中为7，因为存在强烈的周季节性**。

<mark>**ACF,PACF**</mark>

**ACF**：**完整的自相关函数**描述一个观测值与其过去值的相关程度。时间序列可以包含趋势，季节性，周期性和残差等成分。ACF在寻找相关性时会考虑所有这些成分。

**PACF**：**部分自相关函数**或者偏自相关函数。基本上，它不是找到像ACF这样的滞后与当前的相关性，而是找到**残差**（在去除了之前的滞后已经解释的影响之后仍然存在）**与下一个滞后值的相关性**。因此，如果残差中有任何可以由下一个滞后建模的隐藏信息，我们可能会获得良好的相关性，并且在建模时我们会将下一个滞后作为特征。

**截尾**是指时间序列的自相关函数（ACF）或偏自相关函数（PACF）在某阶后均为0的性质（比如AR的PACF）

**拖尾**是ACF或PACF并不在某阶后均为0的性质（比如AR的ACF）。

**截尾：在大于某个常数k后快速趋于0为k阶截尾
拖尾：始终有非零取值，不会在k大于某个常数后就恒等于零(或在0附近随机波动)**

###### 定阶核心作用

#### 1. 识别AR(p)模型阶数

- **PACF**：在滞后p阶后**截尾**

- **ACF**：**拖尾**（逐渐衰减）

- **判定依据**：PACF最后一个显著超出置信区间的滞后阶数

*示例*：若PACF在lag3后截尾 → AR(3)模型

#### 2. 识别MA(q)模型阶数

- **ACF**：在滞后q阶后**截尾**

- **PACF**：**拖尾**（逐渐衰减）

- **判定依据**：ACF最后一个显著超出置信区间的滞后阶数

*示例*：若ACF在lag2后截尾 → MA(2)模型

#### 3. 识别ARMA(p,q)模型

- **ACF和PACF都拖尾**

- 需要同时考虑两种图形模式

#### 4. 季节性模型识别

- **关键点**：观察滞后阶数为周期倍数时的峰值

- 例如 s=12（月度数据）：
  
  - ACF在lag12,24处显著 → 季节性MA成分
  
  - PACF在lag12,24处显著 → 季节性AR成分

```
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))
plot_acf(df[period]["rail"], ax=axs[0], lags=35)
axs[0].grid()  

plot_pacf(df[period]["rail"], ax=axs[1], lags=35, method="ywm")
axs[1].grid()
plt.show()
```

## **四.机器学习模型——基本RNN**

从上述案例中，我们有了两个基准模型，即**简单预测和SARIMA模型**，接下去我们用机器学习模型来预测时间序列。

首先从一个基本的线性模型开始。我们的目标是基于过去8周（56天）的客流量数据来预测明天的客流量。因此，模型的输入将是序列（通常在模型投入生产后，每天仅使用一个序列），每个序列包含从时间步长 t – 55 到 t 的 56 个值。对于每个输入序列，模型将输出一个值：时间步长 t + 1 的预测值。这里就会出现一个问题：我们用什么作为训练数据？按理来说我们将使用过去的每个56天窗口作为训练数据，每个窗口的目标值将是其紧随其后的值，Keras提供了一个便捷的实用函数，用于帮助我们准备训练集。

```
import tensorflow as tf

my_series=[0,1,2,3,4,5]
my_dataset=tf.keras.utils.timeseries_dataset_from_array(
                my_series,
                target=my_series[3:],
                sequence_length=3,
                batch_size=2
)
```

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-19-15-13-54-image.png" alt="" width="589" data-align="center">

```
rail_train = df["rail"]["2016-01":"2018-12"] / 1e6
rail_valid = df["rail"]["2019-01":"2019-05"] / 1e6
rail_test = df["rail"]["2019-06":] / 1e6    


seq_length = 56
tf.random.set_seed(42)  
train_ds = tf.keras.utils.timeseries_dataset_from_array(
    rail_train.to_numpy(),
    targets=rail_train[seq_length:],
    sequence_length=seq_length,
    batch_size=32,
    shuffle=True,
    seed=42
)
valid_ds = tf.keras.utils.timeseries_dataset_from_array(
    rail_valid.to_numpy(),
    targets=rail_valid[seq_length:],
    sequence_length=seq_length,
    batch_size=32
)       

tf.random.set_seed(42)
model=tf.keras.Sequential([
 tf.keras.layers.Dense(1,input_shape=[seq_length])])

early_stopping_cb=tf.keras.callbacks.EarlyStopping(
        monitor="val_mae",patience=50,restore_best_weights=True
) 

opt=tf.keras.optimizers.SGD(learning_rate=0.02,momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(),optimizer=opt,metrics=["mae"])

history=model.fit(train_ds,validation_data=valid_ds,
                   epochs=500,callbacks=[early_stopping_cb])
```

**为什么选择Huber损失？**

- **鲁棒性**：Huber损失是MAE（绝对误差）和MSE（平方误差）的折衷
  
  - 当误差较小时（|δ| < δ），使用平方项（类似MSE）
  
  - 当误差较大时（|δ| ≥ δ），使用线性项（类似MAE）

- **优势对比**：
  
  | 损失函数      | 异常值敏感性   | 梯度行为     | 适用场景       |
  | --------- | -------- | -------- | ---------- |
  | MSE       | 高敏感      | 梯度大      | 数据干净无异常值   |
  | MAE       | 不敏感      | 恒定梯度     | 存在异常值      |
  | **Huber** | **中等敏感** | **平滑过渡** | **通用回归问题** |

**为什么选择MAE？**

- **直观解释**：平均绝对误差，单位与原始数据相同  
  $ \text{MAE} = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i| $

- **优势**：
  
  - 不受异常值过度影响（相比MSE）
  
  - 更符合业务直觉（平均偏差量）

- **对比MSE/RMSE**：
  
  | 指标      | 计算公式                               | 特点       | 适用场景       |
  | ------- | ---------------------------------- | -------- | ---------- |
  | MSE     | $\frac{1}{n}\sum(y-\hat{y})^2$     | 放大大误差    | 强调大误差的惩罚   |
  | RMSE    | $\sqrt{\text{MSE}}$                | 单位一致     | 需要与数据同单位   |
  | **MAE** | **$\frac{1}{n}\sum\|y-\hat{y}\|$** | **鲁棒性强** | **通用回归评估** |

**为什么这样设置早停策略？**

1. **监控指标选择**：
   
   - `val_mae` > `val_loss`：更直观反映模型实际性能
   
   - `val_loss`可能受Huber损失内部机制影响，不如MAE透明

<mark>接下来我们使用基础循环神经网络尝试解决问题：</mark>

**首先需要解释一下输入的问题：Keras中的所有递归层都期望接受形状为[批量大小，时间步长，维度]的3D输入，其中维度对于一维时间序列为1，对于多维时间序列则更大。input_shape参数会忽略第一维度（即批量大小），又由于循环层可以接受任意长度的输入序列，我们可以将第二维度设为None,意味着任意大小。最后由于我们处理的是单变量时间序列，因此最后维度是1.**

**默认情况下，Keras中的循环层仅返回最终输出。若要使其返回每个时间步的输出，必须设置return_sequences=True**

```
def fit_and_evaluate(model, train_set, valid_set, learning_rate, epochs=500):

    early_stopping_cb = tf.keras.callbacks.EarlyStopping(
        monitor="val_mae", patience=50, restore_best_weights=True)

    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)

    model.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=["mae"])

    history = model.fit(train_set, validation_data=valid_set, epochs=epochs,
                        callbacks=[early_stopping_cb])

    valid_loss, valid_mae = model.evaluate(valid_set)

    return valid_mae * 1e6 
```

```
tf.random.set_seed(42)  
model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(1, input_shape=[None, 1])
]) 

fit_and_evaluate(model, train_ds, valid_ds, learning_rate=0.02)
```

初始状态 h(init) 被设置为 0，并被传递给单个递归神经元，同时传递第一个时间步的值 x(0)。神经元计算这些值的加权和加上偏置项，并**应用激活函数对结果进行处理，默认使用双曲正切函数**tanh()。结果即为第一个输出 y0。在简单的 RNN 中，此输出也是新的状态 h0。此新状态与下一个输入值 x(1) 一起传递给相同的递归神经元，并重复此过程直至最后一个时间步。最后，该层仅输出最后一个值：在本例中序列长度为 56 步，因此最后一个值为 y55。这是一个序列到向量的模型。

**结果肯定很差，为什么呢**？

一：该模型就一个神经元，每个时间步预测时，它只能使用当前时间步的输入值和上一个时间步的输出值。换句话说就是，RNN的记忆能力极其有限，仅能记住一个数字，即上一个时间步的输出值，所以模型一共3个参数。而我们之前的模型就已经有57个参数值了。

二：时间序列中的值范围从0到约1.4，但由于默认激活函数为tanh，递归层只能输出-1到+1之间的值。它无法预测1.0到1.4之间的值。

#### 数据流维度变化(batch_size=32)

| 阶段    | 形状                            | 说明                      |
| ----- | ----------------------------- | ----------------------- |
| 输入数据  | (32, 56, 1)                   | 32个样本，每个样本56个时间步，每步1个特征 |
| RNN处理 | 每个时间步：(32, 1) → RNN → (32, 1) | 隐藏状态维度=1                |
| 最终输出  | (32, 1)                       | 最后一个时间步的RNN输出           |

```
tf.random.set_seed(42)
univar_model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, input_shape=[None, 1]),
    tf.keras.layers.Dense(1)  
])

fit_and_evaluate(univar_model, train_ds, valid_ds, learning_rate=0.05)
```

上面我们采用32个神经元，让模型捕捉更多维的时间趋势，清晰分离不同模式，效果肯定会更好！

<mark>使用深层RNN解决</mark>

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-19-16-41-28-image.png" alt="" width="490" data-align="center">

```
tf.random.set_seed(42)  
deep_model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]),
    tf.keras.layers.SimpleRNN(32, return_sequences=True),
    tf.keras.layers.SimpleRNN(32),
    tf.keras.layers.Dense(1)
])

fit_and_evaluate(deep_model, train_ds, valid_ds, learning_rate=0.01)
```

#### return_sequences 的作用

| 参数值          | 输出               | 用途场景    |
|:------------:| ---------------- | ------- |
| `False` (默认) | 仅最后时间步的输出 (32,)  | 序列到单值预测 |
| `True`       | 所有时间步的输出 (T, 32) | 序列到序列处理 |

#### 数据流维度变化 (以单个样本为例)

假设输入序列：56个时间步，每步1个特征 → 形状 `(56, 1)`

#### 1. **第一层：`SimpleRNN(32, return_sequences=True)`**

- **输入**：`(56, 1)`

- **处理**：
  
  - 对每个时间步t∈[1,56]：
    
    - 计算32个神经元的输出
  
  - **保留所有时间步的输出**

- **输出**：`(56, 32)`  
  (56个时间步 × 每步32个特征)

#### 2. **第二层：`SimpleRNN(32, return_sequences=True)`**

- **输入**：`(56, 32)` (来自第一层)

- **处理**：
  
  - 对每个时间步t∈[1,56]：
    
    - 接收前一层的32维特征
    
    - 计算新的32维特征
  
  - **继续保留所有时间步输出**

- **输出**：`(56, 32)`  
  (时间步数不变，特征更新)

#### 3. **第三层：`SimpleRNN(32)` (默认return_sequences=False)**

- **输入**：`(56, 32)` (来自第二层)

- **处理**：
  
  - 对每个时间步t∈[1,56]：
    
    - 计算32个神经元的输出
  
  - **仅保留最后时间步(t=56)的输出**

- **输出**：`(32,)`  
  (整个序列的摘要向量)

#### 4. **输出层：`Dense(1)`**

- **输入**：`(32,)`

- **输出**：`(1,)` (预测值)

<mark>处理多变量时间序列：</mark>每一个时间步上，不只是有过去时间段的数值，还有日期类型（3个，工作日，周末，节假日），公交数据客流量

```
df_mulvar=df[["bus","rail"]]/1e6
df_mulvar["next_day_type"]=df["day_type"].shift(-1)
df_mulvar=pd.get_dummies(df_mulvar,dtype=float)

mulvar_train=df_mulvar["2016-01":"2018-12"]
mulvar_valid=df_mulvar["2019-01":"2019-05"]
mulvar_test=df_mulvar["2019-06":]


train_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_train.to_numpy(),  
    targets=mulvar_train[["rail","bus"]][seq_length:],  
    sequence_length=seq_length,
    batch_size=32,
    shuffle=True,
    seed=42
)
valid_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_valid.to_numpy(),
    targets=mulvar_valid[["rail","bus"]][seq_length:],
    sequence_length=seq_length,
    batch_size=32
) 


mulvar_model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),
    tf.keras.layers.Dense(2)
]) 

fit_and_evaluate(mulvar_model, train_ds, valid_ds,
                 learning_rate=0.02)
```

**疑点：**

之前我们所讨论的都是预测了下一个步长的值，为什么这么说，我们之前的模型不都在验证集上一个时间段评估了误差嘛，那不就是评估了很多步长的值？其实笔者在这里也思考了许久，这也是个误区，就是这么理解的，我们之前的模型，都是通过56个时间步长，用权重和偏置得到的是只有下一个时间下的值，从输出就能看出。而我们能在验证集上评估的原因，就是我们通过训练集得到的每个神经元的权重，验证集就是一个时间段，那么同样也通过56个窗口移动，每个验证集的样本又通过与权重的乘积得到一个值，再与验证集的真实值比较，计算误差。所以总的来说我们上面所有代码都只是预测了下一个时间步长的一个值。

**接下去我们所讨论的是，预测后面好几个时间步长的值，比如说预测后面14个值**：

### **方法一**： 序列到向量（1维）

 使用我们之前为铁路时间序列训练的model RNN，让其预测下一个值，并将该值添加到输入中，假设该预测值已经发生；然后我们再次使用该模型预测下一个值，依此类推，如下所示的代码：

```
import numpy as np

X = rail_valid.to_numpy()[np.newaxis, :seq_length, np.newaxis]
for step_ahead in range(14):
    y_pred_one = deep_model.predict(X)
    X = np.concatenate([X, y_pred_one.reshape(1, 1, 1)], axis=1)
```

在此代码中，我们提取验证期前56天的铁路客流量数据，并将数据转换为形状为[1, 56, 1]的NumPy数组（需注意循环层要求3D输入）。随后我们反复使用模型预测下一个值，并将每个预测结果沿时间轴（axis=1）追加到输入序列中。

**注意：如果模型在某个时间步发生错误，则后续时间步的预测也会受到影响：错误倾向于累积。因此，建议仅对少量时间步使用此技术。**

### **方法二**：序列到向量（14维）

训练一个 RNN 来一次性预测接下来的 14 个值。我们仍可使用序列到向量模型，但其输出将为 14 个值而非 1 个。不过，我们首先需要将目标值修改为包含接下来的 14 个值的向量。为此，我们可以再次使用timeseries_dataset_from_array()，但这次要求它创建不包含目标值（targets=None）且序列长度为seq_length + 14 的数据集。然后我们可以使用数据集的 map() 方法对每个序列批次应用自定义函数，将其拆分为输入和目标值。在此示例中，我们使用多变量时间序列作为输入（使用全部五列），并预测未来14天的铁路客运量：

```
def split_inputs_and_targets(mulvar_series,ahead=14,target_col=1):
    return mulvar_series[:,:-ahead],mulvar_series[:,-ahead:,target_col]

ahead_train_ds=tf.keras.utils.timeseries_dataset_from_array(
    mulvar_train.to_numpy(),
    targets=None,
    sequence_length=seq_length+14,
    batch_size=32,
    shuffle=True,
    seed=42
).map(split_inputs_and_targets) 

ahead_valid_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_valid.to_numpy(),
    targets=None,
    sequence_length=seq_length + 14,
    batch_size=32
).map(split_inputs_and_targets) 


tf.random.set_seed(42)

ahead_model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),
    tf.keras.layers.Dense(14)
]) 

fit_and_evaluate(ahead_model, ahead_train_ds, ahead_valid_ds,
                 learning_rate=0.02)


X = mulvar_valid.to_numpy()[np.newaxis, :seq_length]  # shape [1, 56, 5]
Y_pred = ahead_model.predict(X)  # shape [1, 14]
```

比方说 预测示例：

假设预测铁路运输量（特征索引1）：

- 输入：前56天的[温度, 运输量, 油价, 节假日, 促销]等5个特征

- 输出：未来14天的运输量预测值

为什么需要`target_col`？

**多变量预测场景**：

- 输入包含多个特征（如温度、油价等）

- 但只需预测其中一个目标特征（如运输量）

**这里我们取target_col=1的意思是取特征索引是1的特征作为目标特征**

### **方法三：序列到序列**

与其只在最后一个时间步训练模型预测接下来的14个值，我们可以在每个时间步训练模型预测接下来的14个值。换句话说，我们可以将这个序列到向量RNN转换为序列到序列RNN。**这种技术的好处是，损失函数将包含每个时间步长上 RNN 输出的项，而不仅仅是最后一个时间步长的输出。这意味着将有更多错误梯度流经模型，且它们无需像以前那样大量流经时间维度，因为它们来自每个时间步长的输出，而不仅仅是最后一个**。这将同时稳定并加速训练过程。需要明确的是，在时间步0时，模型将输出一个包含时间步1至14预测值的向量，然后在时间步1时，模型将预测时间步2至15，依此类推。换句话说，目标是连续窗口的序列，每个时间步向后移动一个时间步。**目标不再是向量，而是与输入长度相同的序列（在长度上是相同的，这很关键，与下文中return_sequences=True,呼应）**，每个时间步长包含一个14维向量。

```
def to_windows(dataset,length):
    dataset=dataset.window(length,shift=1,drop_remainder=True)
    return dataset.flat_map(lambda window_ds:window_ds.batch(length))

my_series=tf.data.Dataset.range(7)
dataset=to_windows(to_windows(my_series,3),4)
```

#### **示例数据流**

my_series = tf.data.Dataset.range(7)  # [0, 1, 2, 3, 4, 5, 6]
dataset = to_windows(to_windows(my_series, 3), 4)

#### 第一步：内层`to_windows(my_series, 3)`

原始序列: [0, 1, 2, 3, 4, 5, 6]

应用窗口长度=3：

窗口1: [0, 1, 2]
窗口2: [1, 2, 3]
窗口3: [2, 3, 4]
窗口4: [3, 4, 5]
窗口5: [4, 5, 6]

结果数据集: [[0,1,2], [1,2,3], [2,3,4], [3,4,5], [4,5,6]]

#### 第二步：外层`to_windows(..., 4)`

输入序列: [[0,1,2], [1,2,3], [2,3,4], [3,4,5], [4,5,6]]

应用窗口长度=4：

窗口1: [[0,1,2], [1,2,3], [2,3,4], [3,4,5]]
窗口2: [[1,2,3], [2,3,4], [3,4,5], [4,5,6]]

最终数据集:

[
  [[0,1,2], [1,2,3], [2,3,4], [3,4,5]],
  [[1,2,3], [2,3,4], [3,4,5], [4,5,6]]
]

```
dataset=dataset.map(lambda S:(S[:,0],S[:,1:])
list(dataset)
```

[0,1,2,3]——[[1,2],[2,3],[3,4],[4,5]]

[1,2,3,4]——[[2,3],[3,4],[4,5],[5,6]]

```
def to_seq2seq_dataset(series, seq_length=56, ahead=14, target_col=1,
                       batch_size=32, shuffle=False, seed=None):
    ds = to_windows(tf.data.Dataset.from_tensor_slices(series), ahead + 1)
    ds = to_windows(ds, seq_length).map(
        lambda S: (S[:, 0], S[:, 1:, target_col]))
    if shuffle:
        ds = ds.shuffle(8 * batch_size, seed=seed)
    return ds.batch(batch_size)
```

<mark>笔者在这里会在大脑中推演几遍到充分理解，读者也需搞明白再往下！！！</mark>

```
seq2seq_train = to_seq2seq_dataset(mulvar_train, shuffle=True, seed=42)
seq2seq_valid = to_seq2seq_dataset(mulvar_valid) 


tf.random.set_seed(42)  
seq2seq_model = tf.keras.Sequen
tial([
    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 5]),
    tf.keras.layers.Dense(14)
    # equivalent: tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(14))
    # also equivalent: tf.keras.layers.Conv1D(14, kernel_size=1)
]) 


fit_and_evaluate(seq2seq_model, seq2seq_train, seq2seq_valid,
                 learning_rate=0.1)


X = mulvar_valid.to_numpy()[np.newaxis, :seq_length] #这里只选取了一个56天的样本数据，56*5
y_pred_14 = seq2seq_model.predict(X)[0,-1]  #0：批次索引只有一个样本，-1：最后一个时间步的14个预测值
```

```
#计算验证集下的每个样本预测后14天与真实值差值的平均
Y_pred_valid = seq2seq_model.predict(seq2seq_valid)
for ahead in range(14):
    preds = pd.Series(Y_pred_valid[:-1, -1, ahead],
                      index=mulvar_valid.index[56 + ahead : -14 + ahead])
    mae = (preds - mulvar_valid["rail"]).abs().mean() * 1e6
    print(f"MAE for +{ahead + 1}: {mae:,.0f}")
```

**我们的目标实际上就是每个样本预测的56个的最后一个的其中的14个值，读者体会一下......**

## **五.深层RNN及变体**

训练一个处理长序列的RNN，必须让它在多个时间步上运行，这使得展开后的 RNN 成为一个非常深的网络。就像任何深度神经网络一样，它可能面临不稳定梯度的问题。此外，当 RNN 处理长序列时，它会逐渐忘记序列中的前几个输入。让我们分别探讨这两个问题，首先从不稳定梯度问题开始。

### **①.梯度不稳的问题的解决方式：**

我们在深度神经网络中用于缓解不稳定梯度问题的许多技巧也适用于 RNN：**良好的参数初始化、更快的优化器、dropout 等**。然而，**非饱和激活函数（如ReLU）在此处可能帮助不大**。由于每个时间步都使用相同的权重，第二个时间步的输出也可能略微增加，第三个时间步也是如此，直到输出爆炸——而非饱和激活函数无法阻止这种情况。你可以通过使用更小的学习率来降低这种风险，或者使用饱和激活函数，如双曲正切函数（这解释了为什么它是默认选择）。

同样地，梯度本身也可能爆炸。如果你发现训练不稳定，你可能需要监控梯度的大小（例如使用 TensorBoard），并可能使用**梯度裁剪**。

此外，**批量归一化在 RNN 中无法像在深度前馈网络中那样高效地使用**。事实上，你无法在时间步之间使用它，只能在递归层之间使用。更准确地说，从技术上讲，可以在记忆单元中添加一个BN层（如你稍后将看到的），使其在每个时间步长上应用（既应用于该时间步长的输入，应用于前一步的隐藏状态）。然而，同一BN层将在每个时间步使用相同的参数，无论输入和隐藏状态的实际缩放和偏移如何，在实践中，这不会产生良好的结果。

César Laurent等人于2015年在一篇论文中所示： 作者发现当**BN仅应用于层的输入而非隐藏状态时，效果略有提升**。换言之，当在递归层之间（即图中的垂直方向）应用时，效果略优于不应用，但在递归层内部（即水平方向）应用时则无明显效果。在Keras中，你可以通过在每个递归层之前添加一个批量归一化层来在层之间应用批量归一化，但这会减慢训练速度，且可能帮助不大。

- **在标准RNN中的应用难点：**
  
  1. **序列长度依赖：** RNN处理的是变长序列。BN需要在整个批次的所有序列的**相同时间步**上计算均值和方差。这意味着：
     
     - 批次内所有序列必须**填充到相同长度**。
     
     - 对于每个时间步 `t`，需要在 `[batch_size, feature_dim]` 这个切片上计算均值和方差。
  
  2. **时间步依赖性：** RNN不同时间步的输入分布差异可能很大（例如，句首词和句尾词）。BN在每个时间步独立地进行归一化，破坏了序列在时间维度上的动态特性。时间步 `t` 的归一化依赖于当前批次中所有序列在 `t` 时刻的值，这可能导致模型在预测时间步 `t` 时，间接地“窥见”了该批次中其他序列在 `t` 时刻的信息（虽然不如CNN中那么直接，但理论上存在信息泄露的担忧）。
  
  3. **小批次问题：** 如果批次大小很小，`μ_B` 和 `σ²_B` 的估计会非常不准确且噪声大，严重影响归一化效果和训练稳定性。这在处理长序列或大型模型时尤其常见（因为内存限制导致批次大小被迫变小）。
  
  4. **推理不一致性：** 虽然推理时使用移动平均，但如果序列长度在训练和推理时不一致，或者处理的是单样本序列（Batch Size=1），BN的行为会变得复杂或不理想。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-20-09-30-35-image.png" alt="" width="462" data-align="center">

另一种**归一化方法通常更适合RNN：层归一化**。这一想法由Jimmy Lei Ba等人于2016年在一篇论文中提出：它与批量归一化非常相似，但不同之处在于，**层归一化不是在批量维度上进行归一化，而是在特征维度上进行归一化**。一个优势是，它可以在每个时间步上实时计算所需的统计信息，并且可以独立地对每个实例进行计算。这也意味着它在**训练和测试阶段的行为一致（与批量归一化不同），且无需像批量归一化那样使用指数移动平均来估计训练集中所有实例的特征统计量**。与批量归一化类似，层归一化为每个输入学习一个缩放因子和一个偏移参数。在 RNN 中，**它通常用于输入与隐藏状态的线性组合之后。**

- **在RNN中的优势：**
  
  1. **序列长度无关性：** LN的计算完全基于单个时间步的单个样本（的激活向量）。它**不关心批次大小**，也**不要求批次内序列长度一致**。每个时间步独立计算，完美适配变长序列。
  
  2. **时间步独立性：** LN在单个时间步内部进行归一化，不混合不同时间步的信息，更符合RNN按时间步计算的特性，避免了BN可能存在的信息泄露担忧。
  
  3. **小批次鲁棒性：** LN的统计量计算仅依赖于单个样本，因此即使在**Batch Size = 1**（在线学习或实时推理）的情况下，也能正常工作且效果稳定。在小批次训练中表现远优于BN。
  
  4. **训练/推理一致性：** 计算方式在训练和推理时完全一致，无需额外处理移动平均，实现简单且行为可预测。

#### BN vs LN 在RNN中的关键区别总结

| 特性                    | 批量归一化 (BN)                                    | 层归一化 (LN)                                |
| --------------------- | --------------------------------------------- | ---------------------------------------- |
| **归一化维度**             | **批次维度 + 特征维度**<br>(在批次内所有样本的**每个特征**上计算统计量)  | **特征维度**<br>(在**单个样本的所有特征**上计算统计量)       |
| **依赖关系**              | 依赖**当前批次内所有样本**在**相同位置/时间步**的数据               | 仅依赖**当前样本**在**当前时间步**的数据                 |
| **序列长度**              | 要求批次内序列**长度一致**（需填充）                          | **天然支持变长序列**，无需填充对齐                      |
| **时间步处理**             | 每个时间步独立归一化，但统计依赖批次内同时间步样本                     | 每个时间步完全独立归一化                             |
| **小批次 (Small Batch)** | 表现差（统计量估计不准，噪声大）                              | **表现良好**（统计量基于单个样本）                      |
| **Batch Size=1**      | **无法有效工作**（方差为0或接近0）                          | **完美工作**                                 |
| **训练/推理**             | 不同：训练用批次统计，推理用移动平均统计                          | **相同**：计算方式完全一致                          |
| **RNN适用性**            | **困难且有局限**：主要用于输入层或CNN部分；在隐藏层应用需谨慎处理序列长度和时序依赖 | **非常适合**：是RNN和Transformer隐藏层归一化的**首选方法** |
| **计算开销**              | 相对较低（每个特征维度计算一次）                              | 相对较高（每个样本计算一次所有特征的统计）                    |
| **主要优势**              | 在CNN、固定长度输入、大批次场景效果显著                         | 变长序列、小                                   |

**简单来说：**

- **BN：** 老师批改全班试卷。他把所有学生的 **第1题** 成绩拿出来算平均分和标准差，然后调整每个学生第1题的分数。接着对第2题、第3题做同样的事情。**学生A第1题的分数调整依赖于学生B第1题的分数。**

- **LN：** 老师批改试卷。他只看 **学生A** 的整份试卷，算学生A自己三题的平均分和标准差，然后调整学生A每题的成绩。接着单独对学生B的整份试卷做同样的事情。**学生A的成绩调整完全不看学生B的试卷。**

```
class LNSimpleRNNCell(tf.keras.layers.Layer):
    def __init__(self, units, activation="tanh", **kwargs):
        super().__init__(**kwargs)
        self.state_size = units
        self.output_size = units
        self.simple_rnn_cell = tf.keras.layers.SimpleRNNCell(units,
                                                             activation=None)
        self.layer_norm = tf.keras.layers.LayerNormalization()
        self.activation = tf.keras.activations.get(activation)

    def call(self, inputs, states):
        outputs, new_states = self.simple_rnn_cell(inputs, states)
        norm_outputs = self.activation(self.layer_norm(outputs))
        return norm_outputs, [norm_outputs]  


tf.random.set_seed(42)  
custom_ln_model = tf.keras.Sequential([
    tf.keras.layers.RNN(LNSimpleRNNCell(32), return_sequences=True,
                        input_shape=[None, 5]),
    tf.keras.layers.Dense(14)
])  


fit_and_evaluate(custom_ln_model, seq2seq_train, seq2seq_valid,
                 learning_rate=0.1)
```

我们的 **LNSimpleRNNCell 类**继承自tf.keras.layers.Layer 类，与任何自定义层一样。构造函数接受单元数和所需的激活函数，设置 state_size 和 output_size 属性，然后创建一个不带激活函数的 SimpleRNNCell（因为我们希望在层归一化操作后、激活函数前进行线性运算）。随后构造函数创建 LayerNormalization 层，最后获取所需的激活函数。call() 方法首先应用 simpleRNNCell，该层计算当前输入与前一层隐藏状态的线性组合，并返回结果两次（**实际上，在 SimpleRNNCell 中，输出等于隐藏状态：即 new_states[0] 等于 outputs，因此在 call() 方法的其余部分中可以安全地忽略 new_states）。接下来，call() 方法应用层归一化**，随后应用激活函数。最后，它两次返回输出：一次作为输出，一次作为新的隐藏状态。

Keras提供的绝大多数循环层和单元都提供了dropout和recurrent_dropout超参数：前者定义应用于输入的dropout率，后者定义在时间步之间应用于隐藏状态的dropout率。因此，**在 RNN 中无需创建自定义单元来在每个时间步应用 dropout。**

### **②.解决短期记忆问题**

#### **1.LSTM cells**

若将LSTM单元视为黑盒，其使用方式与基本单元类似，但性能显著更优：训练收敛速度更快，且能识别数据中的更长时序模式。在Keras中，可直接使用LSTM层替代SimpleRNN层：

```
model=tf.keras.Sequential([
  tf.keras.layers.LSTM(32,return_sequences=True,input_shape=[None,5]),
  tf.keras.layers.Dense(14)])
```

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-20-10-44-15-image.png" alt="" width="581" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-20-11-00-47-image.png" alt="" width="567" data-align="center">

***如果不看盒子内部，LSTM 单元与普通单元完全相同，只是其状态被分为两个向量：h(t)
和 c(t)。你可以将 h(t)视为短期状态，将 c(t)视为长期状态。***

当长期状态 c(t–1) 从左向右穿过网络时，你可以看到它首先**经过一个遗忘门，丢弃一些记忆，然后
通过加法操作添加一些新记忆（该操作添加了由输入门选中的记忆）**。结果 c 直接输出，无需进一步转换。因此，在每个时间步，一些记忆被丢弃，一些记忆被添加。

此外，在加法操作之后，**长期状态会被复制并通过tanh 函数，然后结果会被输出门过滤**。这产生了短期状态 h（它等于该时间步的单元输出 y）。

现在让我们看看新记忆来自哪里以及门是如何工作的。

首先，当前输入向量x和前一时间步的短期状态h被输入到四个不同的全连接层。它们各自承担不同的功能：**主层是输出g的层**。它承担着分析当前输入x和前一（短期）状态h的常规角色。在基本单元中，除了这一层外没有其他组件，其输出直接输出到 y 和 h 。但在 LSTM 单元中，该层的输出不会直接输出；相反，其**最重要的部分存储在长期状态中（其余部分被丢弃）**。

**其他三个层是门控控制器**。由于它们使用逻辑激活函数，输出范围为 0 到 1。如你所见，门控器的输出被输入到元素级乘法操作中：如果它们输出0，则关闭门控器；如果输出1，则打开门控器。具体来说：**遗忘门（由f控制）控制长期状态中哪些部分应被清除。输入门（由i控制）控制g中哪些部分应被添加到长期状态中。最后，输出门（由o控制）控制在当前时间步应读取并输出长期状态的哪些部分，既输出到h也输出到y。**

#### **2.GRU cells**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-20-11-09-37-image.png" alt="" width="494" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-20-11-12-30-image.png" alt="" width="465" data-align="center">

GRU单元是LSTM单元的简化版本，其性能似乎同样出色这解释了其日益普及的原因）。以下是
主要简化之处：

**两个状态向量合并为单一向量h(t)**。

**单一门控器z(t)同时控制遗忘门和输入门**。如果门控制器输出 1，则遗忘门打开（= 1）且输入门关闭（1 – 1 = 0）。如果输出 0，则相反情况发生。换言之，**每次需要存储记忆时，存储位置会先被清除**。这实际上是LSTM 单元本身的一种常见变体。没有输出门；完整的状态向量在每个时间步输出。然而，有一个新的门控制器r(t)控制前一状态的哪个部分将被传递给主层（g(t)）。

**z(t):更新门：控制历史信息保留比例，替代LSTM的输入门和遗忘门，决定新状态中保留多少旧状态信息**

**r(t):重置门：控制历史信息的遗忘程度，决定在生成候选状态时忽略多少历史信息，提供短期记忆的灵活控制**

**g(t):候选隐藏状态：计算可能的新状态，重置门控制历史信息参与程度，提供非线性变换能力**

<mark>注：Keras 提供了 tf.keras.layers.GRU 层：使用它只需用 GRU 替换 SimpleRNN 或 LSTM。它还提供了tf.keras.layers.GRUCell，以便您基于 GRU 单元创建自定义单元。</mark>

```
gru_model = tf.keras.Sequential([
    tf.keras.layers.GRU(32, return_sequences=True, input_shape=[None, 5]),
    tf.keras.layers.Dense(14)
])
```

#### **3.BiLSTM**

BiLSTM将正向隐藏状态和反向隐藏状态进行拼接或其他融合操作，在LSTM基础上每个时间步同时运行两个LSTM，一个按照序列的正向顺序处理数据，另一个反向顺序处理数据

```
def build_multivariate_lstm(input_shape):
    """构建多变量LSTM模型"""
    model = Sequential([
        Bidirectional(LSTM(20, return_sequences=True), input_shape=input_shape),
        Bidirectional(LSTM(20)),
        Dense(1)
    ])
    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
    return model
```

## **六.1D-CNN及WaveNet**

<mark>1D-CNN:</mark>

我们之前的2D-CNN是通过2维矩阵扫描整个图像，由此我们迁移该思想到一维序列上，尝试使用CNN的手段帮助循环层更好的记忆。

```
conv_rnn_model=tf.keras.Sequential([
              tf.keras.layers.Conv1D(filters=32,kernel_size=4,strides=2,
                                     activation="relu",input_shape=[None,5]),
              tf.keras.layers.GRU(32,return_sequences=True),
              tf.keras.layers.Dense(14)])
longer_train=to_seq2seq_dataset(mulvar_train,seq_length=112,shuffle=True,seed=42)
longer_valid=to_seq2seq_dataset(mulvar_valid,seq_length=112)


downsampled_train=longer_train.map(lambda X,Y:(X,Y[:,3::2]))
downsampled_valid=longer_valid.map(lambda X,Y:(X,Y[:,3::2]))
```

它以一个1D卷积层开头，该层将输入序列以2倍的因子下采样，步长为2。卷积核大小大于步长，因此所有输入都会用于计算层的输出，因此模型可以学习保留有用信息，仅丢弃不重要的细节。通过缩短序列，卷积层可能有助于GRU层检测更长的模式，因此我们可以将输入序列长度翻倍至112天。注意我们还必须裁剪目标中的前三个时间步：事实上，卷积核大小为4，因此卷积层的第一个输出基于输入时间步0到3，而第一个预测将针对时间步4到17（而非时间步1到14）。此外，我们必须将目标数据以2倍因子进行下采样，因为步长为2.

<mark>WaveNet:</mark>

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-20-11-52-23-image.png" alt="" width="598" data-align="center">

在2016年的一篇论文中，Aaron van den Oord和其他DeepMind研究人员提出了一种名为WaveNet的新型架构。他们将1D卷积层堆叠起来，并在每一层都将膨胀率（即每个神经元输入的间隔）翻倍：第一层卷积层每次只能看到两个时间步，而下一层可以看到四个时间步（其接受野长度为四个时间步），再下一层可以看到八个时间步，依此类推（见图）。这样，较低层学习短期模式，而较高层学习长期模式。由于膨胀率翻倍，网络能够非常高效地处理极长的序列.论文作者实际上堆叠了10个卷积层，扩张率分别为1、2、4、8、…、256、512，然后又堆叠了另一组10个相同的层（扩张率同样为1、2、4、8、…、256、512），接着再次堆叠了另一组相同的10个层。他们通过指出，单个包含这些膨胀率的10个卷积层堆栈将相当于一个超高效的卷积层，其核大小为1,024（但速度更快、功能更强大，且使用显著更少的参数）来解释这一架构。他们在每个层之前对输入序列进行了左填充，填充的零个数等于膨胀率，以确保整个网络中序列长度保持一致。

```
wavenet_model = tf.keras.Sequential()
wavenet_model.add(tf.keras.layers.InputLayer(input_shape=[None, 5]))
for rate in (1, 2, 4, 8) * 2:
    wavenet_model.add(tf.keras.layers.Conv1D(
        filters=32, kernel_size=2, padding="causal", activation="relu",
        dilation_rate=rate))
wavenet_model.add(tf.keras.layers.Conv1D(filters=14, kernel_size=1))
```

causal:填充方式与same类似，但零值仅附加在输入序列的开头，而非两侧，确保卷积层在预测时不会窥探未来。

## **七.总结：**

RNN在使用过程中<mark>首先最重要的是数据预处理这块</mark>，下面是本文所用的到的处理过程，把数据集转换为训练数据+标签（序列到向量（返回时间步长的最后一个值）/序列到序列（返回每个时间步长下的一个或多个值）

```
def to_windows(dataset,length):
    dataset=dataset.window(length,shift=1,drop_remainder=True)
    return dataset.flat_map(lambda window_ds:window_ds.batch(length))  


def to_seq2seq_dataset(series, seq_length=56, ahead=14, target_col=1,
                       batch_size=32, shuffle=False, seed=None):
    ds = to_windows(tf.data.Dataset.from_tensor_slices(series), ahead + 1)
    ds = to_windows(ds, seq_length).map(
        lambda S: (S[:, 0], S[:, 1:, target_col]))
    if shuffle:
        ds = ds.shuffle(8 * batch_size, seed=seed)
    return ds.batch(batch_size) 


tf.keras.utils.timeseries_dataset_from_array(
                my_series,
                target=my_series[3:],
                sequence_length=3,
                batch_size=2
)
```

### 三种方法对比总结

| **特性**     | `to_windows` | `to_seq2seq_dataset` | `timeseries_dataset_from_array` |
| ---------- | ------------ | -------------------- | ------------------------------- |
| **输入类型**   | Dataset      | 数组                   | 数组                              |
| **输出结构**   | 窗口数据集        | (历史序列, 未来序列)         | (窗口, 目标值)                       |
| **多变量支持**  | ✓            | ✓                    | ✓                               |
| **多步预测**   | ✗            | ✓                    | ✓ (需手动配置)                       |
| **内置批处理**  | ✗            | ✓                    | ✓                               |
| **随机打乱支持** | ✗            | ✓                    | ✓                               |
| **最佳适用场景** | RNN数据准备      | Seq2Seq模型            | 单步预测/简单序列建模                     |

### 选择建议：

1. 需要**最灵活的基础窗口处理** → 用 `to_windows`

2. 构建**Seq2Seq预测模型**（如LSTM编解码器） → 用 `to_seq2seq_dataset`

3. 进行**单步预测**或**快速原型开发** → 用 `timeseries_dataset_from_array`

4. 当数据**已经是Dataset对象**时 → 优先使用 `to_windows`

5. 需要**完整管道（打乱+批处理）** → 使用 `to_seq2seq_dataset` 或 `timeseries_dataset_from_array`
