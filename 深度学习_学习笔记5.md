# **用RNN和注意力机制处理自然语言**

1.训练用于预测句子中下一个字符的字符 RNN（char-RNN）

2.使用无状态RNN

3.构建有状态RNN

4.构建RNN进行情感分析

5.用RNN构建能够执行神经机器翻译（NMT）的编码器-解码器架构

6.引入注意力机制提升RNN编码-解码性能

7.引入纯注意力架构(Transformer)

8.基于Transformer的GRT和BERT

9.使用HuggingFace开发的优秀的Transformer库

## **一.使用无状态的字符RNN产生莎士比亚文本**

```
import tensorflow as tf

shakespeare_url="https://homl.info/shakespeare"
filepath=tf.keras.utils.get_file("shakespeare.txt",shakespeare_url)
with open(filepath) as f:
    shakespeare_txt=f.read()
```

Downloading data from https://homl.info/shakespeare
1115394/1115394 [==============================] - 7s 7us/step

```
print(shakespeare_txt[:200])
```

First Citizen:
Before we proceed any further, hear me speak.
All:
Speak, speak.
First Citizen:
You are all resolved rather to die than to famish?
All:
Resolved. resolved.
First Citizen:
First, you

```
#文本向量化：设置 split="character" 以获得字符级编码，而不是默认的单词级编码，
#并使用 standardize="lower" 将文本转换为小写

text_vec_layer=tf.keras.layers.TextVectorization(split="character",standardize="lower")
text_vec_layer.adapt([shakespeare_txt])
encoded=text_vec_layer([shakespeare_txt])[0]
encoded
```

<mark>tf.keras.layers.TextVectorization</mark> 是 TensorFlow/Keras 中用于文本预处理的核心层，以下是其主要参数及默认值：

#### 主要参数列表

| 参数                           | 类型           | 默认值                             | 说明                                         |
| ---------------------------- | ------------ | ------------------------------- | ------------------------------------------ |
| **`max_tokens`**             | int          | `None`                          | 词汇表的最大大小（保留最常见的 max_tokens 个词）             |
| **`standardize`**            | str/callable | `"lower_and_strip_punctuation"` | 文本标准化方式                                    |
| **`split`**                  | str/callable | `"whitespace"`                  | 文本分词方式                                     |
| **`ngrams`**                 | int/tuple    | `None`                          | N-gram 范围（如 2 或 (1,3)）                     |
| **`output_mode`**            | str          | `"int"`                         | 输出格式：'int', 'binary', 'count', 'tf_idf'    |
| **`output_sequence_length`** | int          | `None`                          | 输出序列长度（固定长度）                               |
| **`pad_to_max_tokens`**      | bool         | `True`                          | 是否填充到 max_tokens（当 output_mode 不是 'int' 时） |
| **`vocabulary`**             | list/str     | `None`                          | 预定义词汇表（列表或文件路径）                            |
| **`idf_weights`**            | list         | `None`                          | TF-IDF 权重（当 output_mode='tf_idf'）          |
| **`ragged`**                 | bool         | `False`                         | 是否输出不规则张量                                  |
| **`sparse`**                 | bool         | `False`                         | 是否输出稀疏张量                                   |
| **`encoding`**               | str          | `"utf-8"`                       | 文件编码（当从文件加载词汇表时）                           |

<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([21,  7, 10, ..., 22, 28, 12], dtype=int64)>

```
#每个字符都映射到一个整数，从 2 开始。
#TextVectorization 层保留了值 0 作为填充标记，并保留了值 1 作为未知字符。
#我们暂时不需要这两个标记，因此让我们从字符 ID 中减去 2，
#并计算不同字符的数量以及字符总数

encoded-=2
n_tokens=text_vec_layer.vocabulary_size()-2
dataset_size=len(encoded)
print(n_tokens)
print(dataset_size)
```

39

1115394

参考笔记4中RNN数据预处理方式：

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-29-16-12-19-image.png" alt="" width="427" data-align="center">

```
#对于每个字符预测下一个字符
def to_dataset(sequence,length,shuffle=False,seed=None,batch_size=32):
    ds=tf.data.Dataset.from_tensor_slices(sequence)
    ds=ds.window(length+1,shift=1,drop_remainder=True)
    ds=ds.flat_map(lambda window_ds:window_ds.batch(length+1))
    if shuffle:
        ds=ds.shuffle(buffer_size=100_000,seed=seed)
    ds=ds.batch(batch_size)
    return ds.map(lambda window:(window[:,:-1],window[:,1:])).prefetch(1)
```

```
length=100
tf.random.set_seed(42)
train_set=to_dataset(encoded[:1_000_000],length=length,shuffle=True,seed=42)
valid_set=to_dataset(encoded[1_000_000:1_060_000],length=length)
test_set=to_dataset(encoded[1_060_000:],length=length)
```

```
#我们使用 Embedding 层作为第一层，用于编码字符 ID
#Embedding 层的输入维数等于不同字符 ID 的数量，输出维数是一个可调整的超参数——我们暂时将其设置为 16。
#Embedding 层的输入将是形状为 [批量大小，窗口长度] 的二维张量，
#而 Embedding 层的输出将是形状为 [批量大小，窗口长度，嵌入大小] 的三维张量。

model=tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=n_tokens,output_dim=16),
    tf.keras.layers.LSTM(128,return_sequences=True),
    tf.keras.layers.Dense(n_tokens,activation="softmax")
])
#用 Dense 层作为输出层：它必须有 39 个单元（n_tokens），因为文本中有 39 个不同的字符，
#并且我们希望输出每个可能字符的概率（在每个时间步）。 
#39 个输出概率在每个时间步长上的总和应为 1，因此我们将 Softmax 激活函数应用于 Dense 层的输出
model.compile(loss="sparse_categorical_crossentropy",optimizer="nadam",metrics=['accuracy'])

model_ckpt=tf.keras.callbacks.ModelCheckpoint("my_shakespeare_model",monitor="val_accuracy",
                                              save_best_only=True)
history=model.fit(train_set,validation_data=valid_set,epochs=20,callbacks=[model_ckpt])
```

我们来深入探讨**嵌入维度（Embedding Dimension）、嵌入层（Embedding Layer）的数学原理及其实现方式。**

### 1. 核心概念：从离散符号到连续向量

* **问题：** 计算机无法直接处理文本或类别等离散符号。传统的表示方法（如独热编码 One-Hot Encoding）存在维度灾难（高维稀疏）和无法表达语义相似性的问题。
* **解决方案：嵌入（Embedding）**
  * **思想：** 为词汇表（或类别集合）中的每个唯一符号（token）学习一个固定长度的、稠密的（dense）、实数值的向量（vector）来表示它。
  * **目标：** 在嵌入空间中，**语义或功能上相似的符号，其对应的向量在几何上也应该接近**（例如，通过余弦相似度或欧氏距离衡量）。例如，“king”和“queen”的向量应该比“king”和“apple”的向量更接近。
  * **嵌入层：** 神经网络中专门负责执行这种映射的层。它本质上是一个**可学习的查找表（Lookup Table）** 或 **参数矩阵（Parameter Matrix）**。

### 2. 数学原理

* **词汇表（Vocabulary）：** 假设我们有一个词汇表 `V`，包含所有可能被处理的符号（例如单词）。`|V|` 表示词汇表的大小（即唯一符号的数量）。
* **独热编码（One-Hot Encoding）：** 词汇表中第 `i` 个符号的传统表示是一个长度为 `|V|` 的向量，其中只有第 `i` 个位置是 `1`，其余位置都是 `0`。
  * 符号 `w_i` 的独热向量： `o_i = [0, 0, ..., 1, ..., 0, 0]^T` (第 `i` 位为 1)。
  * **问题：** 维度 `|V|` 通常非常大（几万到几十万甚至百万），导致向量极其稀疏；任意两个不同符号的向量正交（点积为0），无法表达任何语义关系。
* **嵌入矩阵（Embedding Matrix）：** 嵌入层的核心是一个可学习的参数矩阵 `E`。
  * 形状： `[|V|, d_emb]`
    * `|V|`： 词汇表大小，行数。
    * `d_emb`： **嵌入维度（Embedding Dimension）**，列数。这是一个**关键的超参数**。
  * 矩阵 `E` 的第 `i` 行 (`E[i]`) 就是词汇表中第 `i` 个符号 `w_i` 的嵌入向量（`embedding vector`）。
    * `embedding_vector(w_i) = E[i, :]` (一个长度为 `d_emb` 的实值向量)。
* **嵌入过程（数学视角）：**
  1. 给定一个符号 `w`，它在词汇表中的索引是 `i`。
  2. 获取该符号的独热编码向量 `o_i` (长度为 `|V|`)。
  3. 执行矩阵乘法： `embedding_vector(w) = o_i^T * E`
     * `o_i^T` 是 `1 x |V|` 的行向量 (只有第 `i` 位是 1)。
     * `E` 是 `|V| x d_emb` 的矩阵。
     * 结果 `o_i^T * E` 是一个 `1 x d_emb` 的行向量，**恰好等于矩阵 `E` 的第 `i` 行 (`E[i]`)**。
* **关键点：**
  * 嵌入操作在数学上等价于用符号的索引 `i` 去索引（选择）嵌入矩阵 `E` 的第 `i` 行。
  * 实际实现中，**永远不会显式地构造独热向量 `o_i` 再进行矩阵乘法**！因为这是极其低效的。而是直接通过整数索引 `i` 去查找（`lookup`）矩阵 `E` 的第 `i` 行。这个过程计算复杂度是 `O(1)`，非常高效。
  * 嵌入矩阵 `E` 的所有元素都是**可学习的参数**。在训练神经网络时，通过反向传播算法和优化器（如SGD, Adam），模型会根据任务目标（如分类损失、语言模型困惑度）自动调整 `E` 中的数值，使得嵌入向量能捕捉到符号之间对任务有用的语义和句法关系。

### 3. 嵌入维度 (`d_emb`)

* **定义：** 嵌入维度 `d_emb` 定义了学习到的嵌入向量的长度。它决定了嵌入空间的大小和表示能力。
* **作用与影响：**
  * **表示能力：** 更高的维度 (`d_emb` 大) 理论上可以编码更丰富、更细微的信息和关系。向量空间有更多“方向”来表达不同的语义特征。
  * **模型容量与计算：** `d_emb` 直接影响嵌入层的大小（参数量 = `|V| * d_emb`）和后续层（如RNN, Transformer的第一层）的输入维度。更大的 `d_emb` 通常意味着更大的模型容量、更强的拟合能力，但也意味着更多的计算量（内存占用、计算时间）和更高的过拟合风险。
  * **信息瓶颈：** 太小的 `d_emb` (`d_emb` 小) 会成为信息瓶颈，无法充分表示词汇的语义信息，限制模型性能。向量空间过于拥挤，难以区分不同符号。
  * **经验法则：**
    * 常见范围： `50` 到 `1024` 之间，`300` 是一个广泛使用的经验值（尤其在Word2Vec, GloVe等预训练词向量中）。
    * 通常与模型的其他部分（如Transformer的隐藏层维度 `d_model`）相关联。在Transformer中，嵌入层的输出维度 `d_emb` 通常等于模型的主隐藏层维度 `d_model`（即 `d_emb = d_model`），以便直接输入到后续层。
    * 更大的词汇表 `|V|` 或更复杂的任务可能需要更大的 `d_emb`。
* **选择：** `d_emb` 是一个需要通过实验（如超参数调优）来确定的超参数。需要在模型性能、计算效率和过拟合之间取得平衡。

### 4. 实现方式（在深度学习框架中）

在PyTorch、TensorFlow/Keras等框架中，嵌入层被实现为一个高效的、基于索引查找的层。

* **PyTorch (`torch.nn.Embedding`):**
  
  ```python
  import torch
  import torch.nn as nn
  # 定义： vocab_size = 10000, embedding_dim = 300
  embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=300)
  # 输入： 一个包含符号索引的张量。例如，batch_size=32, seq_len=10 的句子批次。
  # 输入张量形状: [batch_size, seq_len] = [32, 10]
  input_indices = torch.LongTensor([[5, 23, 17, ..., 8], [...], ...]) # 32x10的整数张量
  # 前向传播： 查找嵌入
  embedded_output = embedding_layer(input_indices)
  # 输出形状: [batch_size, seq_len, embedding_dim] = [32, 10, 300]
  ```
  
  * `nn.Embedding` 内部维护着一个形状为 `[vocab_size, embedding_dim]` 的参数矩阵。
  * 输入是任意形状的整数张量（通常是 `[batch_size, sequence_length]`），其中的每个整数 `i` (`0 <= i < vocab_size`) 代表一个符号在词汇表中的索引。
  * 输出是输入张量中每个索引对应的嵌入向量。输出张量的形状是 `输入张量形状 + [embedding_dim]`。例如，输入 `[32, 10]` 得到输出 `[32, 10, 300]`。

* **TensorFlow/Keras (`tf.keras.layers.Embedding`):**
  
  ```python
  from tensorflow.keras.layers import Embedding
  
  embedding_layer = Embedding(input_dim=vocab_size, output_dim=300)
  # 输入： 同样是一个整数索引张量，形状如 (batch_size, seq_len)
  # 输出： 形状为 (batch_size, seq_len, 300) 的张量
  ```

* **核心操作：** 无论框架如何封装，其底层最核心的操作都是：
  
  ```python
  # 伪代码，展示核心思想
  def embedding_lookup(indices, embedding_matrix):
      """
      indices: 整数索引张量，形状任意 (假设为 shape S)
      embedding_matrix: 可学习参数矩阵，形状 [vocab_size, d_emb]
      """
      output = []
      for index in flatten(indices): # 遍历所有索引
          vector = embedding_matrix[index] # 索引第 index 行
          output.append(vector)
      return reshape(output, S + [d_emb]) # 恢复原始索引张量形状，并添加 d_emb 维度
  ```
  
  * 这是一个高度优化过的操作，通常利用GPU的并行内存访问能力高效执行。

### 5. 嵌入层的训练

1. **初始化：** 嵌入矩阵 `E` 通常被随机初始化（例如，从均值为0、标准差较小的正态分布中采样）。
2. **前向传播：** 给定一批输入数据（包含符号索引），嵌入层执行查找操作，输出对应的嵌入向量批次。
3. **模型计算：** 这些嵌入向量被送入模型的后续层（如RNN、CNN、Transformer、全连接层）进行计算，最终产生预测（如下一个词的概率、文本分类结果）。
4. **损失计算：** 将模型的预测与真实标签比较，计算损失（如交叉熵损失）。
5. **反向传播：** 损失函数关于模型所有参数（**包括嵌入矩阵 `E` 的所有元素**）的梯度被计算出来。
6. **参数更新：** 优化器（如Adam）利用计算出的梯度更新嵌入矩阵 `E` 以及其他模型参数。**只有实际在训练批次中出现的符号的嵌入向量会被更新。**
   * 这是嵌入层高效的关键之一：虽然矩阵 `E` 很大 (`|V| * d_emb`)，但每个训练批次只使用其中一小部分行（对应批次中出现的符号）。反向传播时，**只计算和更新这些被使用行（符号）对应的梯度**。这是一种**稀疏更新**，大大节省了计算资源。

### 6. 使用场景

嵌入层无处不在，尤其是在处理离散符号输入时：

1. **自然语言处理 (NLP):**
   * **词嵌入 (Word Embedding):** 最经典应用。将单词映射为向量。是Word2Vec、GloVe、BERT、GPT等模型的基础输入层。
   * **字符嵌入 (Character Embedding):** 处理字符级模型或OOV（未登录词）问题。
   * **子词嵌入 (Subword Embedding):** 如Byte Pair Encoding (BPE), WordPiece, SentencePiece 生成的子词单元的嵌入。
2. **推荐系统 (Recommender Systems):**
   * **用户ID嵌入 (User Embedding):** 将用户ID映射为向量，表示用户偏好。
   * **物品ID嵌入 (Item Embedding):** 将物品（商品、电影、音乐等）ID映射为向量，表示物品特征。
   * 协同过滤的核心思想可以通过用户嵌入和物品嵌入的点积来实现。
3. **类别特征处理 (Categorical Feature Encoding):**
   * 在表格数据中，将高基数（大量唯一值）的类别特征（如城市、产品类别、用户职业）通过嵌入层映射为低维稠密向量，比独热编码或标签编码更有效，是深度学习处理表格数据（如通过TabNet、DeepFM、或简单地将嵌入向量与其他连续特征拼接输入MLP）的常用技术。
4. **图神经网络 (Graph Neural Networks):**
   * **节点嵌入 (Node Embedding):** 将图中的节点ID映射为向量，作为GNN的输入特征（特别是在节点没有初始特征的情况下）。
5. **其他序列建模：** 如生物信息学中的DNA/RNA/蛋白质序列处理。

### 7. 预训练嵌入 vs. 从头训练

* **预训练嵌入 (Pre-trained Embeddings):**
  * 如Word2Vec、GloVe、FastText训练好的词向量文件。
  * **优点：** 利用了大规模无标注语料中的丰富语义信息；可以作为良好的初始化，加速模型收敛并可能提高最终性能；特别适用于标注数据有限的任务。
  * **使用方式：** 通常用这些预训练向量**初始化**嵌入矩阵 `E`。在后续训练中，可以选择：
    * **冻结 (Freeze):** 保持预训练嵌入不变，仅训练模型其他部分。
    * **微调 (Fine-tune):** 允许预训练嵌入在目标任务训练过程中继续更新。
* **随机初始化，从头训练 (Random Initialization, Train from Scratch):**
  * 嵌入矩阵 `E` 完全随机初始化，并在目标任务训练过程中学习。
  * **适用场景：** 任务有足够多的标注数据；词汇表或符号系统非常特定，与通用预训练嵌入的领域差异很大（如特定领域的术语、代码标识符）；需要嵌入学习特定于任务的表示。

### 8. 总结

* **核心目的：** 将**离散符号**高效地表示为**连续、稠密、低维**的实数向量（嵌入向量），以便神经网络能够处理。
* **数学本质：** 一个可学习的**查找表**（嵌入矩阵 `E`，形状 `[|V|, d_emb]`)。符号的嵌入向量是其索引在 `E` 中对应的行。
* **嵌入维度 (`d_emb`):** 关键超参数，控制向量长度、表示能力和模型复杂度。需要在信息容量和计算效率间权衡。
* **实现：** 深度学习框架（`nn.Embedding` / `Embedding`）通过高效的**整数索引查找**实现。**不**显式计算独热编码和矩阵乘法。
* **训练：** 嵌入矩阵 `E` 是模型的可学习参数，通过反向传播和优化器更新。采用**稀疏更新**策略（只更新批次中实际出现的符号对应的行）。
* **魔力来源：** 通过在大规模数据上的训练，嵌入向量能够自动学习并编码符号之间复杂的**语义和语法关系**（相似性、类比关系等），使得“king - man + woman ≈ queen”这样的向量运算成为可能。
* **应用广泛：** NLP、推荐系统、类别特征处理、图学习等任何需要处理离散符号输入的深度学习任务的基础组件。

**模型训练完成后，只能用于对训练时相同大小的批次进行预测**

```
text_input = ["To be or not to b"]
vectorized_input = text_vec_layer(text_input)
y_proba=model.predict(vectorized_input)[0,-1]#从批量预测结果中提取第一个样本在序列最后一个位置的输出
y_pred=tf.argmax(y_proba)
text_vec_layer.get_vocabulary()[y_pred+2]
```

```
#引入tf.random.categorical函数创建根据概率分布的随机数样本
log_probas=tf.math.log([[0.5,0.4,0.1]])
tf.random.set_seed(42)
tf.random.categorical(log_probas,num_samples=9)#产生9个样本，有0，1，2三种ID，概率分别为0.5，0.4，0.1
```

```
def next_char(text,temperature=1):
    vectorized_input = text_vec_layer([text])
    y_proba=model.predict(vectorized_input)[0,-1:]
    #取最后一个时间步（即最后一个字符位置）的所有输出（即整个概率向量）。
    #注意这里使用切片`-1:`是为了保持维度，结果形状为(1, vocab_size)
    rescaled_logits=tf.math.log(y_proba)/temperature
    char_id=tf.random.categorical(rescaled_logits,num_samples=1)[0,0]
    return text_vec_layer.get_vocabulary()[char_id+2]

def extend_text(text,n_chars=100,temperature=1):
    for _ in range(n_chars):
        text+=next_char(text,temperature)
    return text


tf.random.set_seed(42)
print(extend_text("To be or not to be",temperature=0.01))
print(extend_text("To be or not to be",temperature=1))
print(extend_text("To be or not to be",temperature=100))
```

## **二.有状态的RNN**

无状态的RNN：每次训练迭代中，模型都从一个全为0的隐藏状态开始，然后在每个时间步更新状态，在最后一个时间步后，会丢弃它。

有状态的RNN：在处理完一个训练批次后保留这个最终状态，并将其用作下一个训练批次的初始状态（当批次中的每个输入序列都恰好从前一个批次中相应序列的结束位置开始时，有状态 RNN 才有意义）

**构建有状态 RNN 的第一步是使用顺序且不重叠的输入序列（而不是我们用来训练无状态 RNN 的打乱且重叠的序列）**

因此，在创建tf.data.Dataset 时，**调用 window() 方法时必须使用 shift=length（而不是shift=1）此外，我们不能调用 shuffle() 方法。**

实上，如果我们调用 batch(32)，那么 32 个连续的窗口将被放入同一个批次中，而下一个批次将不会从这些窗口的中断处继续。第一个批次将包含窗口 1 到 32，而第二个批次将包含窗口 33 到64，因此，如果您考虑每个批次的第一个窗口（即窗口 1 和 33），您会发现它们不是连续的。解决这个问题最简单的方法是**将批次大小设置为 1**：

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-29-16-11-45-image.png" alt="" width="479" data-align="center">

```
def to_dataset_for_stateful_rnn(sequence, length):
    ds = tf.data.Dataset.from_tensor_slices(sequence)
    ds = ds.window(length + 1, shift=length, drop_remainder=True)
    ds = ds.flat_map(lambda window: window.batch(length + 1)).batch(1)
    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)

stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)
stateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000],
                                                 length)
stateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)
```

**创建有状态 RNN。我们需要在创建每个循环层时将 stateful 参数设置为 True，因为有状态 RNN 需要知道批次大小（因为它将为批次中的每个输入序列保留一个状态）。因此，我们必须在第一层设置 batch_input_shape 参数**

```
tf.random.set_seed(42)  
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16,
                              batch_input_shape=[1, None]),
    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),
    tf.keras.layers.Dense(n_tokens, activation="softmax")
])
```

在每个 epoch 结束时，我们需要重置状态，然后再返回到文本的开头。为此，我们可以使用一个小型的自定义 Keras 回调：

```
class ResetStatesCallback(tf.keras.callbacks.Callback):
    def on_epoch_begin(self, epoch, logs):
        self.model.reset_states() 
model_ckpt = tf.keras.callbacks.ModelCheckpoint(
    "my_stateful_shakespeare_model",
    monitor="val_accuracy",
    save_best_only=True)
```

```
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam",
              metrics=["accuracy"])
history = model.fit(stateful_train_set, validation_data=stateful_valid_set,
                    epochs=30, callbacks=[ResetStatesCallback(), model_ckpt])
```

| 特性         | 有状态RNN                        | 无状态RNN           |
| ---------- | ----------------------------- | ---------------- |
| **状态保留**   | 批次间保留隐藏状态                     | 每次预测重置隐藏状态       |
| **批次输入要求** | 固定批次大小                        | 可变批次大小           |
| **输入形状**   | `batch_input_shape=[1, None]` | 标准 `input_shape` |
| **状态初始化**  | 手动管理 (`reset_states()`)       | 自动初始化为零          |
| **序列连续性**  | 跨批次保持序列连续性                    | 每批次独立处理          |
| **适用场景**   | 长序列、流式数据                      | 独立序列、标准训练        |
| **内存效率**   | 更高（处理长序列）                     | 较低（需完整序列）        |
| **实现复杂度**  | 更高（需状态管理）                     | 较低（自动处理）         |

有状态RNN和无状态RNN的核心区别在于**隐藏状态的生命周期管理**：

- **有状态RNN**：跨批次保留状态 → 处理超长序列/连续数据

- **无状态RNN**：每批次重置状态 → 标准批处理场景

选择依据：

- 需要处理超长序列或连续数据流？ → 有状态RNN

- 处理独立序列或标准批处理？ → 无状态RNN

- 需要简化训练流程？ → 无状态RNN

- 内存受限处理长序列？ → 有状态RNN

 性能与资源考量：

1. **内存效率**：
   
   - 有状态RNN：更高效处理长序列（无需完整序列加载）
   
   - 无状态RNN：需要完整序列内存

2. **计算效率**：
   
   - 有状态RNN：适合连续处理（状态复用）
   
   - 无状态RNN：适合并行批处理

3. **实现复杂度**：
   
   - 有状态RNN：需要手动状态管理
   
   - 无状态RNN：开箱即用

**此模型训练完成后，只能用于对训练时相同大小的批次进行预测。为了避免此限制，可以创建一个相同的无状态模型，并将有状态模型的权重复制到此模型。**

```
stateless_model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),
    tf.keras.layers.GRU(128, return_sequences=True),
    tf.keras.layers.Dense(n_tokens, activation="softmax")
]) 
stateless_model.build(tf.TensorShape([None, None]))

stateless_model.set_weights(model.get_weights())
shakespeare_model = tf.keras.Sequential([
    text_vec_layer,
    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens
    stateless_model
]) 

tf.random.set_seed(42)

print(extend_text("to be or not to be", temperature=1))
```

## **三.情感分析**

NLP 最常见的应用之一是**文本分类——尤其是情绪分析**。如果说 MNIST 数据集上的图像分类是计算机视觉领域的“Hello world！”，那么 IMDb 评论数据集上的情绪分析就是自然语言处理领域的“Hello world！”。IMDb 数据集包含 50,000 条英文电影评论（25,000 条用于训练，25,000 条用于测试），这些评论均来自著名的互联网电影数据库 (IMDb)，并且每条评论都有一个简单的二分类目标，指示其是负面 (0) 还是正面 (1)。与 MNIST 数据集一样，IMDb 评论数据集之所以受欢迎，有充分的理由：它足够简单，可以在笔记本电脑上在合理的时间内完成，但又足够具有挑战性，既有趣又有回报。让我们使用 TensorFlow Datasets 库加载 IMDb 数据集。我们将使用训练集的前 90% 进行训练，剩余的 10% 用于验证：

```
import tensorflow_datasets as tfds
import tensorflow as tf
raw_train_set,raw_valid_set,raw_test_set=tfds.load(
    name="imdb_reviews",
    split=["train[:90%]","train[90%:]","test"],
    as_supervised=True
)
tf.random.set_seed(42)
train_set=raw_train_set.shuffle(5000,seed=42).batch(32).prefetch(1)
valid_set=raw_valid_set.batch(32).prefetch(1)
test_set=raw_test_set.batch(32).prefetch(1)
```

```
for review,label in raw_train_set.take(5):
    print(review.numpy().decode("utf-8"))
    print("label:",label.numpy())
```

This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.
label: 0
I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.
label: 0
Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.
label: 0
This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.
label: 1
As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no "men" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.
label: 1

```
vocab_size=1000
text_vec_layer=tf.keras.layers.TextVectorization(max_tokens=vocab_size)#保留最常见的1000个词
text_vec_layer.adapt(train_set.map(lambda reviews,labels:reviews))
```

```
embed_size=128
tf.random.set_seed(42)
model=tf.keras.Sequential([
    text_vec_layer,
    tf.keras.layers.Embedding(vocab_size,embed_size),
    tf.keras.layers.GRU(128),
    tf.keras.layers.Dense(1,activation="sigmoid")
])
model.compile(loss="binary_crossentropy",optimizer="nadam",metrics=["accuracy"])
history=model.fit(train_set,validation_data=valid_set,epochs=10)
```

Epoch 1/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 35s 46ms/step - accuracy: 0.4950 - loss: 0.6940 - val_accuracy: 0.5024 - val_loss: 0.6929
Epoch 2/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 32s 46ms/step - accuracy: 0.5051 - loss: 0.6934 - val_accuracy: 0.5016 - val_loss: 0.6929
Epoch 3/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 32s 46ms/step - accuracy: 0.5049 - loss: 0.6918 - val_accuracy: 0.5012 - val_loss: 0.6943
Epoch 4/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 32s 45ms/step - accuracy: 0.5032 - loss: 0.6909 - val_accuracy: 0.5012 - val_loss: 0.6970
Epoch 5/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 32s 45ms/step - accuracy: 0.5042 - loss: 0.6880 - val_accuracy: 0.5044 - val_loss: 0.6951
Epoch 6/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 32s 46ms/step - accuracy: 0.5114 - loss: 0.6876 - val_accuracy: 0.5016 - val_loss: 0.6968
Epoch 7/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 32s 45ms/step - accuracy: 0.4988 - loss: 0.6885 - val_accuracy: 0.5052 - val_loss: 0.6955
Epoch 8/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 32s 46ms/step - accuracy: 0.5020 - loss: 0.6859 - val_accuracy: 0.5036 - val_loss: 0.6956
Epoch 9/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 32s 46ms/step - accuracy: 0.5066 - loss: 0.6859 - val_accuracy: 0.5040 - val_loss: 0.6970
Epoch 10/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 32s 45ms/step - accuracy: 0.5061 - loss: 0.6831 - val_accuracy: 0.5060 - val_loss: 0.6958

```
model.evaluate(test_set)
```

782/782 ━━━━━━━━━━━━━━━━━━━━ 16s 21ms/step - accuracy: 0.5074 - loss: 0.6941

[0.6944463849067688, 0.5054000020027161]

#### **掩码（Masking）**

**<mark>由于评论的长度各不相同，因此当 TextVectorization 层将它们转换为包含标记 ID 的序列时，它会使用填充标记（ID 为 0）填充较短的序列，使其与批次中最长的序列一样长。因此，大多数序列以许多填充标记结尾——通常有几十个甚至几百个。即使我们使用了 GRU 层（它比 SimpleRNN 层好得多），它的短期记忆仍然不够好，所以当它处理许多填充标记时，最终会忘记评论的内容！一种解决方案是向模型输入等长句子的批次（这也能加快训练速度）。另一种解决方案是让 RNN 忽略填充标记。这可以通过使用掩码来实现。</mark>**

```
#掩码方法1设置mask_zero=True
embed_size = 128
tf.random.set_seed(42)
model = tf.keras.Sequential([
    text_vec_layer,
    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),
    tf.keras.layers.GRU(128),
    tf.keras.layers.Dense(1, activation="sigmoid")
])

from datetime import datetime
log_dir = "logs/fit/" + datetime.now().strftime("%Y%m%d-%H%M%S")
# 定义TensorBoard回调
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,  # 日志保存目录
    histogram_freq=1,  # 每1个epoch记录权重直方图
    write_graph=True,  # 可视化计算图
    write_images=True,  # 记录权重直方图图片
    update_freq="epoch"  # 每个epoch更新一次指标
)

model.compile(loss="binary_crossentropy", optimizer="nadam",
              metrics=["accuracy"])
history = model.fit(train_set, validation_data=valid_set, epochs=10,callbacks=[tensorboard_callback])
```

Epoch 1/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 27s 34ms/step - accuracy: 0.6644 - loss: 0.5976 - val_accuracy: 0.7916 - val_loss: 0.4529
Epoch 2/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 23s 33ms/step - accuracy: 0.8266 - loss: 0.3941 - val_accuracy: 0.8604 - val_loss: 0.3276
Epoch 3/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 23s 33ms/step - accuracy: 0.8728 - loss: 0.3048 - val_accuracy: 0.8732 - val_loss: 0.3093
Epoch 4/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 23s 33ms/step - accuracy: 0.8863 - loss: 0.2750 - val_accuracy: 0.8668 - val_loss: 0.3096
Epoch 5/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 23s 33ms/step - accuracy: 0.8961 - loss: 0.2570 - val_accuracy: 0.8616 - val_loss: 0.3169
Epoch 6/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 23s 33ms/step - accuracy: 0.9033 - loss: 0.2374 - val_accuracy: 0.8672 - val_loss: 0.3217
Epoch 7/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 23s 33ms/step - accuracy: 0.9211 - loss: 0.2077 - val_accuracy: 0.8596 - val_loss: 0.3624
Epoch 8/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 23s 33ms/step - accuracy: 0.9281 - loss: 0.1920 - val_accuracy: 0.8604 - val_loss: 0.3638
Epoch 9/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 23s 33ms/step - accuracy: 0.9398 - loss: 0.1638 - val_accuracy: 0.8548 - val_loss: 0.3959
Epoch 10/10
704/704 ━━━━━━━━━━━━━━━━━━━━ 23s 33ms/step - accuracy: 0.9502 - loss: 0.1436 - val_accuracy: 0.8468 - val_loss: 0.4282

```
#掩码方法2:直接在TextVectorization中设置可以划分为不同大小张量，目前在Keras上运行不了，还不支持
text_vec_layer_ragged = tf.keras.layers.TextVectorization(
     max_tokens=vocab_size, ragged=True)

embed_size = 128
tf.random.set_seed(42)
model = tf.keras.Sequential([
     text_vec_layer_ragged,
     tf.keras.layers.Embedding(vocab_size, embed_size),
     tf.keras.layers.GRU(128),
     tf.keras.layers.Dense(1, activation="sigmoid")
])
model.compile(loss="binary_crossentropy", optimizer="nadam",
               metrics=["accuracy"])
history = model.fit(train_set, validation_data=valid_set, epochs=5)
```

#### **复用预训练的嵌入层和预训练语言模型**

**<mark>预训练嵌入层</mark>**：**谷歌的Word2vec embeddings,斯坦福的GloVe embeddings,Facebook的FastText embeddings**

```
#GloVe:
# 下载GloVe文件（示例：glove.6B.100d.txt）
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip

# 加载嵌入矩阵
import numpy as np
embedding_dim = 100
embeddings_index = {}
with open("glove.6B.100d.txt", encoding='utf-8') as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, "f", sep=" ")
        embeddings_index[word] = coefs

# 构建嵌入矩阵
vocab = text_vec_layer.get_vocabulary()  # 获取文本向量化层的词汇表
word_index = dict(zip(vocab, range(len(vocab))))
embedding_matrix = np.zeros((len(vocab), embedding_dim))
for word, i in word_index.items():
    if i < len(vocab) and word in embeddings_index:
        embedding_matrix[i] = embeddings_index[word]

# 应用到嵌入层
embedding_layer = tf.keras.layers.Embedding(
    len(vocab), embedding_dim, 
    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),
    trainable=False  # 冻结权重（也可以用True）
)
——————————————————————————————————————————————————————————————————————————————————————
FastText:
import gensim.downloader as api

# 下载FastText模型
fasttext_model = api.load('fasttext-wiki-news-subwords-300')

# 构建嵌入矩阵
embedding_matrix = np.zeros((len(vocab), 300))
for word, i in word_index.items():
    if word in fasttext_model:
        embedding_matrix[i] = fasttext_model[word]
——————————————————————————————————————————————————————————————————————————————————————————————————————————

model = tf.keras.Sequential([
    text_vec_layer,
    embedding_layer,  # 使用预训练嵌入
    tf.keras.layers.GRU(128),
    tf.keras.layers.Dense(1, activation="sigmoid")
])
model.compile(loss="binary_crossentropy", optimizer="nadam", metrics=["accuracy"])
model.fit(train_set, validation_data=valid_set, epochs=10)
```

**<mark>预训练语言模型</mark>**：Tensorflow Hub模型：**BERT/ALBERT/ELECTRA/UniversalSentence Encoder**/ 以及**HuggingFace transformers**

```
import tensorflow_hub as hub

# 使用BERT预处理+编码层
preprocessor = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
encoder = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4",
    trainable=True)  # 是否微调

# 构建模型
inputs = tf.keras.layers.Input(shape=(), dtype=tf.string)
preprocessed = preprocessor(inputs)
outputs = encoder(preprocessed)
pooled_output = outputs["pooled_output"]  # 句向量表示
dense = tf.keras.layers.Dense(1, activation="sigmoid")(pooled_output)
model = tf.keras.Model(inputs, dense)

_______________________________________________________________________________________________


encoder = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(), dtype=tf.string),
    hub.KerasLayer(encoder, trainable=False),  # 冻结特征提取器
    tf.keras.layer.Dense(64,activation="relu")
    tf.keras.layers.Dense(1, activation="sigmoid")
])
```

```
from transformers import TFAutoModel, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
bert_model = TFAutoModel.from_pretrained("bert-base-uncased")

# 自定义模型
inputs = tf.keras.layers.Input(shape=(), dtype=tf.string)
tokens = tokenizer(inputs.tolist(), padding=True, return_tensors="tf")
embeddings = bert_model(**tokens).last_hidden_state[:, 0, :]  # 取[CLS]向量
outputs = tf.keras.layers.Dense(1, activation="sigmoid")(embeddings)
model = tf.keras.Model(inputs, outputs)
```

## **四.神经机器翻译（编码器-解码器）**

建立一个NMT模型（将英语翻译成西班牙语）

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-30-09-39-14-image.png" alt="" width="531" data-align="center">

**英语句子作为编码器的输入，解码器输出西班牙语翻译**。需要注意的是，西班牙语翻译在训练期间也用作解码器的输入，但会后移一步。换句话说，在训练期间，解码器会将其在上一步应该输出的单词作为输入，而不管它实际输出的内容是什么。这被称为**教师强制（teacher forcing）——一种显著加快训练速度并提升模型性能的技术**。对于第一个单词，解码器会被赋予序列起始 (SOS) 标记，并且解码器应该以序列结束 (EOS) 标记结束句子。每个单词最初都由其 ID 表示（例如，“soccer” 的 ID 为 854）。接下来，嵌入层返回单词嵌入。这些词向量随后被输入到编码器和解码器。

在每一步中，解码器都会为输出词汇表（例如西班牙语）中的每个单词输出一个分数，然后softmax激活函数将这些分数转换为概率。例如，在第一步，“Me”的概率可能是7%，“Yo”的概率可能是1%，依此类推。概率最高的单词将被输出。这非常类似于常规的分类任务，实际上，可以使用“sparse_categorical_crossentropy”损失函数来训练模型，就像我们在char-RNN模型中所做的那样。

请注意，在推理阶段（训练之后），你不会将目标句子输入到解码器。相反，**你需要输入上一步刚刚输出的单词。**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-30-10-07-25-image.png" alt="" width="612" data-align="center">

```
import tensorflow as tf
from pathlib import Path

# 下载并解压
url = "https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"
zip_path = Path(tf.keras.utils.get_file("spa-eng.zip", origin=url,
                                        cache_dir="datasets", extract=True))

# 定位解压目录
extract_dir = zip_path.parent / zip_path.stem

# 确认目录存在
if not extract_dir.exists():
    raise RuntimeError(f"解压目录不存在：{extract_dir}")

# 自动查找 spa.txt
spa_files = list(extract_dir.rglob("spa.txt"))
if not spa_files:
    raise FileNotFoundError("spa.txt 未在解压目录中找到")
file_path = spa_files[0]

# 读取并拆行
text= file_path.read_text(encoding="utf-8")
```

```
text = text.replace("¡", "").replace("¿", "")
pairs = [line.split("\t") for line in text.splitlines()]
np.random.seed(42)  
np.random.shuffle(pairs)
sentences_en, sentences_es = zip(*pairs)  
```

```
for i in range(3):
    print(sentences_en[i],"=>",sentences_es[i])
```

How boring! => Qué aburrimiento!
I love sports. => Adoro el deporte.
Would you like to swap jobs? => Te gustaría que intercambiemos los trabajos?

```
vocab_size=1000
max_length=50

text_vec_layer_en=tf.keras.layers.TextVectorization(
    vocab_size,output_sequence_length=max_length
)
text_vec_layer_es=tf.keras.layers.TextVectorization(
    vocab_size,output_sequence_length=max_length
)
text_vec_layer_en.adapt(sentences_en)
text_vec_layer_es.adapt([f"startofseq {s} endofseq" for s in sentences_es])
```

```
text_vec_layer_en.get_vocabulary()[:10]
```

['',
 '[UNK]',
 np.str_('the'),
 np.str_('i'),
 np.str_('to'),
 np.str_('you'),
 np.str_('tom'),
 np.str_('a'),
 np.str_('is'),
 np.str_('he')]

```
text_vec_layer_es.get_vocabulary()[:10]
```

['',
 '[UNK]',
 np.str_('startofseq'),
 np.str_('endofseq'),
 np.str_('de'),
 np.str_('que'),
 np.str_('a'),
 np.str_('no'),
 np.str_('tom'),
 np.str_('la')]

```
X_train = tf.constant(sentences_en[:100_000])
X_valid = tf.constant(sentences_en[100_000:])

X_train_dec = tf.constant([f"startofseq {s}" for s in sentences_es[:100_000]])
X_valid_dec = tf.constant([f"startofseq {s}" for s in sentences_es[100_000:]])

Y_train = text_vec_layer_es([f"{s} endofseq" for s in sentences_es[:100_000]])
Y_valid = text_vec_layer_es([f"{s} endofseq" for s in sentences_es[100_000:]])
```

```
encoder_inputs=tf.keras.layers.Input(shape=[],dtype=tf.string)
decoder_inputs=tf.keras.layers.Input(shape=[],dtype=tf.string)
```

```
embed_size=128
encoder_input_ids=text_vec_layer_en(encoder_inputs)
decoder_input_ids=text_vec_layer_es(decoder_inputs)

encoder_embedding_layer=tf.keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)
decoder_embedding_layer=tf.keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)

encoder_embeddings=encoder_embedding_layer(encoder_input_ids)
decoder_embeddings=decoder_embedding_layer(decoder_input_ids)
```

```
encoder=tf.keras.layers.LSTM(512,return_state=True)
encoder_outputs,*encoder_state=encoder(encoder_embeddings)

decoder=tf.keras.layers.LSTM(512,return_sequences=True)
decoder_outputs=decoder(decoder_embeddings,initial_state=encoder_state)
```

这里需要说明一下：设置return_state=True是因为我们需要**返回LSTM层的最后状态作为解码器的输入状态**，即initial_state=encoder_state

```
output_layer=tf.keras.layers.Dense(vocab_size,activation="softmax")
Y_proba=output_layer(decoder_outputs)
```

```
model=tf.keras.Model(inputs=[encoder_inputs,decoder_inputs],outputs=[Y_proba])
model.compile(loss="sparse_categorical_crossentropy",optimizer="nadam",metrics=["accuracy"])

model.fit((X_train,X_train_dec),Y_train,epochs=10,validation_data=((X_valid,X_valid_dec),Y_valid))
```

**<mark>优化输出层的策略：</mark>**（笔者没怎么看懂...）

当输出词汇量很大时，为每个可能的单词输出概率可能会非常慢。例如，如果目标词汇量包含 50,000 个西班牙语单词而不是 1,000 个，那么解码器将输出 50,000 维向量，而对如此大的向量计算 softmax 函数将非常耗费计算资源。为了避免这种情况，一种解决方案是，只查看模型输出的正确单词和随机样本的错误单词的 logit，然后仅基于这些 logit 计算损失的近似值。这种采样 softmax 技术由 Sébastien Jean 等人于 2015 年提出。在 TensorFlow 中，您可以在训练期间使用 **tf.nn.sampled_softmax_loss() 函数来实现此目的，并在推理时使用常规的 softmax 函数**（采样 softmax 不能在推理时使用，因为它需要知道目标单词）。
另一种可以加速训练的方法——**与采样softmax兼容——是将输出层的权重与解码器嵌入矩阵的转置绑定**。这可以显著减少模型参数的数量，从而加快训练速度，有时还可以提高模型的准确率，尤其是在训练数据较少的情况下。嵌入矩阵相当于独热编码，后面跟着一个没有偏置项和激活函数的线性层，用于将独热向量映射到嵌入空间。输出层则相反。因此，如果模型能够找到一个转置接近其逆的嵌入矩阵（这样的矩阵称为正交矩阵），那么就无需为输出层单独学习一组权重。

```
def translate(sentence_en):
    translation=""
    for word_idx in range(max_length):
        X=np.array([sentence_en], dtype=object)
        X_dec=np.array(["startofseq "+translation], dtype=object)
        y_proba=model.predict((X,X_dec))[0,word_idx]
        predicted_word_id=np.argmax(y_proba)
        predicted_word=text_vec_layer_es.get_vocabulary()[predicted_word_id]
        if predicted_word=="endofseq":
           break
        translation+=" "+predicted_word
    return translation.strip()


translate("I like soccer")
translate("I like soccer and also going to the beach")
```

'me gusta el fútbol'

'me gusta jugar al fútbol y a la playa'

### **提升翻译性能的两个手段：双向RNN,Beam Search**

#### **双向RNN**

在每个时间步，**常规循环层仅查看过去和当前的输入，然后生成输出**。换句话说，它是因果关系这意味着它无法预测未来。这种类型的 RNN 在预测时间序列或序列到序列 (seq2seq) 模型的解码器中很有意义。但对于文本分类等任务，或在 seq2seq 模型的编码器中，通常最好在编码给定单词之前先查看下一个单词。例如，考虑短语“the right arm”、“the right person”和“the right to criticize”：要正确编码单词“right”，你需要提前查看。一种解决方案是在相同的输入上运行两个循环层，一个从左到右读取单词，另一个从右到左读取单词，然后在每个时间步合并它们的输出，通常是通过连接它们来实现的。这就是双向循环层所做的

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-30-14-49-28-image.png" alt="" width="385" data-align="center">

```
encoder = tf.keras.layers.Bidirectional(
                     tf.keras.layers.LSTM(256, return_state=True))
```

只有一个问题。该层现在将返回四个状态，而不是两个：前向 LSTM 层的最终短期状态和长期状态，以及后向 LSTM 层的最终短期状态和长期状态。我们不能直接将这个四元组状态用作解码器 LSTM 层的初始状态，因为它只需要两个状态（短期和长期）。我们不能将解码器设计成双向的，因为它必须保持因果关系：否则它会在训练过程中作弊，从而无法正常工作。相反，我们可以将两个短期状态和两个长期状态连接起来：

```
encoder_outputs, *encoder_state = encoder(encoder_embeddings)

encoder_state = [tf.concat(encoder_state[::2], axis=-1), # short-term (0 & 2)

                 tf.concat(encoder_state[1::2], axis=-1)] # long-term (1 & 3)
```

#### **Beam Search(集束搜索)**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-30-14-54-50-image.png" alt="" width="575" data-align="center">

假设你已经训练了一个编码器-解码器模型，并用它将句子“我喜欢足球”翻译成西班牙语。你希望它能输出正确的翻译“me gusta el fútbol”，但不幸的是，它输出的是“me gustan los jugadores”，意思是“我喜欢这些球员”。查看训练集，你会注意到很多句子，例如“我喜欢汽车”，翻译成“me gustan los autos”，因此模型在看到“我喜欢”之后输出“me gustan los”也就不足为奇了。不幸的是，在这种情况下，这是一个错误，因为“足球”是单数。模型无法回溯并修复它，因此它尝试尽可能地完成句子，在本例中使用了“jugadores”这个词。我们如何才能让模型有机会回溯并修复它之前犯下的错误呢？

最常见的解决方案之一是**集束搜索**：**它会跟踪 k 个最有可能的句子（例如，前三个）的简短列表，并在每个解码步骤中尝试将这些句子扩展一个单词，只保留 k 个最有可能的句子。参数 k 称为集束宽度。**
例如，假设你使用模型翻译句子“我喜欢足球”，使用集束宽度为 3 的集束搜索（参见图 ）。在第一个解码步骤中，模型将输出翻译句子中每个可能的第一个单词的估计概率。假设前三个单词是“me”（估计概率为 75%）、“a”（3%）和“como”（1%）。这就是我们目前的简短列表。
接下来，我们使用该模型为每个句子寻找下一个单词。对于第一个句子（“me”），模型可能输出“gustan”这个词的概率为36%，“gusta”这个词的概率为32%，“encanta”这个词的概率为16%，以此类推。需要注意的是，这些实际上是条件概率，因为句子以“me”开头。对于第二个句子（“a”），模型可能输出“mi”这个词的条件概率为50%，以此类推。假设词汇表有1000个单词，那么每个句子最终会有1000个概率。
接下来，我们计算我们考虑的3000个双词句子（3×1000）的概率。我们通过将每个单词的估计条件概率乘以它所完成的句子的估计概率来实现这一点。例如，句子“me”的估计概率为 75%，而单词“gustan”（假设第一个单词是“me”）的估计条件概率为 36%，因此句子“me gustan”的估计概率为 75% × 36% = 27%。计算所有 3,000 个双词句子的概率后，我们只保留排名前 3 位的句子。在这个例子中，它们都以单词“me”开头：“me gustan”（27%）、“me gusta”（24%）和“me encanta”（12%）。目前，“me gustan”句子胜出，但“me gusta”尚未被淘汰。

然后，我们重复相同的过程：使用模型预测这三个句子中的下一个单词，并计算我们考虑的所有 3,000 个三词句子的概率。现在排名前三的可能是“me gustan los”（10%）、“me gusta el”（8%）和“me gusta mucho”（2%）。下一步，我们可能会得到“me gusta el fútbol”（6%）、“me gusta mucho el”（1%）和“me gusta el deporte”（0.2%）。请注意，“me gustan”被剔除了，正确的翻译现在就在前面。我们无需任何额外的训练，只需更明智地使用它，就能提升编码器-解码器模型的性能。

```
def beam_search(sentence_en, beam_width, verbose=False):
    X = np.array([sentence_en])            # encoder input
    X_dec = np.array(["startofseq"])       # decoder input
    y_proba = model.predict((X, X_dec))[0, 0]     # first token's probas
    top_k = tf.math.top_k(y_proba, k=beam_width)
    top_translations = [                   # list of best (log_proba, translation)
        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])
        for word_proba, word_id in zip(top_k.values, top_k.indices)
    ]

    #displays the top first words in verbose mode

    if verbose:
        print("Top first words:", top_translations)

    for idx in range(1, max_length):
        candidates = []
        for log_proba, translation in top_translations:
            if translation.endswith("endofseq"):
                candidates.append((log_proba, translation))
                continue                      # translation is finished, so don't try to extend it
            X = np.array([sentence_en])       # encoder input
            X_dec = np.array(["startofseq " + translation])  # decoder input
            y_proba = model.predict((X, X_dec))[0, idx]      # last token's proba
            for word_id, word_proba in enumerate(y_proba):
                word = text_vec_layer_es.get_vocabulary()[word_id]
                candidates.append((log_proba + np.log(word_proba),
                                   f"{translation} {word}"))
        top_translations = sorted(candidates, reverse=True)[:beam_width]

        # displays the top translation so far in verbose mode

        if verbose:
            print("Top translations so far:", top_translations)

        if all([tr.endswith("endofseq") for _, tr in top_translations]):
            return top_translations[0][1].replace("endofseq", "").strip()
```

上面这个函数首先使用模型找到前 k 个单词作为翻译的起始词（其中 k 是集束宽度）。对于每个前 k 个翻译，它会评估所有可能添加到该翻译中的单词的条件概率。这些扩展的翻译及其概率会被添加到候选列表中。在遍历完所有前 k 个翻译以及所有可以完成它们的单词后，我们只保留概率最高的前 k 个候选，并不断迭代，直到它们都以 EOS 代币结束。然后返回最高的翻译（移除其 EOS 代币后）。

**注意：如果 p(S) 是句子 S 的概率，p(W|S) 是假设翻译以 S 开头，单词 W 的条件概率，那么句子 S' = concat(S, W) 的概率为 p(S') = p(S) * p(W|S)。随着单词数量的增加，概率会越来越小。为了避免概率过小而导致浮点精度错误，该函数跟踪的是对数概率而不是概率：回想一下，log(a*b) = log(a) + log(b)，因此 log(p(S')) = log(p(S)) + log(p(W|S))。**

## **五.注意力机制**

回想一下之前从单词“soccer”到其翻译“fútbol”的路径：它相当长！这意味着这个词（以及所有其他单词）的表示需要经过许多步骤才能实际使用。我们能不能让这条路径更短一些？这是 Dzmitry Bahdanau 等人在 2014 年发表的一篇具有里程碑意义的论文的核心思想。论文中，作者提出了一种技术，使解码器能够在每个时间步长上专注于相应的单词（由编码器编码）。例如，在解码器需要输出单词“fútbol”的时间步长上，它会将注意力集中在单词“soccer”上。这意味着从输入单词到其翻译的路径现在要短得多，因此 RNN 的短期记忆限制的影响要小得多。注意力机制彻底改变了神经机器翻译（以及深度学习），使其在最佳翻译水平上取得了显著提升，尤其是在处理长句（例如超过 30 个单词）方面。
**注意
NMT 中最常用的指标是双语评估基准 (BLEU) 分数，它将模型生成的每个翻译与人类生成的几个良好翻译进行比较：它计算出现在任何目标翻译中的 n-gram（n 个单词的序列）的数量，并根据生成的 n-gram 在目标翻译中出现的频率调整分数。**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-30-15-14-07-image.png" alt="" width="560" data-align="center">

图展示了添加了注意力机制的编码器-解码器模型。左侧是编码器和解码器。我们不再仅仅将编码器的最终隐藏状态以及每一步的前一个目标单词发送给解码器（虽然图中没有显示，但仍然会发送），而是将**编码器的所有输出也发送给解码器**。由于解码器无法一次性处理所有这些编码器的输出，因此需要将它们聚合起来：在每个时间步，**解码器的记忆单元都会计算所有编码器输出的加权和。这决定了它在此步骤中将关注哪些词**。权重 $\alpha_{(t,i)}$是第 t 个解码器时间步的第 i 个编码器输出的权重。例如，如果权重 $\alpha_{(3,2)}$ 远大于权重 $\alpha_{(3,0)}$ 和 $\alpha_{(3,1)}$ ，那么解码器将更加关注编码器对单词 #2（“soccer”）的输出，而不是其他两个输出，至少在此时间步是这样。解码器的其余部分与之前的工作方式相同：在每个时间步，记忆单元接收我们刚刚讨论过的输入，**加上上一个时间步的隐藏状态**，最后（尽管图中未显示）它接收上一个时间步的目标词（或者在推理时，接收上一个时间步的输出）。

但是这些$\alpha_{(t,i)} $权重从何而来呢？它们是由一个小型神经网络生成的,**这个神经网络被称为对齐(Alignment model)模型（或注意力层），它与编码器-解码器模型的其余部分联合训练**。该对齐模型如图右侧所示。**它首先是一个由单个神经元组成的 Dense 层，该层处理编码器的每个输出**，
**以及解码器的先前隐藏状态（例如 $h_{(2)}$）**。该层为每个编码器输出（例如 $e_{(3,2)}$）输出一个分数（或能量）：**这个分数衡量每个输出与解码器先前隐藏状态的对齐程度**。例如，在图中，模型已经输出了“me gusta el”（意思是“我喜欢”），所以它现在期待一个名词：“soccer”这个词与当前状态最匹配，因此它获得了高分。最后，**所有分数都会经过一个softmax层**，得到每个编码器输出的最终权重（例如α）。**给定解码器时间步长的所有权重加起来等于1。这种特殊的注意力机制被称为<mark>Bahdanau注意力机制</mark>**（以2014年论文的第一作者命名）。由于它将编码器的输出与解码器之前的隐藏状态连接起来，因此有时也被称为<mark>连接注意力机制（或加性注意力机制）concatenative attention (or additive attention)</mark>

另一种常见的注意力机制，称为 **<mark>Luong 注意力或乘性注意力</mark>**，由 Minh-Thang Luong 等人于 2015 年提出。由于对齐模型的目标是衡量编码器输出与解码器先前隐藏状态之间的相似性，因此作者建议简单地**计算这两个向量的点积，因为这通常是一个相当不错的相似性度量**，而且现代硬件可以非常高效地计算它。为了实现这一点，两个向量必须具有相同的维数。点积会给出一个分数，所有分数（在给定的解码器时间步长）都会经过一个softmax层，得出最终权重，就像Bahdanau的注意力机制一样。

Luong等人提出的另一个简化方法是，**使用解码器在当前时间步长而非前一时间步长的隐藏状态（即$h_t$，而不是$h_{t-1}$），然后直接使用注意力机制的输出（记为h˜(t)）来计算解码器的预测，而不是用它来计算解码器的当前隐藏状态**。

研究人员还提出了一种点积机制的变体，**其中编码器输出首先经过一个全连接层（没有偏置项），然后再计算点积。这被称为“通用(general)”点积方法**。研究人员将这两种点积方法与连接注意力机制（添加了一个缩放参数向量v）进行了比较，并观察到点积变体的表现优于连接注意力机制。因此，连接注意力机制现在已很少使用。这三种注意力机制的公式总结在公式中。

![](C:\Users\Lenovo\AppData\Roaming\marktext\images\2025-07-30-15-42-55-image.png)

<mark>Keras提供了tf.keras.layers.Attention()作为Luong注意力机制，提供了tf.keras.layers.AdditiveAttention()作为Bahdanau注意力机制。</mark>

```
encoder = tf.keras.layers.Bidirectional(
    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True)) 


encoder_outputs, *encoder_state = encoder(encoder_embeddings)
encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)
                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)
decoder = tf.keras.layers.LSTM(512, return_sequences=True)
decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state) 


attention_layer = tf.keras.layers.Attention()
attention_outputs = attention_layer([decoder_outputs, encoder_outputs])

output_layer = tf.keras.layers.Dense(vocab_size, activation="softmax")
Y_proba = output_layer(attention_outputs) 


model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],
                       outputs=[Y_proba])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam",
              metrics=["accuracy"])
model.fit((X_train, X_train_dec), Y_train, epochs=10,
          validation_data=((X_valid, X_valid_dec), Y_valid)) 


translate("I like soccer and also going to the beach")
```

简而言之，**注意力层提供了一种将模型的注意力集中在部分输入上的方法**。但还有另一种理解这一层的方式：**<mark>它充当一种可微分的记忆检索机制</mark>。**
例如，假设编码器分析了输入句子“我喜欢足球”，并且它理解到“我”是主语，“喜欢”是动词，因此它将这些信息编码到了这些词的输出中。现在假设解码器已经翻译了主语，并且认为接下来应该翻译动词。为此，它需要从输入句子中获取动词。这类似于字典查找：就好像编码器创建了一个字典 {"subject": "They", "verb": "played", …}，而解码器想要查找与键“verb”对应的值。但是，该模型没有离散的标记来表示键（例如“subject”或“verb”）；相反，它拥有在训练过程中学习到的这些概念的向量化表示，因此它用于查找的查询不会与字典中的任何键完全匹配。解决方案是，**计算查询与字典中每个键之间的相似度度量，然后使用softmax函数将这些相似度得分转换为加起来等于1的权重**。正如我们之前所见，这正是注意力层所做的。**如果表示动词的键与查询最相似**，那么该键的权重将接近于1。接下来，注意力层计算相应值的加权和：如果“动词”键的权重接近于1，那么加权和将非常接近单词“played”的表示。**这就是为什么Keras的注意力层和AdditiveAttention层都需要一个列表作为输入，包含两到三个项：查询、键以及可选的值**。如果您不传递任何值，那么它们将自动等于键。因此，回顾前面的代码示例，再次强调，**解码器的输出是查询**，而**编码器的输出既是键也是值**。对于每个解码器输出（即每个查询），注意层返回与解码器输出最相似的编码器输出（即键/值）的加权和。归根结底，注意机制是一个可训练的记忆检索系统。它非常强大，以至于您实际上可以仅使用注意机制构建最先进的模型。

## **六.Transformer架构**

**注意力机制就是Transformer 的原始架构**。在 2017 年的一篇开创性论文中，谷歌研究团队提出了“注意力机制”这一理念。他们创建了一个名为 <mark>Transformer 的架构</mark>，该架构显著提升了神经机器翻译 (NMT) 领域的最高水平，无需使用任何循环层或卷积层，仅使用了注意力机制（以及嵌入层、全连接层、归一化层和其他一些零碎的组件）。由于该模型不是循环的，因此它不像 RNN 那样容易受到梯度消失或爆炸的影响，可以用更少的步骤进行训练，更容易在多个 GPU 上并行化，并且比 RNN 更能捕捉长距离模式。2017 年 Transformer 的原始架构如图 所示。简而言之，图的左侧部分是编码器，右侧部分是解码器。每个嵌入层输出一个**形状为[批量大小、序列长度、嵌入大小]的**三维张量。之后，张量在流经Transformer时会逐渐变换，但其形状保持不变。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-30-16-16-55-image.png" alt="" width="417" data-align="center">

如果使用 Transformer 进行 NMT，那么在训练期间，您必须将英语句子输入编码器，并将相应的西班牙语翻译输入解码器，并在每个句子的开头插入一个**额外的 SOS 标记(startofseq)**。在推理时，您必须多次调用 Transformer，每次生成一个单词的翻译，并在每一轮中将部分翻译输入解码器，就像我们之前**在 Translate() 函数中所做的那样**。

```
def translate(sentence_en):
    translation=""
    for word_idx in range(max_length):
        X=np.array([sentence_en], dtype=object)
        X_dec=np.array(["startofseq "+translation], dtype=object)
        y_proba=model.predict((X,X_dec))[0,word_idx]
        predicted_word_id=np.argmax(y_proba)
        predicted_word=text_vec_layer_es.get_vocabulary()[predicted_word_id]
        if predicted_word=="endofseq":
           break
        translation+=" "+predicted_word
    return translation.strip()


translate("I like soccer")
```

编码器的作用是**逐步转换输入（英语句子的单词表示），直到每个单词的表示都能在句子的上下文中完美地捕捉单词的含义**。例如，如果您将“I like soccer”这句话输入编码器，那么“like”这个词一开始的表示会相当模糊，因为这个词在不同的语境下可能有不同的含义：想想““I like soccer”和“It’s like that”。但经过编码器之后，单词的表征应该能够捕捉到给定句子中“like”的正确含义（例如，喜欢），以及翻译可能需要的任何其他信息（例如，它是一个动词）。

解码器的作用是**将翻译后的句子中的每个单词表征逐渐转换为翻译中下一个单词的单词表征**。例如，如果要翻译的句子是“I like soccer”，而解码器的输入句子是“<SOS> me gusta el fútbol”，那么经过解码器之后，“el”这个词的表征最终将转换为“fútbol”这个词的表征。同样，“fútbol”这个词的表征也将转换为EOS代币的表征。
经过解码器后，每个单词表示都会经过一个最终的 Dense 层，该层使用softmax 激活函数，希望能够输出下一个单词的正确概率较高，而其他所有单词的概率较低。预测的句子应该是“me gusta el fútbol<EOS>”。
<u>这就是大概的流程；现在让我们更详细地看一下图 </u>:

首先，请注意，**编码器和解码器都包含堆叠 N 次的模块**。在论文中，N = 6。整个编码器堆栈的最终输出会在这 N 层中的每一层馈送到解码器。
放大后，会发现大多数组件已经很熟悉：

**两个嵌入层(Embedding)**；

**多个跳跃连接**，每个跳跃连接后接一个层归一化层；

**多个前馈模块(Feed Forward)，每个模块由两个全连接层组成**（第一个使用 ReLU 激活函数，第二个不使用激活函数）；

最后，**输出层是一个使用 Softmax 激活函数的全连接层**。如果需要，您还可以在注意层和前馈模块之后添加一些 Dropout。

由于所有这些层都是时间分布的，因此每个单词都会被独立处理。但是，我们如何才能通过完全独立地查看单词来翻译一个句子呢？嗯，我们做不到，所以新的组件就派上用场了：

1.<mark>编码器的多头注意力层(multi-head attention layer)</mark>通过关注（即关注）同一句子中的所有其他单词来更新每个单词的表征。这样，“like”这个词原本模糊的表征就变成了更丰富、更准确的表征，捕捉到了它在给定句子中的精确含义。我们稍后会详细讨论它的工作原理。

2.<mark>解码器的屏蔽多头注意力层(masked multi-head attention layer)</mark>也做着同样的事情，但当它处理一个单词时，它不会关注位于该单词之后的单词：它是一个因果层。例如，当它处理“gusta”这个词时，它只会关注“<SOS> me gusta”，而忽略“el fútbol”这个词（否则就是作弊）。

3.<mark>解码器的上层多头注意力层(multi-head attention layer)</mark>是**解码器关注英文句子中单词的地方**。在这种情况下，这被称为**交叉注意力**，而不是**自注意力**。例如，解码器在处理单词“el”并将其表示转换为单词“fútbol”的表示时，可能会密切关注单词“soccer”。

4.<mark>位置编码是密集向量(positional encodings)（很像词向量）</mark>，表示**每个单词在句子中的位置**。第 n 个位置编码被添加到每个句子中第 n 个单词的词向量中。这是必要的，因为 Transformer 架构中的所有层都忽略了单词的位置：如果没有位置编码，你可以对输入序列进行打乱，而 Transformer 也会以相同的方式对输出序列进行打乱。显然，单词的顺序很重要，这就是为什么我们需要以某种方式向 Transformer 提供位置信息：**在单词表示中添加位置编码**是实现此目的的好方法。

```
#编码器的结构实现：

N = 6  # 编码器层数
num_heads = 8  # 注意力头数
dropout_rate = 0.1  # Dropout比例
n_units = 128  # 前馈网络中间层维度

#填充掩码(Padding Mask) 的实现，用于处理序列中不同长度的样本。
#作用是在计算注意力时忽略填充位置（padding tokens）
encoder_pad_mask=tf.math.not_equal(encoder_input_ids,0)[:,tf.newaxis]

Z = encoder_in  # 输入嵌入 (batch_size, seq_len, embed_size) 


for _ in range(N):
    # === 1. 多头自注意力子层 ===
    skip = Z  # 残差连接

    # 创建多头注意力层
    attn_layer = tf.keras.layers.MultiHeadAttention(
        num_heads=num_heads, 
        key_dim=embed_size,  # 每个头的维度
        dropout=dropout_rate
    )

    # 自注意力计算 (Q=K=V=Z)
    Z = attn_layer(
        Z,             # query
        value=Z,       # value
        attention_mask=encoder_pad_mask  # 应用填充掩码
    )

    # 残差连接 + 层归一化
    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))

    # === 2. 前馈神经网络子层 ===
    skip = Z  # 残差连接

    # 两层全连接网络
    Z = tf.keras.layers.Dense(n_units, activation="relu")(Z)  # 扩展维度
    Z = tf.keras.layers.Dense(embed_size)(Z)  # 投影回原始维度

    # Dropout
    Z = tf.keras.layers.Dropout(dropout_rate)(Z)

    # 残差连接 + 层归一化
    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))
```

```
 decoder_pad_mask=tf.math.not_equal(decoder_input_ids,0)[:,tf.newaxis]
 #创建下三角矩阵掩码
 causal_mask=tf.linalg.band_part(
     tf.ones((batch_max_len_dec,batch_max_len_dec),tf.bool),-1,0
 )

encoder_outputs = Z  # 编码器输出
Z = decoder_in  # 解码器输入（含位置编码）

for _ in range(N):  # 循环N层
    # === 1. 带掩码的多头自注意力 ===
    skip = Z
    attn_layer = tf.keras.layers.MultiHeadAttention(
        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate
    )
    # 应用组合掩码：因果掩码 + 填充掩码
    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)
    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))

    # === 2. 编码器-解码器多头注意力 ===
    skip = Z
    attn_layer = tf.keras.layers.MultiHeadAttention(
        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate
    )
    # 注意：这里应该使用编码器输出作为value(交叉注意力机制)
    Z = attn_layer(Z, value=encoder_outputs, attention_mask=decoder_pad_mask)
    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))

    # === 3. 前馈网络 ===
    skip = Z
    Z = tf.keras.layers.Dense(n_units, activation="relu")(Z)  # 扩展维度
    Z = tf.keras.layers.Dense(embed_size)(Z)  # 投影回原始维度
    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))
```

```
Y_proba = tf.keras.layers.Dense(vocab_size, activation="softmax")(Z)

model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],
                           outputs=[Y_proba])

model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam",
metrics=["accuracy"])

model.fit((X_train, X_train_dec), Y_train, epochs=10,
validation_data=((X_valid, X_valid_dec), Y_valid))
```

#### 填充掩码(encoder_pad_mask)

1. **输入数据**：`encoder_input_ids`
   
   - 形状：`(batch_size, sequence_length)`
   
   - 包含词ID序列，0通常表示填充位置（例如：`[15, 28, 0, 0, 0]`）

2. **掩码生成**：
   
   - `tf.math.not_equal(encoder_input_ids, 0)`：创建布尔张量
     
     - 非填充位置：`True`
     
     - 填充位置：`False`
     
     - 示例：输入 `[15, 0, 0, 42]` → 输出 `[True, False, False, True]`
   
   - `[:, tf.newaxis]`：增加维度
     
     - 从 `(batch_size, seq_len)` → `(batch_size, 1, seq_len)`
     
     - 原因：为了匹配多头注意力需要的维度 `(batch_size, num_heads, seq_len, seq_len)`
- 在批次处理时，不同序列长度不同，需要填充到相同长度

- 防止模型关注无意义的填充位置

- 在计算注意力权重时，填充位置会被设置为负无穷大（`-inf`），使softmax后权重为0

#### 因果掩码(causal_mask)

```
causal_mask = tf.linalg.band_part(
 tf.ones((batch_max_len_dec, batch_max_len_dec), -1, 0
)
```

##### 参数解析：

- `tf.ones((batch_max_len_dec, batch_max_len_dec))`
  
  - 创建一个形状为 `(seq_len, seq_len)` 的全1矩阵
  
  - `batch_max_len_dec`：批次中解码器序列的最大长度

- `tf.linalg.band_part(input, num_lower, num_upper)`
  
  - 创建带状矩阵（保留对角线周围特定范围的元素）
  
  - **关键参数**：
    
    - `num_lower = -1`：保留主对角线及其下方的所有元素
    
    - `num_upper = 0`：只保留主对角线（不包括对角线上方元素）
  
  - 效果：创建一个**下三角矩阵**（包括主对角线）

##### 作用：

- **确保自回归性质**：在生成序列时，每个位置只能关注当前位置及之前的位置

- **防止信息泄露**：阻止模型"看到"未来的信息

- **实现方式**：在注意力计算中，将未来位置的注意力权重设为极小值（`-inf`） 

### **注**：

#### **1.词义表征**：

是指在语言理解和生成过程中，人们对词语、句子等语言单位所表达的意义在大脑中的存储和呈现方式。

#### **2.表征**：

是信息在头脑中的呈现方式，是信息记载或表达的方式，是指可以指代某种东西的符号或信号，即某一事物缺席时，它代表该事物；

#### **3.图中，进入每个多头注意力层的前两个箭头代表键和值，第三个箭头代表查询：**

在自注意力层中，这三个箭头等于前一层输出的词表征；而在解码器的上层注意力层中，键和值等于编码器最终的词表征，查询等于前一层输出的词表征。

#### **4.自注意力（Self-Attention）和交叉注意力（Cross-Attention）的区别与联系**：

它们的使用场景和数学原理。它们是Transformer架构及其衍生模型（如BERT、GPT等）的核心组件，理解它们至关重要。在深入区别之前，先快速回顾注意力机制的核心思想：它允许模型在处理序列数据（如句子）时，**动态地聚焦于输入的不同部分**。它通过计算一个“**相关性分数**”来实现，该分数决定了在生成当前输出时，应该给予输入序列中其他部分多少“注意力”。

**数学基础：缩放点积注意力（Scaled Dot-Product Attention）**

这是Transformer中使用的基础注意力计算单元。公式如下：

`Attention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V`

* **Q (Query)：** 查询向量。代表当前需要计算注意力的位置（你想知道“谁”或“什么”是相关的）。
* **K (Key)：** 键向量。代表序列中每个位置提供的“标识”或“内容”，用于与Query匹配。
* **V (Value)：** 值向量。代表序列中每个位置实际包含的“信息”或“内容”，最终加权求和的结果用于输出。
* **d_k：** Key向量的维度（通常也是Query的维度）。`sqrt(d_k)`用于缩放点积，防止其值过大导致softmax梯度消失。
* **softmax：** 将点积分数（Q*K^T）归一化为概率分布（和为1），表示每个Value的权重。

**现在，关键区别在于 Q, K, V 的来源！**

---

##### 1. 自注意力 (Self-Attention)

* **定义：** 在**同一个序列内部**计算注意力。序列中的**每个元素（位置）同时扮演Query、Key和Value的角色**。每个元素通过与其他所有元素（包括自身）计算注意力分数，来整合整个序列的信息。

* **Q, K, V 来源：** **全部来自同一个输入序列 X。**
  
  * 通常：`Q = X * W^Q`, `K = X * W^K`, `V = X * W^V` （`W^Q`, `W^K`, `W^V` 是可学习的权重矩阵）。

* **功能：** 捕捉序列内部的**长距离依赖关系**和**上下文信息**。每个位置的输出表示都融合了序列中所有其他位置的相关信息。

* **计算过程：**
  
  1. 输入序列 `X` (形状 `[seq_len, d_model]`) 经过三个不同的线性变换得到 `Q`, `K`, `V` (形状均为 `[seq_len, d_k/d_v]`，通常 `d_k = d_v`)。
  2. 计算 `Q * K^T` (形状 `[seq_len, seq_len]`)，得到**每对位置之间的原始相关性分数**。
  3. 缩放：`(Q * K^T) / sqrt(d_k)`
  4. 应用 `softmax`（按行），得到注意力权重矩阵 `A` (形状 `[seq_len, seq_len]`)，`A[i, j]` 表示第 `i` 个位置（作为Query）对第 `j` 个位置（作为Key）的注意力权重。
  5. 加权求和：`Output = A * V` (形状 `[seq_len, d_v]`)。第 `i` 行的输出是序列中所有位置 `j` 的 `V[j]` 以 `A[i, j]` 为权重的加权和。

* **使用场景：**
  
  * **Transformer Encoder：** BERT、ViT 等模型的核心，用于理解输入序列（文本、图像patch）的整体上下文和内部关系。例如，确定句子中代词“它”指代的是哪个名词。
  * **Transformer Decoder 的 Masked Self-Attention：** 在生成输出序列时（如机器翻译），确保当前位置只能关注到它**之前**已生成的输出位置（防止信息泄露），但仍需理解已生成部分的内部关系。这是“Masked”的含义。
  * **任何需要建模序列元素间相互关系的任务：** 文本分类、命名实体识别、句子表示学习等。

* **数学原理图示：**
  
  ```
  输入序列 X: [词1, 词2, 词3]
  Q = X * W^Q -> [Q1, Q2, Q3]
  K = X * W^K -> [K1, K2, K3]
  V = X * W^V -> [V1, V2, V3]
  
  计算 Output1 (对应词1):
      Score1 = [Q1·K1, Q1·K2, Q1·K3] / sqrt(d_k)
      Weights1 = softmax(Score1) = [w11, w12, w13]
      Output1 = w11*V1 + w12*V2 + w13*V3  (融合了词1、词2、词3的信息)
  同理计算 Output2, Output3。
  ```

---

##### 2. 交叉注意力 (Cross-Attention)

* **定义：** 在两个**不同的序列**之间计算注意力。**一个序列提供 Query，另一个序列提供 Key 和 Value**。允许一个序列中的元素从另一个序列中检索相关信息。

* **Q, K, V 来源：** **Query 来自序列 A，Key 和 Value 来自序列 B。**
  
  * 通常：`Q = A * W^Q`, `K = B * W^K`, `V = B * W^V` （`W^Q`, `W^K`, `W^V` 是可学习的权重矩阵，可能独立于自注意力层中的权重）。

* **功能：** 建立**两个不同序列或表示之间**的关联。允许一个序列（通常是目标或查询序列）从另一个序列（通常是源或上下文序列）中**有选择性地提取信息**。

* **计算过程：**
  
  1. 序列 `A` (形状 `[len_A, d_model_A]`) 经过线性变换得到 `Q` (形状 `[len_A, d_k]`)。
  2. 序列 `B` (形状 `[len_B, d_model_B]`) 经过线性变换得到 `K` (形状 `[len_B, d_k]`) 和 `V` (形状 `[len_B, d_v]`)。
  3. 计算 `Q * K^T` (形状 `[len_A, len_B]`)，表示序列 `A` 中每个位置（Query）与序列 `B` 中每个位置（Key）的相关性。
  4. 缩放：`(Q * K^T) / sqrt(d_k)`
  5. 应用 `softmax`（按行），得到注意力权重矩阵 `A_cross` (形状 `[len_A, len_B]`)，`A_cross[i, j]` 表示序列 `A` 中第 `i` 个位置（作为Query）对序列 `B` 中第 `j` 个位置（作为Key）的注意力权重。
  6. 加权求和：`Output = A_cross * V` (形状 `[len_A, d_v]`)。第 `i` 行的输出是序列 `B` 中所有位置 `j` 的 `V[j]` 以 `A_cross[i, j]` 为权重的加权和。**这个输出序列的形状与 Query 序列 `A` 相同**，但其每个位置的内容是 `V` (来自序列 `B`) 的加权和。

* **使用场景：**
  
  * **Transformer Decoder：** 这是交叉注意力最经典的应用。Decoder 在生成目标序列的每个元素时：
    * 使用 **Decoder 自身的（Masked）Self-Attention 输出**作为 **Query (`Q`)**。
    * 使用 **Encoder 的最终输出序列**作为 **Key (`K`) 和 Value (`V`)**。
    * **目的：** 让 Decoder 在生成当前目标词时，能够“查阅”或“关注”源语言（Encoder 输出）中最相关的部分。例如，在英译法时，生成法语单词“chien”时，交叉注意力会关注到源英语句子中的单词“dog”。
  * **多模态任务：** 连接不同模态的信息。
    * *图像描述生成：* Query 来自文本序列（描述词），Key/Value 来自图像特征（区域或网格）。
    * *视觉问答：* Query 来自问题文本，Key/Value 来自图像特征。
  * **检索增强生成：** Query 来自当前生成步骤或用户查询，Key/Value 来自外部知识库检索到的文档片段。
  * **融合不同来源信息的模型：** 需要整合来自两个独立编码器信息流的任何场景。

* **数学原理图示：**
  
  ```
  序列 A (Decoder输出): [A1, A2]  -> Q = [Q_A1, Q_A2] (来自A)
  序列 B (Encoder输出): [B1, B2, B3] -> K = [K_B1, K_B2, K_B3], V = [V_B1, V_B2, V_B3] (来自B)
  
  计算 Output_A1 (对应Decoder位置1):
      Score_A1 = [Q_A1·K_B1, Q_A1·K_B2, Q_A1·K_B3] / sqrt(d_k)
      Weights_A1 = softmax(Score_A1) = [w11, w12, w13]
      Output_A1 = w11*V_B1 + w12*V_B2 + w13*V_B3  (融合了Encoder位置1、2、3的信息)
  同理计算 Output_A2。
  ```

---

##### 区别与联系总结表

| 特性       | 自注意力 (Self-Attention)                    | 交叉注意力 (Cross-Attention)                          |
|:-------- |:---------------------------------------- |:------------------------------------------------ |
| **输入来源** | **单一序列** (`X`)                           | **两个序列** (`A` 提供 Query, `B` 提供 Key/Value)        |
| **Q来源**  | 输入序列 `X`                                 | 序列 `A`                                           |
| **K来源**  | 输入序列 `X`                                 | 序列 `B`                                           |
| **V来源**  | 输入序列 `X`                                 | 序列 `B`                                           |
| **核心功能** | **捕捉序列内部元素间的依赖关系**                       | **建立两个不同序列间元素的关联**                               |
| **主要目的** | 理解序列自身的上下文和结构                            | 从一个序列中检索信息以帮助处理或生成另一个序列                          |
| **典型应用** | Transformer Encoder, Decoder 的 Masked SA | Transformer Decoder (连接 Encoder 输出), 多模态任务, 检索增强 |
| **输出序列** | 与输入序列 `X` 同长度 (`len_X`)                  | 与 Query 序列 `A` 同长度 (`len_A`)                     |
| **输出内容** | `X` 自身 Value (`V_X`) 的加权和                | `B` 的 Value (`V_B`) 的加权和                         |
| **计算关系** | `Output[i] = f(X[i], X[1..len_X])`       | `Output_A[i] = f(A[i], B[1..len_B])`             |
| **联系**   | **共享相同的底层数学操作** (缩放点积注意力)                |                                                  |
| **联系**   | 可学习权重矩阵 `W^Q`, `W^K`, `W^V` 结构相同         |                                                  |
| **联系**   | 在复杂模型（如Transformer）中**串联或并联使用**          |                                                  |

---

##### 关键联系再强调

1. **共同基础：** 它们都基于**缩放点积注意力机制**。计算注意力权重 (`softmax(QK^T / sqrt(d_k))`) 和加权求和 (`A * V`) 的数学操作是完全相同的。区别仅在于 `Q`, `K`, `V` 的来源定义。
2. **参数结构：** 它们都使用可学习的线性变换矩阵 (`W^Q`, `W^K`, `W^V`) 来生成 `Q`, `K`, `V`。这些矩阵的结构（输入输出维度）在两种注意力中本质相同，尽管参数值通常是独立学习的（尤其是在不同层）。
3. **协同工作：** 在完整的Transformer架构（尤其是Decoder）中，自注意力和交叉注意力是**协同工作**的：
   * **Decoder：** 首先使用 **Masked Self-Attention** 理解已生成的目标序列部分的内部关系。
   * 然后使用 **Cross-Attention**，将上一步得到的表示作为 `Q`，去关注 `Encoder` 的输出 (`K`, `V`)，从而融合源序列的关键信息。
   * 最后通过前馈网络生成最终输出并预测下一个词。

##### 总结

* **自注意力** 是**向内看**，聚焦于**自身序列**内部元素之间的关系，用于构建丰富的上下文表示。它是理解单个序列的基础。
* **交叉注意力** 是**向外看**，聚焦于**另一个序列**中的相关信息，用于在序列之间建立桥梁和信息传递。它是实现序列到序列转换、信息检索和多模态融合的关键。

理解它们如何共享基础计算单元 (`Q`, `K`, `V`, `softmax`, 加权和) 但又通过定义 `Q`, `K`, `V` 的来源来实现截然不同的功能，是掌握Transformer及其强大能力的关键。它们在模型的不同层、不同组件中灵活组合，共同赋予了模型处理复杂序列任务（如翻译、摘要、问答）的卓越能力。

#### **5.位置编码 (Positional Encoding, PE)**

- **核心问题：** 自注意力机制本身是**置换不变（Permutation Invariant）** 的。它计算的是序列元素之间的相关性，但**完全不考虑元素在序列中的绝对位置或相对顺序**。对于“The cat chased the mouse”和“The mouse chased the cat”这样的句子，如果只靠自注意力，模型可能无法区分谁追谁。

- **核心思想：** 向模型的输入嵌入（Input Embeddings）中**显式注入位置信息**，使模型能够感知序列元素的**绝对位置**和/或**相对位置**。

- **数学原理（Sinusoidal PE - Transformer原论文方法）：**
  
  - 使用一组**预定义**的、**固定**的正弦和余弦函数来生成位置编码向量。
  
  - 对于序列中的位置 `pos`（从 `0` 到 `seq_len-1`）和嵌入维度 `i`（从 `0` 到 `d_model-1`），位置编码向量 `PE(pos)` 的第 `i` 维值计算如下：  
    `PE_{(pos, 2k)} = sin(pos / 10000^{2k / d_model})`  
    `PE_{(pos, 2k+1)} = cos(pos / 10000^{2k / d_model})`
    
    - 其中 `k` 的范围是 `0` 到 `d_model/2 - 1`。
  
  - **直观解释：**
    
    - 每个位置 `pos` 对应一个唯一的 `d_model` 维向量 `PE(pos)`。
    
    - 维度 `i` 对应一个波长从 `2π` (当 `i=0`) 到 `2π * 10000` (当 `i=d_model-1`) 的三角函数。不同维度对应不同频率的正弦/余弦波。
    
    - 偶数维度 (`2k`) 用正弦，奇数维度 (`2k+1`) 用余弦。
    
    - 频率 `10000^{2k / d_model}` 随维度 `i` 增加而**指数级下降**。

- **实现方式：**
  
  1. **生成编码矩阵：** 预先计算好一个矩阵 `PE`，形状为 `[max_seq_len, d_model]`。矩阵的第 `pos` 行就是 `PE(pos)`。
  
  2. **添加到输入嵌入：** 对于输入序列 `X`（形状 `[seq_len, d_model]`，由词嵌入得到），将位置编码矩阵 `PE` 的前 `seq_len` 行直接**加**到 `X` 上：  
     `X_final = X + PE[0:seq_len, :]`
     
     - 这是Transformer原论文和最常见的方式。
  
  3. **可选：** 有时也会考虑拼接（Concatenation）或其他融合方式，但加法是最简单有效的。

- **使用场景：**
  
  - **所有基于Transformer的模型：** 只要模型需要处理**有序序列**（文本、时间序列、音频帧、图像patch序列等），就必须使用位置编码。
  
  - **必须位置感知的任务：** 机器翻译（词序至关重要）、文本摘要、问答、图像分类（ViT中需要给图像patch编码位置）、语音识别等。

- **为什么用正弦/余弦？**
  
  - **相对位置编码：** 关键优势在于它能**自然地编码相对位置**。对于固定的偏移量 `k`，`PE_{pos+k}` 可以表示为 `PE_{pos}` 的一个**线性变换**（与 `pos` 无关）。这使得模型更容易学习关注“相对距离为k”的位置。
  
  - **值域有界：** 正弦余弦函数的值域在 `[-1, 1]` 之间，与经过归一化处理的词嵌入值域匹配。
  
  - **外推到更长序列：** 理论上，正弦/余弦编码可以处理比训练时见过的序列更长的序列（虽然效果可能下降）。

- **其他位置编码方案：**
  
  - **可学习的位置嵌入 (Learned Positional Embeddings)：** 将位置 `pos` 视为一个类别，为每个位置学习一个嵌入向量（类似词嵌入）。BERT、GPT等常用此方法。
    
    - *优点：* 更灵活，可能学习到更复杂的模式。
    
    - *缺点：* 固定最大长度限制，无法外推；需要额外参数；训练数据不足时可能不如Sinusoidal PE。
  
  - **相对位置编码 (Relative Position Encodings)：** 不编码绝对位置，而是编码元素之间的相对距离（如 `i-j`），通常融入到注意力分数的计算中（如T5, Transformer-XL）。更符合注意力机制的本质。
  
  - **旋转位置编码 (Rotary Position Embedding, RoPE)：** 通过旋转操作将位置信息融入Q、K向量本身（如LLaMA, GPT-NeoX）。在长文本上表现优异。

- **与注意力机制的联系：**
  
  - **不可或缺的补充：** 位置编码是解决自注意力机制**置换不变性**缺陷的关键手段。没有位置编码，Transformer无法处理有序序列任务。
  
  - **协同工作：** 位置信息通过加法被注入到词嵌入中，形成最终的输入表示 `X_final`。然后，多头注意力机制在 `X_final` 上计算，同时利用了**词汇语义信息**（来自词嵌入）和**位置信息**（来自PE）。模型在学习Q、K、V投影时，自然地融合了这两种信息。

```
class PositionalEncoding(tf.keras.layers.Layer):
      def __init__(self,max_length,embed_size,dtype=tf.float32,**kwargs):
          super().__init__(dtype=dtype,**kwargs)
          assert embed_size %2==0,"embed_size must be even"
          p,i=np.meshgrid(np.array(max_length),2*np.arange(embed_size//2))
          pos_emb=np.empty((1,max_length,embed_size))
          pos_emb[0,:,::2]=np.sin(p/10_000**(i/embed_size)).T
          pos_emb[0,:,1::2]=np.cos(p/10_000**(i/embed_size)).T
          self.pos_encodings=tf.constant(pos_emb.astype(self.dtype))
          self.supports_masking=True
      def call(self,inputs):
          batch_max_length=tf.shape(inputs)[1]
          return inputs+self.pos_encodings[:,:batch_max_length]

pos_embed_layer=PositionalEncoding(max_length,embed_size)
encoder_in=pos_embed_layer(encoder_embeddings)
decoder_in=pos_embed_layer(decoder_embeddings)
```

#### **6.多头注意力 (Multi-Head Attention, MHA)**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-31-10-50-50-image.png" alt="" width="343" data-align="center">

<img title="" src="https://i-blog.csdnimg.cn/img_convert/604e4bb1216d4a3d6296aae31bd0f34b.png" alt="" width="581" data-align="center">

- **核心思想：** 将单一的注意力计算过程**并行化**为 `h` 个独立的“头”（heads），每个头在**不同的子空间（子表示）** 中学习关注输入序列的不同方面或模式，最后将各头的结果**拼接（concatenate）** 并投影回原始维度。

- **目的：** 增强模型的表达能力，使其能够**同时关注来自不同位置的不同类型的依赖关系**（例如，关注语法关系、语义关系、指代关系等）。单一注意力头可能无法充分捕捉这些复杂多样的模式。

- **数学原理与实现步骤：**
  
  1. **线性投影（分头）：** 对输入序列 `X` (形状 `[seq_len, d_model]`) 分别应用 `h` 组独立的线性投影，得到 `h` 组 `Q`, `K`, `V` 矩阵。每组投影将 `d_model` 维向量降到 `d_k`, `d_k`, `d_v` 维（通常 `d_k = d_v = d_model / h`）。
     
     - `head_i = Attention(Q_i, K_i, V_i) = softmax( (Q_i * K_i^T) / sqrt(d_k) ) * V_i`
     
     - 其中 `Q_i = X * W_i^Q` (形状 `[seq_len, d_k]`), `K_i = X * W_i^K`, `V_i = X * W_i^V`。`W_i^Q`, `W_i^K`, `W_i^V` 是第 `i` 个头独有的可学习投影矩阵。
  
  2. **独立注意力计算：** 对每个头 `i`，使用缩放点积注意力公式独立计算其输出 `head_i` (形状 `[seq_len, d_v]`)。
  
  3. **拼接（Concatenate）：** 将所有 `h` 个头的输出 `head_i` 沿着特征维度拼接起来，形成一个大的矩阵 (形状 `[seq_len, h * d_v]`)。因为 `d_v = d_model / h`，所以 `h * d_v = d_model`。
  
  4. **最终线性投影：** 将拼接后的结果通过一个**可学习的线性投影层** `W^O` (形状 `[d_model, d_model]`) 映射回 `d_model` 维空间。这一步允许模型整合来自不同头的信息。
     
     - `MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h) * W^O` (形状 `[seq_len, d_model]`)

- **公式总结：**  
  `MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W^O`  
  `where head_i = Attention(Q * W_i^Q, K * W_i^K, V * W_i^V)`

- **使用场景：**
  
  - **Transformer Encoder/Decoder 的核心层：** 无论是处理输入的自注意力（Encoder），还是Decoder中处理自身输出的自注意力或处理Encoder输出的交叉注意力，都使用多头注意力机制。它是Transformer模型能力的基石。
  
  - **替代RNN/CNN：** 在任何需要建模长距离依赖和复杂上下文关系的序列任务中，MHA都是强大的基础模块。

- **优势：**
  
  - **并行化计算：** `h` 个头的计算可以完全并行，提高效率。
  
  - **增强表征能力：** 允许模型在不同子空间关注不同模式的信息，学习更丰富、更鲁棒的表示。
  
  - **降低维度计算量：** 每个头在降维后的空间 (`d_k`, `d_v`) 计算注意力，计算复杂度 `O(seq_len^2 * d_k)` 相对于在 `d_model` 维空间 (`O(seq_len^2 * d_model)`) 更低，虽然总计算量可能接近（因为有 `h` 个头），但并行性更好。

- **与单头注意力的联系：** 当 `h=1` 且 `d_k = d_v = d_model` 时，多头注意力退化回单头注意力（但通常 `W^O` 层仍然存在）。MHA是单头注意力的泛化和增强。

#### **7.掩码多头注意力 (Masked Multi-Head Attention)**

- **核心思想：** 在多头注意力的**注意力权重计算阶段**（`softmax`之前）引入一个**掩码（Mask）**，强制将某些位置的注意力权重设置为一个**极小的负值（如 `-inf`）**，使得 `softmax` 后这些位置的权重趋近于0，从而阻止模型关注这些位置。

- **目的：** 在**自回归生成（Autoregressive Generation）** 场景下（如机器翻译、文本生成），确保解码器在预测**当前位置**的输出时，**只能依赖该位置之前已生成的输出**，而不能“偷看”未来的信息。这是保证序列生成正确性的关键。

- **数学原理与实现：**
  
  1. **计算原始分数：** 与标准多头注意力一样，计算每个头的 `(Q * K^T) / sqrt(d_k)` (形状 `[seq_len, seq_len]`)。这个矩阵包含了所有Query-Key对的原始相关性分数。
  
  2. **应用掩码：** 创建一个掩码矩阵 `M` (形状 `[seq_len, seq_len]`)，通常是一个**下三角矩阵（Lower Triangular Matrix）**：
     
     - `M[i, j] = 0` 当 `j <= i` (允许关注当前位置 `i` 及之前的位置 `j`)
     
     - `M[i, j] = -inf` (或一个非常大的负数，如 `-1e9`) 当 `j > i` (禁止关注当前位置 `i` 之后的位置 `j`)
  
  3. **掩码加法：** 将掩码矩阵 `M` **加到**原始分数矩阵上：`MaskedScores = (Q * K^T) / sqrt(d_k) + M`
     
     - 对于被禁止的位置 (`j > i`)，其分数 `(Q[i]·K[j]) / sqrt(d_k)` 加上 `-inf` 后结果仍为 `-inf`。
  
  4. **Softmax：** 对 `MaskedScores` 按行（`dim=-1`）应用 `softmax` 函数。
     
     - 对于 `-inf` 的位置，`softmax` 后权重会变为 `0`。
     
     - 只有允许关注的位置 (`j <= i`) 的权重会被正常计算。
  
  5. **加权求和：** 使用掩码后的权重矩阵与 `V` 进行加权求和，得到掩码注意力的输出。后续的拼接和线性投影 `W^O` 步骤与标准多头注意力相同。

- **公式（关键步骤）：**  
  `head_i = softmax( (Q_i * K_i^T) / sqrt(d_k) + M ) * V_i`

- **使用场景：**
  
  - **Transformer Decoder 的第一层：** 这是掩码多头注意力的经典应用位置。Decoder在处理**目标序列自身**时（即**自注意力**部分），必须使用掩码，确保在预测第 `t` 个词时只能看到第 `1` 到 `t-1` 个词。
  
  - **任何需要防止信息泄露的自回归模型：** 语言建模、序列生成任务（如音乐、代码生成）。

- **与非掩码注意力的区别与联系：**
  
  - **区别：** 核心区别在于是否在 `softmax` 之前应用了**防止关注未来位置的掩码 `M`**。掩码注意力**限制**了注意力的范围。
  
  - **联系：**
    
    - 底层计算单元（缩放点积）和多头机制完全相同。
    
    - 在Decoder中，掩码多头自注意力后面通常紧接着一个**非掩码的交叉注意力层**（使用Encoder的输出作为K, V），该层可以看到完整的源序列信息。
    
    - 在**Encoder**中使用的自注意力是**非掩码**的，因为Encoder需要看到整个输入序列来建立全局表示。
    
    - 在**训练**Decoder时使用掩码自注意力，在**推理**（生成）时，由于是逐词生成，自然只能看到已生成的部分，掩码确保了训练和推理行为的一致性。

- **多头注意力 (MHA)：** Transformer的**核心动力引擎**。通过并行多个注意力头，在不同子空间捕捉多样化的依赖关系，显著提升模型的表征能力。用于Encoder的自注意力、Decoder的掩码自注意力和交叉注意力。

- **掩码多头注意力 (Masked MHA)：** MHA的一种**特殊形式**，专为**自回归生成**设计。通过在注意力权重计算中引入**下三角掩码**，确保模型在预测当前位置时**只能依赖过去信息**，防止信息泄露。是Decoder自注意力层的标配。

- **位置编码 (PE)：** 解决自注意力**置换不变性**问题的**关键补充**。通过将**位置信息**（绝对或相对）显式注入输入表示（通常用Sinusoidal函数或可学习嵌入），使模型能够理解序列的顺序。是所有处理有序序列的Transformer模型的必需品。

## **七.Transformer Model in Hugging Face**

这里具体就不细说了，主要是引用Hugging Face山的预训练模型做一些文本分类，情感分析之类的，已经是属于NLP及多模态领域内容，感兴趣的可以自己研究研究：

https://huggingface.co/tasks  https://huggingface.co/models,https://huggingface.co/datasets

```
from transformers import pipeline

classifier = pipeline("sentiment-analysis")  # many other tasks are available
result = classifier("The actors were very convincing.")
```

```
result
```

```
classifier(["I am from India.", "I am from Iraq."])
```

```
model_name = "huggingface/distilbert-base-uncased-finetuned-mnli"
classifier_mnli = pipeline("text-classification", model=model_name)
classifier_mnli("She loves me. [SEP] She loves me not.")
```

```
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = TFAutoModelForSequenceClassification.from_pretrained(model_name) 

#——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————
token_ids = tokenizer(["I like soccer. [SEP] We all love soccer!",
                       "Joe lived for a very long time. [SEP] Joe is old."],
                      padding=True, return_tensors="tf")
token_ids 
#——————————————————————————————————————————————————————————————————————————————————————
token_ids = tokenizer([("I like soccer.", "We all love soccer!"),
                       ("Joe lived for a very long time.", "Joe is old.")],
                      padding=True, return_tensors="tf")
token_ids
#————————————————————————————————————————————————————————————————————————————————————————————————————————————

outputs = model(token_ids)
outputs   

#——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————
Y_probas = tf.keras.activations.softmax(outputs.logits)#logits代表原始未归一化后输出值
Y_probas
Y_pred = tf.argmax(Y_probas, axis=1)
Y_pred  # 0 = contradiction, 1 = entailment, 2 = neutral  
```

```
#训练样本，微调：
sentences = [("Sky is blue", "Sky is red"), ("I love her", "She loves me")]
X_train = tokenizer(sentences, padding=True, return_tensors="tf").data
y_train = tf.constant([0, 2])  # contradiction, neutral
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(loss=loss, optimizer="nadam", metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=2)
```

注：由于模型输出的是logits而不是概率，所以需要用tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)作为损失函数，而不是

sparse_categorical_crossentropy
