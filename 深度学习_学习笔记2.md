# Deep Neural Networks参数配置

1.有关梯度消失，梯度爆炸问题及常见的解决方案

2.迁移学习和无监督预训练

3.讨论各种优化器

4.学习率策略

5.适用于大型神经网络的正则化技术

6.总结

## **一.梯度问题**

反向传播算法的第二阶段通过从输出层到输入层的传播，沿途传播误差梯度。一旦算法计算出成本函数对网络中每个参数的梯度，它便利用这些梯度通过梯度下降步骤更新每个参数。遗憾的是，随着算法向下层推进，梯度往往会变得越来越小。因此，梯度下降更新会使下层的连接权重几乎保持不变，训练也无法收敛到一个良好的解。这就是所谓的消失梯度问题。

在某些情况下，情况可能相反：梯度会变得越来越大，直到层级获得异常巨大的权重更新，导致算法发散。这就是梯度爆炸问题，它在循环神经网络中最为常见。

#### **1.权重初始化技术：稳定方差原理**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-12-19-13-11-image.png" alt="" width="562" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-12-19-15-08-image.png" alt="" width="365" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-12-18-56-53-image.png" alt="" width="462" data-align="center">

| 参数             | 可选值                            | 作用                      |
| -------------- | ------------------------------ | ----------------------- |
| `scale`        | 浮点数                            | 方差缩放因子 (Glorot=1, He=2) |
| `mode`         | "fan_in", "fan_out", "fan_avg" | 分母计算方式                  |
| `distribution` | "normal", "uniform"            | 分布类型                    |

| 初始化方法  | 训练速度   | 收敛稳定性 | 适用场景           |
|:------:|:------:|:-----:|:--------------:|
| 随机初始化  | 慢(×1)  | 经常发散  | 不推荐            |
| Glorot | 快(×3)  | 稳定    | Sigmoid/Tanh网络 |
| He     | 很快(×5) | 非常稳定  | ReLU网络         |

```
#直接使用预设初始化器

# Glorot初始化（默认）
dense = tf.keras.layers.Dense(50, activation="tanh") 

# He初始化（ReLU推荐）
dense = tf.keras.layers.Dense(50, activation="relu", 
                              kernel_initializer="he_normal")

# LeCun初始化（SELU推荐）
dense = tf.keras.layers.Dense(50, activation="selu",
                              kernel_initializer="lecun_normal") 


# 自定义He初始化（基于fan_avg）
he_avg_init = tf.keras.initializers.VarianceScaling(
    scale=2.,          # 方差缩放因子,或者也可以调为1
    mode="fan_avg",    # 使用(fan_in + fan_out)/2,fan_in
    distribution="uniform"  # 均匀分布/正态分布
)

dense = tf.keras.layers.Dense(50, activation="sigmoid",
                              kernel_initializer=he_avg_init)
```

#### **2.更好的激活函数使用**

1.Leaky ReLU(在零点处不光滑，不连续性可能导致梯度下降在最优点附近跳跃，减缓收敛速度)

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-12-19-20-19-image.png" alt="" width="457" data-align="center">

```
leaky_relu=tf.keras.layers.LeakyReLU(alpha=0.2)# default alpha=0.3
dense=tf.keras.layers.Dense(50,activation=leaky_relu,
                                kernel_initializer="he_normal")
model=tr.keras.layers.Sequential([
                      [...] #这里是很多层
                      tf.keras.layers.Dense(50,kernel_initializer="he_normal"),
                      tf.keras.layers.LeakyReLU(alpha=0.2),
                      [...] #很多层])
```

2.ELU,SELU(光滑变体)

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-12-19-28-22-image.png" alt="" width="451" data-align="center">

在 Keras 中使用 ELU 只需设置 activation="elu"，与其他 ReLU 变体一样，应使用 He 初始化。ELU激活函数的主要缺点是其计算速度慢于ReLU函数及其变体（因使用了指数函数）。其在训练过程中的更快收敛速率可能弥补这一计算延迟，但测试时ELU网络仍会比ReLU网络稍慢。

缩放 ELU（SELU）激活函数：顾名思义，它是 ELU 激活函数的缩放变体（约为 ELU 的 1.05 倍，使用 α ≈1.67）。在Keras中使用它，只需设置activation=“selu”

Gunter Klambauer指出：自规范现象的发生（每层输出层趋向于均值为0，方差为1的分布，解决梯度消失问题）：输入特征必须标准化：均值为0，标准差为1。每个隐藏层的权重必须使用 LeCun 标准初始化进行初始化。在 Keras 中，这意味着设置kernel_initializer="lecun_normal"。

若在其他架构中使用 SELU，如循环神经网络（见第 15 章）或包含跳跃连接的网络（即跳过层级的连接，如Wide & Deep 网络），其性能可能不会优于 ELU。您无法使用正则化技术，如 ℓ1或 ℓ2正则化、最大范数、批量归一化或常规 dropout（这些将在本章后文讨论）。这些是重大限制，因此尽管 SELU 具有潜力，但并未获得广泛采用。

3.GELU,Swish,Mish

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-12-19-42-50-image.png" alt="" width="452" data-align="center">

GELU(z) = z Φ(z),Φ(z) 对应于从均值为 0、方差为 1 的正态分布中随机抽取的值小于 z 的概率

泛化后的Swish 函数为 Swishβ(z) = zσ(βz)，因此GELU大致等同于使用β = 1.702的广义Swish函数

(可以像调整其他超参数一样调整β。此外，也可以将β设为可训练参数，并让梯度下降算法进行优化)

mish(z) = ztanh(softplus(z))，其中 softplus(z) = log(1 + exp(z))

**激活函数的选择建议：**

**ReLU仍然是简单任务的良好默认选择：它通常与更复杂的激活函数一样好，而且计算速度非常快，许多库和硬件加速器都提供了ReLU专用的优化。然而，对于更复杂的任务，Swish可能是更好的默认选择，你甚至可以尝试带可学习β参数的参数化Swish来处理最复杂的任务。Mish可能能带来略微更好的结果，但它需要更多的计算资源。Keras 默认支持 GELU 和 Swish；只需使用activation="gelu" 或 activation="swish"。然而，它目前不支持Mish或广义Swish激活函数**

#### **3.批量归一化**

Sergey Ioffe和Christian Szegedy提出了一种名为批量归一化（BN）的技术来解决这些问题。该技术是在模型中**每个隐藏层的激活函数前后添加一个操作**。该操作仅将**每个输入归零并归一化**，然后使用每个层的两个新参数向量对结果进行**缩放和偏移**：一个用于缩放，另一个用于偏移。换句话说，该操作使模型能够学习每个层输入的最佳缩放因子和均值

##### 四大核心参数

| 参数        | 学习方式   | 作用       |
| --------- | ------ | -------- |
| γ (gamma) | 反向传播   | 缩放标准化后的值 |
| β (beta)  | 反向传播   | 偏移标准化后的值 |
| μ (mu)    | 指数移动平均 | 全局输入均值   |
| σ (sigma) | 指数移动平均 | 全局输入标准差  |

```
#在激活函数之后添加BN层
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(300, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(10, activation="softmax")
])

#在激活函数之前添加BN层：
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(300, kernel_initializer="he_normal", use_bias=False),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation("relu"),
    tf.keras.layers.Dense(100, kernel_initializer="he_normal", use_bias=False),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation("relu"),
    tf.keras.layers.Dense(10, activation="softm

amodel.compile(loss="sparse_categorical_crossentropy", optimizer="sgd",
              metrics=["accuracy"])
```

**BN位置选择**：推荐：卷积/全连接 → BN → 激活函数

#### **4.梯度裁剪**

在反向传播过程中对梯度进行裁剪，确保其值不会超过某个阈值。该技术通常应用于循环神经网络，因为在此类网络中使用批量归一化较为复杂

```
optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)#按值裁剪
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer) 该优化器将把梯度向量的每个分量裁剪到-1.0到1.0之间的值。这意味着损失函数的所有偏导数（针对每个可训练参数）都将被裁剪到-1.0到1.0之间。阈值是一个可调的超参数。
```

上述代码：该优化器将把梯度向量的每个分量裁剪到-1.0到1.0之间的值。这意味着损失函数的所有偏导数（针对每个可训练参数）都将被裁剪到-1.0到1.0之间。阈值是一个可调的超参数。

```
optimizer = tf.keras.optimizers.SGD(clipnorm=1.0)#范数裁剪
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer)
```

注意：如果原始梯度向量为[0.9, 100.0]，它主要指向第二个轴的方向；但一旦按值进行裁剪，得到[0.9, 1.0]，它大致指向两个轴之间的对角线。如果希望确保梯度裁剪不会改变梯度向量的方向，应通过设置clipnorm而非clipvalue进行范数裁剪。

## **二.迁移学习（复用训练层）**

尝试找到一个已经完成类似任务的现有神经网络（如何找还是未知），如果找到了，通常可以复用其中大部分层，除了顶层。（在输入具有相似低级特征时效果最佳，输出层与顶部隐藏层往往需要替换）

任务越相似，越希望复用更多层（从下层开始）。对于非常相似的任务，**尽量保留所有隐藏层**，仅替换输出层。**首先尝试冻结所有复用层**（即使其权重不可训练，以便梯度下降不会修改它们，它们将保持固定），然后训练模型并观察其性能。**然后尝试解冻一两个顶层隐藏层**，让反向传播调整它们，并查看性能是否有所提升。**训练数据越多，可以解冻的层数就越多**。在解冻重复使用的层时，**降低学习率**也非常有用：这将避免破坏它们经过精细调优的权重。如果仍然无法获得良好的性能，而且训练数据较少，可以尝试删除顶层隐藏层，并再次冻结所有剩余的隐藏层。可以迭代直到找到合适的重复使用层数。如果有足够的训练数据，可以尝试替换顶层隐藏层而不是删除它们，甚至可以添加更多隐藏层。

下面是一个基于Keras的迁移学习例子：（用的还是fashion MNIST dataset）

```
pos_class_id=class_names.index("Pullover")
neg_class_id=class_names.index("T-shirt/top")


def split_dataset(X,y):
   y_for_B=(y==pos_class_id)|(y==neg_class_id)#标记出属于二分类任务（Pullover 或 T-shirt/top）的样本

   y_A=y[~y_for_B]#提取多分类任务（模型A）的标签数据

   y_B=(y[y_for_B]==pos_class_id).astype(np.float32)#创建二分类任务的二进制标签

   old_class_ids=list(set(range(10))-set([neg_class_id,pos_class_id])) #获取剩余的8个类别ID（用于多分类任务）

   #将原始的8个类别ID重新映射为连续的0-7，确保多分类任务的标签连续
   for old_class_id, new_class_id in zip(old_class_ids, range(8)):
        y_A[y_A == old_class_id] = new_class_id   

   return ((X[~y_for_B], y_A), (X[y_for_B], y_B))  


(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)
(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)
(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)
X_train_B = X_train_B[:200]
y_train_B = y_train_B[:200]

tf.random.set_seed(42)

model_A=tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=[28,28]),
  tf.keras.layers.Dense(100,activation="relu",
                            kernel_initializer="he_normal"),
  tf.keras.layers.Dense(100,activation="relu",
                            kernel_initializer="he_normal"),
  tf.keras.layers.Dense(100,activation="relu",
                            kernel_initializer="he_normal"),
  tf.keras.layers.Dense(8,activation="softmax")
])

model_A.compile(loss="sparse_categorical_crossentropy",
           optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
           metrics=["accuracy"])

history=model_A.fit(X_train_A,y_train_A,epochs=30,
                   validation_data=(X_valid_A,y_valid_A))

model_A.save("my_model_A.keras")

#训练B without A:
tf.random.set_seed(42)
model_B = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(1, activation="sigmoid")
])

model_B.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
                metrics=["accuracy"])
history = model_B.fit(X_train_B, y_train_B, epochs=20,
                      validation_data=(X_valid_B, y_valid_B))
model_B.evaluate(X_test_B, y_test_B)

#训练B with A:
model_A = tf.keras.models.load_model("my_model_A.keras")


#这里model_B_on_A共享一些层，训练model_B_on_A时会影响A，所以需要克隆A，防止此现象发生
model_A_clone = tf.keras.models.clone_model(model_A)
model_A_clone.set_weights(model_A.get_weights()) 

model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1])
model_B_on_A.add(tf.keras.layers.Dense(1, activation="sigmoid"))
```

现在可以训练模型 model_B_on_A 用于任务 B，但由于新的输出层是随机初始化的，它会产生较大的误差（至少在前几个 epoch 中），因此会产生较大的误差梯度，这可能破坏复用的权重。为了避免这种情况，一种方法是在前几个 epoch 中冻结重复使用的层，给新层一些时间来学习合理的权重。要做到这一点，将每个层的可训练属性设置为False，然后编译模型：

```
for layer in model_B_on_A.layers[:-1]:
    layer.trainable = False

optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)
model_B_on_A.compile(loss="binary_crossentropy", optimizer=optimizer,
                     metrics=["accuracy"])  

history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,
                           validation_data=(X_valid_B, y_valid_B))

for layer in model_B_on_A.layers[:-1]:
    layer.trainable = True

optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)
model_B_on_A.compile(loss="binary_crossentropy", optimizer=optimizer,
                     metrics=["accuracy"])
history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,
                           validation_data=(X_valid_B, y_valid_B)) 


model_B_on_A.evaluate(X_test_B, y_test_B)
```

注意：这里每个人得到的结果可能有好有坏，因为设置的类不同，随机种子不同，都可能对效果有影响，只能说有好的结果（运气使然），科学中许多结果无法重复的，上面只是个模板例子罢了，具体效果都需要实践证明！！！

迁移学习在深度卷积神经网络中效果最佳，这类网络倾向于学习更具通用性的特征检测器（尤其在较低层）。

**无监督预训练还没怎么了解，以后更深入点......**

## **三.优化器（优化算法）**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-13-11-36-49-image.png" alt="" data-align="center" width="487">

**常规的梯度下降法通过直接减去成本函数J(θ)对权重的梯度（∇J(θ)）乘以学习率η来更新权重θ。方程为θ ← θ – η∇J(θ)。它不关心之前的梯度值**

**1.动量**：动量优化**非常关注之前的梯度值**：在每次迭代中，它会将局部梯度从动量向量 m（乘以学习率 η）中减去，并通过添加该动量向量来更新权重（见公式 11-5）。换句话说，梯度被用作加速度，而非速度。算法引入了一个新的超参数 β，称为动量，其值必须在 0（高摩擦）和 1（无摩擦）之间设置。典型的动量值为 0.9。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-13-11-10-39-image.png" alt="" width="528" data-align="center">

```
optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)
```

**2.Nesterov加速梯度**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-13-11-15-58-image.png" alt="" width="408" data-align="center">

```
optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9,
nesterov=True)
```

**3.AdaGrad**

简而言之，该算法会衰减学习率，但对于陡峭维度比对于斜率较缓的维度衰减得更快。这被称为自适应学习率。它有助于将更新更直接地指向全局最优解。另一个好处是它对学习率超参数η的调优要求大大降低。

AdaGrad在简单的二次问题上表现良好，但在训练神经网络时常过早停止：学习率被缩小到如此程度，以至于算法在达到全局最优解前就完全停止了。因此，尽管Keras提供了AdaGrad
优化器，但不应将其用于训练深度神经网络（尽管它可能适用于线性回归等简单任务）。然而，理解AdaGrad有助于掌握其他自适应学习率优化器。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-13-11-18-43-image.png" alt="" width="466" data-align="center">

```
optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)
```

**4.RMSProp**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-13-11-24-37-image.png" alt="" width="484" data-align="center">

```
optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)
```

**5.Adam及其变体**

Adam（自适应矩估计）结合了动量优化和RMSProp的理念：与动量优化类似，它跟踪过去梯度的指数衰减平均值；与RMSProp类似，它跟踪过去梯度平方的指数衰减平均值。这些是梯度均值和（未中心化）方差的估计值。均值常被称为第一矩，而方差常被称为第二矩，因此该算法得名。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-13-11-26-02-image.png" alt="" width="418" data-align="center">

```
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9,
                                     beta_2=0.999)
```

变体：AdaMax,Nadam,AdamW

```
optimizer = tf.keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9,
                                       beta_2=0.999) 

optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9,
                                      beta_2=0.999) 

optimizer = tf.keras.optimizers.AdamW(weight_decay=1e-5, learning_rate=0.001,
                                      beta_1=0.9, beta_2=0.999)
```

## **四.学习率的选择**

1.如前面笔记上说的，可以通过训练模型数百次迭代，将学习率从一个非常小的值指数级增加到一个非常大的值，然后观察学习曲线并选择一个略低于学习曲线开始再次快速上升的那个值得学习率。然后重新初始化模型并使用该学习率；

2.从一个较大得学习率开始，在训练不再快速进步时降低它，以下是最常用的学习计划：

**Power scheduling(功率调度)**:$\eta (t)=\eta_0/(1+t/s)^c$  t:迭代次数，$\eta_0$:初始学习率，c:常设为1,s:超参数

```
 lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
 initial_learning_rate=0.01,
 decay_steps=10_000,
 decay_rate=1.0,
 staircase=False
)
optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)
```

**Exponential scheduling(指数学习率调度)**:$\eta(t)=\eta_00.1^{t/s}$   

```
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.01,
    decay_steps=20_000,
    decay_rate=0.1,
    staircase=False
)
optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule) 
```

**Piecewise constant scheduling(分段常数调度)**:在一定数量的 epoch 中使用常数学习率（例如 η0= 0.1 持续 5个 epoch），然后在另一段 epoch 中使用较小的学习率（例如η1 = 0.001，持续50个 epoch），依此类推。尽管这种方法可以工作得非常出色，但需要反复调整以确定合适的学习率序列以及每个学习率的使用时长。

```
lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries=[50_000, 80_000],
    values=[0.01, 0.005, 0.001]
)
optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)
```

**Performance scheduling(性能调度)**：每N步测量验证误差（与早期停止类似），当误差停止下降时，将学习率降低λ倍。

```
model = build_model()
optimizer = tf.keras.optimizers.SGD(learning_rate=lr0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
#当最佳验证集损失没有提高在5次迭代之内，学习率直接乘以0.5
lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
history = model.fit(X_train, y_train, epochs=n_epochs,
                    validation_data=(X_valid, y_valid),
                    callbacks=[lr_scheduler])
```

**1cycle scheduling**:它首先通过线性增加初始学习率 η0，在训练过程中线性增长至 η1。随后在训练的后半段，学习率再次线性下降至 η0，并在最后几个 epoch 中将学习率线性降低多个数量级（仍为线性下降）。最大学习率η1的选取方法与寻找最优学习率的方法相同，而初始学习率η0通常为其十分之一。当使用动量时，我们首先设置较高的动量值（例如0.95），然后在训练的前半段将其线性降低到较低的动量（例如降低到0.85），接着在训练的后半段将其恢复到最大值（例如0.95），并在最后几个 epoch 中使用该最大值完成训练。**(没咋弄懂...抄书上代码...)**

```
class ExponentialLearningRate(tf.keras.callbacks.Callback):
    def __init__(self, factor):
        self.factor = factor
        self.rates = []
        self.losses = []

    def on_epoch_begin(self, epoch, logs=None):
        self.sum_of_epoch_losses = 0

    def on_batch_end(self, batch, logs=None):
        mean_epoch_loss = logs["loss"]  # the epoch's mean loss so far 
        new_sum_of_epoch_losses = mean_epoch_loss * (batch + 1)
        batch_loss = new_sum_of_epoch_losses - self.sum_of_epoch_losses
        self.sum_of_epoch_losses = new_sum_of_epoch_losses
        lr = self.model.optimizer.learning_rate.numpy()
        self.rates.append(lr)
        self.losses.append(batch_loss)
        self.model.optimizer.learning_rate = lr * self.factor  

def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=1e-4,
                       max_rate=1):
    init_weights = model.get_weights()
    iterations = math.ceil(len(X) / batch_size) * epochs
    factor = (max_rate / min_rate) ** (1 / iterations)
    init_lr = K.get_value(model.optimizer.learning_rate)
    model.optimizer.learning_rate = min_rate
    exp_lr = ExponentialLearningRate(factor)
    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,
                        callbacks=[exp_lr])
    model.optimizer.learning_rate = init_lr
    model.set_weights(init_weights)
    return exp_lr.rates, exp_lr.losses     


def plot_lr_vs_loss(rates, losses):
    plt.plot(rates, losses, "b")
    plt.gca().set_xscale('log')
    max_loss = losses[0] + min(losses)
    plt.hlines(min(losses), min(rates), max(rates), color="k")
    plt.axis([min(rates), max(rates), 0, max_loss])
    plt.xlabel("Learning rate")
    plt.ylabel("Loss") 
    plt.grid()  


model = build_model()
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
              metrics=["accuracy"])     

batch_size = 128
rates, losses = find_learning_rate(model, X_train, y_train, epochs=1,
                                   batch_size=batch_size)
plot_lr_vs_loss(rates, losses)  


class OneCycleScheduler(tf.keras.callbacks.Callback):
    def __init__(self, iterations, max_lr=1e-3, start_lr=None,
                 last_iterations=None, last_lr=None):
        self.iterations = iterations
        self.max_lr = max_lr
        self.start_lr = start_lr or max_lr / 10
        self.last_iterations = last_iterations or iterations // 10 + 1
        self.half_iteration = (iterations - self.last_iterations) // 2
        self.last_lr = last_lr or self.start_lr / 1000
        self.iteration = 0

    def _interpolate(self, iter1, iter2, lr1, lr2):
        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1

    def on_batch_begin(self, batch, logs):
        if self.iteration < self.half_iteration:
            lr = self._interpolate(0, self.half_iteration, self.start_lr,
                                   self.max_lr)
        elif self.iteration < 2 * self.half_iteration:
            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration,
                                   self.max_lr, self.start_lr)
        else:
            lr = self._interpolate(2 * self.half_iteration, self.iterations,
                                   self.start_lr, self.last_lr)
        self.iteration += 1
        self.model.optimizer.learning_rate = lr   



model = build_model()
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=tf.keras.optimizers.SGD(),
              metrics=["accuracy"])
n_epochs = 25
onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs,
                             max_lr=0.1)
history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size,
                    validation_data=(X_valid, y_valid),
                    callbacks=[onecycle])
```

## **五.正则化技术避免过拟合**

正则化技术：

#### **1.early stopping 哈**

#### **2.批量归一化**，添加一些batch层（虽然说为了解决梯度不稳定问题，但是也是一个正则化手段）

#### **3.L1,L2正则化**

```
layer = tf.keras.layers.Dense(100, activation="relu",
                    kernel_initializer="he_normal",
                    kernel_regularizer=tf.keras.regularizers.l2(0.01))
#这里l2(0.01)/l1(0.1)/l1_l2(0.1,0.01)都行 


#应用到每一层：
from functools import partial

RegularizedDense = partial(tf.keras.layers.Dense,
                           activation="relu",
                           kernel_initializer="he_normal",
                           kernel_regularizer=tf.keras.regularizers.l2(0.01))

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    RegularizedDense(100),
    RegularizedDense(100),
    RegularizedDense(10, activation="softmax")
])
```

**注意：当使用SGD，动量优化，以及Nesterov优化器时，正则化L1/L2都可以的，但是不适用于Adam及其变体。如果想使用Adam并结合权重衰减，那么不要正则化，改用AdamW**

#### **4.dropout（最常用）**

在每次训练步骤中，每个神经元（包括输入神经元，但始终排除输出神经元）都有一个概率p被暂时“dropout”，这意味着它将在此训练步骤中被完全忽略，但在下一个步骤中可能再次活跃。超参数p称为dropout率，通常设置在10%至50%之间。**在实际应用中，通常只需对顶层一到三层（不包括输出层）的神经元应用dropout。**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-13-19-25-26-image.png" alt="" width="305" data-align="center">

**在循环神经网络中接近20%–30%，而在卷积神经网络中接近40%–50%。**

```
tf.random.set_seed(42)  

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dropout(rate=0.2),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dropout(rate=0.2),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dropout(rate=0.2),
    tf.keras.layers.Dense(10, activation="softmax")
])

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))
```

这里需要**注意：dropout层只对训练集奏效，所以训练出来得到的性能可能会低于验证集的效能(训练时因为丢弃了一部分神经元，模型的容量小于验证集模型容量)，这是一个假象，真正的训练误差和准确性需要将dropout去掉再评估：**

```
# 评估训练集（无Dropout）
train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)

# 评估测试集（无Dropout）
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0){train_acc:.4f}")
```

**MCdropout**:(倾向于提高模型概率估计的可靠性)

model(X)与model.predict(X)类似，**但它返回的是张量而非NumPy数组**，并且支持training参数。
在此代码示例中，**设置training=True可确保Dropout层保持激活状态**，因此所有预测结果会略有不同。**我们对测试集进行100次预测，并计算其平均值**。具体来说，每次调用模型都会返回一个矩阵，其中每行对应一个实例，每列对应一个类别。由于测试集包含10,000个实例和10个类别，因此这是一个形状为[10000, 10]的矩阵。我们将100 个这样的矩阵堆叠起来，因此 y_probas 是一个形状为 [100, 10000, 10] 的 3D 数组。当我们对**第一个维度（轴=0）取平均值**时，得到 y_proba，一个形状为 [10000, 10] 的数组，就像我们使用单次预测时得到的结果一样。就这样！通过启用dropout对多个预测进行平均，我们得到一个**蒙特卡洛估计**，通常比在dropout关闭时单次预测的结果更可靠

```
tf.random.set_seed(42) 
y_probas = np.stack([model(X_test, training=True)
                     for sample in range(100)])
y_proba = y_probas.mean(axis=0)  


y_pred = y_proba.argmax(axis=1)
accuracy = (y_pred == y_test).sum() / len(y_test)
accuracy
```

```
class MCDropout(tf.keras.layers.Dropout):
    def call(self, inputs, training=None):
        return super().call(inputs, training=True)

#how to convert Dropout to MCDropout in a Sequential model 

Dropout = tf.keras.layers.Dropout
mc_model = tf.keras.Sequential([
    MCDropout(layer.rate) if isinstance(layer, Dropout) else layer
    for layer in model.layers
])

mc_model.set_weights(model.get_weights()) 

tf.random.set_seed(42)
np.mean([mc_model.predict(X_test[:1])
         for sample in range(100)], axis=0).round(2)
```

**5.最大范数正则化**

对于每个神经元，它限制其输入连接的权重w，使得∥ w ∥ ≤ r，其中r是最大范数超参数，∥ · ∥是ℓ范数。最大范数正则化不会在整体损失函数中添加正则化损失项。相反，它通常通过在每次训练步骤后计算∥ w ∥，并在必要时对 w 进行缩放（w ← w r /∥ w ∥）来实现。减小 r 可增加正则化强度并帮助减少过拟合。

```
MaxNormDense = partial(tf.keras.layers.Dense,
                       activation="relu", kernel_initializer="he_normal",
                       kernel_constraint=tf.keras.constraints.max_norm(1.))

tf.random.set_seed(42)
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    MaxNormDense(100),
    MaxNormDense(100),
    tf.keras.layers.Dense(10, activation="softmax")
])
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))
```

## **总结：**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-13-20-21-12-image.png" alt="" width="444" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-13-20-21-28-image.png" alt="" width="416" data-align="center">
