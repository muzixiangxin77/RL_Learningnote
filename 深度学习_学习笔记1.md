# **<mark>ANN with Keras:</mark>**

感知机：**Perceptron**

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-10-19-39-02-image.png" alt="" width="300" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-10-19-39-32-image.png" alt="" data-align="center" width="359">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-10-19-37-55-image.png" alt="" width="375" data-align="center">

```
事实上，Scikit_Learn中的感知机Perceptron类等价于SGDClassifier分类器
只需要将超参数改为loss='perceptron',learning_rate='constant',eta0=1,penalty=None
并且感知机的权重优化是线性的，专业点说就是模型学习的都是线性的性质，所以感知机无法处理复杂的图像
[sgd_clf = SGDClassifier(loss="perceptron", penalty=None,
                        learning_rate="constant", eta0=1, random_state=42)]
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris(as_frame=True)
X = iris.data[["petal length (cm)", "petal width (cm)"]].values
y = (iris.target == 0)  # Iris setosa

per_clf = Perceptron(random_state=42)
per_clf.fit(X, y)

X_new = [[2, 0.5], [3, 1]]
y_pred = per_clf.predict(X_new) 
```

感知机的缺点：1.没有概率函数，不会产出概率；2.没有正则化，并且一旦没有预测误差就会训练停止，泛化能力不强；3.一般用Logistic回归和线性SVM分类器解决

**MLP**：一个输入层，一个或多个隐藏层，一个输出层；即多层感知机

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-10-20-15-03-image.png" alt="" width="387" data-align="center">

**DNN**指的是就是深度神经网络，ANN包含很深的隐藏层，隐藏层数很多

**Backpropagation**:反向传播算法，本质上就是一个链式法则的应用，不断优化权重

**激活函数**：(这里需要引入非线性的目的，其实很简单，就是让模型复杂起来，因为如果激活函数是线性的，那么其实整个模型都是一个线性的函数，没法对复杂的问题求解了。)

<mark>1.heaviside(z)=0 if z<0 ;1 if z>=0</mark>

<mark>2.sgn(z)=-1 if z<0;0 if z=0; +1 if z>0</mark>

<mark>3.sigmoid(z)=1/(1+exp(-z))</mark>

<mark>4.tanh(z)=2sigmod(2z)-1</mark>

<mark>5.ReLU(z)=max(0,z)</mark>

<mark>6.softplus(z)=log(1+exp(z))</mark>

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-10-19-52-16-image.png" alt="" width="670" data-align="center">

**<mark>回归MLPs：</mark>**

```
from sklearn.datasets import fetch_california_housing
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegression
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

housing=fetch_california_housing()
X_train_full,X_test,y_train_full,y_test=train_test_split(houing.data,housing.traget,random=42)
X_train,X_valid,y_train,y_valid=train_test_split(X_train_full,y_train_full,random_state=42)

mlg_reg=MLPRegression(hidden_layer_sizes=[50,50,50],random_state=42)
pipeline=make_pipeline(StandardScaler(),mlg_reg)
pipeline.fit(X_train,y_train)
y_pred=pipeline.predict(X_valid)
rmse=mean_squared_error(y_valid,y_pred,squared=False)
```

请注意，此 MLP 在输出层未使用任何激活函数，因此它可以输出任意值。遗憾的是MLPRegressor类不支持输出层的激活函数，只支持隐藏层的激活函数（一般默认是ReLU）...

**<mark>分类MLPs:</mark>**

```
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier

iris = load_iris()
X_train_full, X_test, y_train_full, y_test = train_test_split(
    iris.data, iris.target, test_size=0.1, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_full, y_train_full, test_size=0.1, random_state=42)

mlp_clf = MLPClassifier(hidden_layer_sizes=[5], max_iter=10_000,
                        random_state=42)
pipeline = make_pipeline(StandardScaler(), mlp_clf)
pipeline.fit(X_train, y_train)
accuracy = pipeline.score(X_valid, y_valid)
```

二元分类问题时，只需用sigmoid激活函数的单个输出神经元：因为输出一个介于0-1的值，可将其解释为正类别的概率估计，负类别的估计概率等于1-该数值即可；

对于比如说多标签二元分类问题，判断是否是垃圾邮件，是否是紧急邮件，这就需要两个sigmoid函数，这样可以输出任何标签的组和；

但是对于每个实例只能属于三个或多个可能类别中的一个类别，虽然我们需要对每个类别分配一个输出神经元，但是最后需要用softmax()激活函数处理整个输出层，确保估计的概率在0-1之间，并且总和为1.因为类是互斥的。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-10-20-34-55-image.png" alt="" width="465" data-align="center">

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-10-20-35-26-image.png" alt="" width="327" data-align="center">

## I.静态模型构建：序列(Sequential)API，和功能(functional)API

**<mark>一个简单的用Keras的序列API(Sequential API)构建MLP解决图像分类问题的例子:</mark>**

（fashion_mnist）(train:60000,test:10000)

fashion的类标签为：class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
               "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

**一.数据集的准备**

```
import tensorflow as tf

fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist
X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]
X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]


X_train.shape #(55000,28,28)
X_train.dtype

#图像中每个像素是分布在0-255之间，为了简单处理，我们将其全部转换到0-1之间
X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255.

#跟经典手写集不同，这里的标签，也就是y_train[0]代表的数字，不是类，而是在class_names中的类
class_names[y_train[0]]  
```

**二.MLP模型的准备——构建模型**

```
tf.random.set_seed(42)
model=tf.keras.Sequential()

#输入层：
model.add(tf.keras.layers.Input(shape=[28,28]))

#添加一个Flatten层。其作用是将每个输入图像转换为1维数组：
例如，如果它接收形状为[32, 28,28]的批次，它将重塑为[32, 784]。换句话说，
如果它接收输入数据X，它将计算X.reshape(-1, 784)
model.add(tf.keras.layers.Flatten())

#每个 Dense 层管理自己的权重矩阵，包含所有神经元与其输入之间的连接权重。
它还管理一个偏置项向量（每个神经元一个）。
model.add(tf.keras.layers.Dense(300,activation='relu'))
model.add(tf.keras.layers.Dense(100,activation='relu'))

#我们添加一个具有 10 个神经元的 Dense 输出层（每个类一个），
使用 softmax 激活函数，因为类是互斥的。
model.add(tf.keras.layers.Dense(10,activation='softmax'))
```

当然可以换种简单的写法：

```
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(300, activation="relu"),
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(10, activation="softmax"
)])
```

**三.MLP模型的准备——模型参数及信息查看**

```
#模型的总结：
model.summary()

#获得model的每一层的信息
model.layers

#绘制模型建立的流程图：
tf.keras.utils.plot_model(model, "my_fashion_mnist_model.png", show_shapes=True)

#获得某一层，比如隐藏层的名称
hidden1=model.layers[1]
hidden1.name

#获得某一层的权重矩阵和偏置项
weights,biases=hidden1.get_weights()
weights
weights.shape
biases.shape
```

**四.MLP模型的准备——模型的编译：选择损失函数，优化器，评估指标：**

```
model.compile(loss="sparse_categorical_crossentropy",
              optimizer="sgd",
              metrics=["accuracy"]) 

上下等价
（下面可以调整学习率）
model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=[tf.keras.metrics.sparse_categorical_accuracy])
```

**解释一下上述的损失函数：**

**sparse_categorical_crossentropy**，是因为我们有稀疏标签，且每类是互斥的。

例如 (3个类别)：类别0 -> `0`；类别1 -> `1`；类别2 -> `2`

如果每个实例对应每个类有一个目标概率（如ont-hot向量[0,1,0]代表类别2），我们使用**categorical_crossentropy损失函数**,要求标签格式为one-hot向量形式

例如 (3个类别)：类别0 -> `[1, 0, 0]`；类别1 -> `[0, 1, 0]`；类别2 -> `[0, 0, 1]`。

如果我们进行**二元分类或多标签二元分类，和在上面举过的例子一样，使用sigmoid激活函数**，而不是softmax函数，并且**损失函数用"binary_crossentropy"损失函数**

如果想将稀疏标签（类索引，对应第一种）转换为one-hot向量标签，可以使用tf.keras.utils.to_categorical()函数，反之可以用np.argmax()函数并设置axis=1

**解释一下优化器的使用**：

在使用 SGD 优化器时，调整学习率非常重要。因此，你通常希望使用optimizer=tf.keras.
optimizers.SGD(learning_rate=__???__) 来设置学习率，而不是optimizer="sgd"，后者默认学习率为 0.01。

**五.MLP模型的训练及评估**

由于训练集包含 55,000 张图像，因此模型每个 epoch 处理 1,719 个批次：
1,718 个批次大小为 32，1 个批次大小为 24。进度条之后，你可以看到
每个样本的平均训练时间，以及训练集和验证集上的损失和准确率，训练集上损失率及准确率:loss:0.2235,sparse_categorical:0.92;验证集上损失率及准确率：val_loss:0.3056;val_sparse_categorical:0.8894说明存在一定程度上的过拟合现象。

![](C:\Users\Lenovo\AppData\Roaming\marktext\images\2025-07-11-19-18-15-image.png)

```
history = model.fit(X_train, y_train, epochs=30,
                    validation_data=(X_valid, y_valid)) 

#这里还可以传递验证集的比例参数validation_split=0.1/0.2/0.3

#如果出现训练集分布不均匀情况，那么在调用fit()时,
#方法一：设置class_weight：给欠代表（样本少）的类别更高的权重，给过代表（样本多）的类别更低的权重
from sklearn.utils import class_weight
class_weights = class_weight.compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train
)
#方法二：设置样本权重sample_weight：为单个样本设置权重，比class_weight更精细
# 假设 X_train 有 1000 个样本
sample_weights = np.ones(1000)  # 默认所有样本权重为1

# 假设前100个样本是专家标注的，给予更高权重 (e.g., 3.0)
sample_weights[:100] = 3.0

# 假设索引500-600的样本来自低质量来源，给予较低权重 (e.g., 0.7)
sample_weights[500:600] = 0.7

#方法三：同时传入class_weight,sample_weight，该样本最终权重为两者乘积


history.params
print(history.epoch)


import matplotlib.pyplot as plt
import pandas as pd

pd.DataFrame(history.history).plot(
    figsize=(8, 5), xlim=[0, 29], ylim=[0, 1], grid=True, xlabel="Epoch",
    style=["r--", "r--.", "b-", "b-*"])
plt.legend(loc="lower left")  # extra code

plt.show()
```

```
model.evaluate(X_test, y_test)
```

如果你对模型的性能不满意，你应该回去调整超参数。首先要检查的是学习率。

如果这没有帮助，尝试使用另一个优化器（并且在更改任何超参数后始终重新调整学习率）。

如果性能仍然不理想，则尝试调整模型超参数，如层数、每层神经元数量以及各隐藏层使用的激活函数类型。

你还可以尝试调整其他超参数，如批量大小（可在fit()方法中通过batch_size参数设置，默认值为32）。

**六.MLP模型的分类预测**

```
X_new = X_test[:3]
y_proba = model.predict(X_new)
y_proba.round(2)  

y_pred = y_proba.argmax(axis=-1)
y_pred

np.array(class_names)[y_pred]


y_new = y_test[:3]
y_new
```

**<mark>一个简单的用Keras的序列API(Sequential API)构建MLP解决预测回归问题的例子:</mark>**

假设你的训练集 `X_train` 是：

- 表格数据：1000个样本，每个样本有20个特征
  
  - `X_train.shape = (1000, 20)`
  
  - `X_train.shape[1:] = (20,)` → **特征维度**

- 图像数据：5000张28x28像素的灰度图
  
  - `X_train.shape = (5000, 28, 28)`
  
  - `X_train.shape[1:] = (28, 28)` → **图像高度和宽度**

- 彩色图像：3000张64x64像素的RGB图
  
  - `X_train.shape = (3000, 64, 64, 3)`
  
  - `X_train.shape[1:] = (64, 64, 3)` → **高度、宽度和通道数**

```
tf.random.set_seed(42)

#对特征标准化：
norm_layer=tf.keras.layers.Normalization(input_shape=X_train.shape[1:])

#构造模型
model=tf.keras.Sequential([
   norm_layer,
   tf.keras.layers.Dense(50,activation="relu"),
   tf.keras.layers.Dense(50,activation="relu"),
   tf.keras.layers.Dense(50,activation="relu"),
   tf.keras.layer.Dense(1)
])

#优化器选择：
optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(loss="mse",optimizer=optimizer,metrics=["RootMeanSquaredError"])

# 使标准化层适配训练数据：计算并存储特征的均值和标准差
norm_layer.adapt(X_train)

history=model.fit(X_train,y_train,epochs=30,validation_data=(X_valid,y_valid))

mse_test,rmse_test=model.evaluate(X_test,y_test)


X_new=X_test[:3]
y_pred=model.predict(X_new)
```

## 为什么选择 Adam 而不是其他优化器？

| 优化器               | 优点               | 缺点           | 适用场景            |
| ----------------- | ---------------- | ------------ | --------------- |
| **Adam**          | 自适应学习率，快速收敛，鲁棒性好 | 超参数调整空间较小    | **通用首选**，尤其适合新手 |
| SGD (随机梯度下降)      | 理论保证，简单          | 收敛慢，需手动调整学习率 | 需要精细调优时         |
| SGD with Momentum | 减少震荡，加速收敛        | 需调整动量参数      | 图像分类等任务         |
| RMSprop           | 自适应学习率           | 没有动量项        | RNN/LSTM 网络     |
| Adagrad           | 自适应学习率           | 学习率衰减太快      | 稀疏数据            |

### 与分类模型的关键区别

| 方面            | 回归模型 (当前代码)         | 分类模型                             |
| ------------- | ------------------- | -------------------------------- |
| **输出层**       | 1个神经元，无激活函数         | 神经元数=类别数，使用softmax/sigmoid激活     |
| **损失函数**      | 均方误差(MSE)           | 交叉熵损失(categorical_crossentropy等) |
| **评估指标**      | RMSE, MAE, R²等连续值指标 | 准确率(accuracy), F1-score, AUC等    |
| **输出值**       | 连续数值(如房价、温度)        | 类别概率或类别标签                        |
| **激活函数(输出层)** | 通常无(线性激活)           | softmax(多分类)或sigmoid(二分类)        |
| **最后一层示例**    | `Dense(1)`          | `Dense(3, activation='softmax')` |
| **预测结果**      | 标量值(如12.45, 0.87)   | 概率向量(如[0.1, 0.8, 0.1])或类别索引      |
| **常见问题类型**    | 房价预测、销量预测、温度预测      | 图像分类、情感分析、垃圾邮件检测                 |
| **数据标准化**     | 更重要，对回归性能影响大        | 重要但影响相对较小                        |
| **模型复杂度**     | 通常需要更多层/神经元捕捉连续关系   | 可能结构更简单                          |

**<mark>使用功能(Function API)API的Keras构建复杂模型</mark>**

和上面的序列API不同，上面的序列API只能按照固定流程，按顺序进行网络处理，只能通过深层路径，没有简单模式。

**注：**

**concat层:用于特征融合（合并），将不同分支的特征进行合并，但是保留原始信息**

**宽度路径：处理简单的特征交互，擅长“记忆”，保持模型的稀疏性和可解释性**

**深度路径：处理复杂非线性关系，擅长“泛化”，捕捉高阶特征交互**

非顺序神经网络：宽深神经网络(Wide&Deep)：它将所有或部分输入直接连接到输出层，这种架构使神经网络能够学习深层模式 （通过深层路径）和简单规则（通过短路径）。相比之下，常规的 MLP强制所有数据流经完整的层堆栈；因此，数据中的简单模式可能因这一系列变换而扭曲。

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-11-20-19-24-image.png" alt="" width="222" data-align="center">

```
#构建每一层
normalization_layer=tf.keras.layers.Normalization()
hidden_layer1=tf.keras.layers.Dense(30,activation="relu")
hidden_layer2=tf.keras.layers.Dense(30,activation="relu")
concat_layer=tf.keras.layers.Concatenate()
output_layer=tf.keras.layers.Dense(1)

#输入层及隐藏层，以及短路径的实现
input_=tf.keras.layers.Input(shape=X_train.shape[1:])
normalized=normalization_layer(input_)
hidden1=hidden_layer1(normalized)
hidden2=hidden_layer2(hidden1)
concat=concat_layer([normalized,hidden2])

#输出层
output=output_layer(concat)


model=tf.keras.Model(inputs=[input_],outputs=[output])
```

<img title="" src="file:///C:/Users/Lenovo/AppData/Roaming/marktext/images/2025-07-11-20-29-43-image.png" alt="" width="284" data-align="center">

```
input_wide=tf.keras.layers.Input(shape[5]) #feature 0 to 4
input_deep=tf.keras.layers.Input(shape[6]) #feature 2 to 7

norm_layer_wide=tf.keras.layers.Normalization()
norm_layer_deep=tf.keras.layers.Normalization()

norm_wide=norm_layer_wide(input_wide)
norm_deep=norm_layer_deep(input_deep)
hidden1=tf.keras.layers.Dense(30,activation="relu")(norm_deep)
hidden2=tf.keras.layers.Dense(30,activation="relu")(hidden1)

concat=tf.keras.layers.concatenate([norm_wide,hidden2])

output=tf.keras.layers.Dense(1)(concat)

model=tf.keras.Model(inputs=[input_wide,input_deep],outputs=[output])
```

```
optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(loss="mse",optimizer=optimizer,metrics=["RootMeanSquaredError"])


X_train_wide,X_train_deep=X_train[:,:5],X_train[:,2:]
X_valid_wide,X_valid_deep=X_valid[:,:5],X_train[:,2:]
X_test_wide,X_test_deep=X_test[:,:5],X_test[:,2:]


norm_layer_wide.adapt(X_train_wide)
norm_layer_deep.adapt(X_train_deep)


history=model.fit((X_train_wide,X_train_deep),y_train,epochs=20,
                  validation_data=((X_valid_wide,X_valid_deep),y_valid))


mse_test=model.evaluate((X_test_wide,X_test_deep),y_test)
```

当我们面对**多任务问题：**

1.一个问题既需要回归，又要分类；

2.基于一个数据集有多个独立任务；

3.正则化技术（减少过拟合提升模型泛化能力）

那么可以参考如下模板：

```
# 保持原有特征处理路径
concat = tf.keras.layers.concatenate([norm_wide, hidden2])

# 添加两个输出层：
# 1. 回归输出（连续值）
reg_output = tf.keras.layers.Dense(1, name="regression")(concat)

# 2. 分类输出（假设二分类）
cls_output = tf.keras.layers.Dense(1, activation="sigmoid", name="classification")(concat)

# 创建多输出模型
model = tf.keras.Model(
    inputs=[input_wide, input_deep],
    outputs=[reg_output, cls_output]  # 两个输出
)  


model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss={
        "regression": "mse",           # 回归任务损失
        "classification": "binary_crossentropy"  # 分类任务损失
    },
    metrics={
        "regression": ["RootMeanSquaredError"],
        "classification": ["accuracy"]  # 分类准确率
    },
    loss_weights=[0.7, 0.3]  # 可选：调整任务重要性权重
) 


# 需要为两个任务提供标签
y_train_reg = ...  # 回归标签 (连续值)
y_train_cls = ...  # 分类标签 (0/1)

#拟合时
history = model.fit(
    # 双输入数据
    x=[X_train_wide, X_train_deep],

    # 双标签数据（字典形式）
    y={
        "regression": y_train_reg,      # 回归标签（连续值）
        "classification": y_train_cls   # 分类标签（0/1或one-hot）
    },

    epochs=20,
    batch_size=32,

    # 验证数据同样需要双输入+双标签
    validation_data=(
        [X_valid_wide, X_valid_deep],
        {"regression": y_valid_reg, "classification": y_valid_cls}
    )
)

# 评估时：
results = model.evaluate(
    (X_test_wide, X_test_deep),
    {"regression": y_test_reg, "classification": y_test_cls}  # 双标签
) 

#输出结果：总损失，回归损失，分类损失，回归RMSE，分类准确率
```

## II.动态模型构建使用子类API(subclassing API)

```
import tensorflow as tf

class WideAndDeepModel(tf.keras.Model):
    def __init__(self,units=30,activation="relu",**kwargs):
        super().__init__(**kwargs)
        self.norm_layer_wide=tf.keras.layers.Normalization()
        self.norm_layer_deep=tf.keras.layers.Normalization()
        self.hidden1=tf.keras.layers.Dense(units,activation=activation)
        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)
        self.main_output=tf.keras.layers.Dense(1)
        self.aux_output=tf.keras.layers.Dense(1)

    def call(self,inputs):
        input_wide,input_deep=inputs
        norm_wide=self.norm_layer_wide(input_wide)
        norm_deep=self.norm_layer_deep(input_deep)
        hidden1=self.hidden1(norm_deep)
        hidden2=self.hidden2(hidden1)
        concat=tf.keras.layers.concatenate([norm_wide,hidden2])
        output=self.main_output(concat)
        aux_output=self.aux_output(concat)#这里也可以不用concat层，可以用hidden中的层都可以，看需求
        return output,aux_output 

model=WideAndDeepModel(30,activation="relu",name="Wake")
```

## III.微调神经网络的超参数

超参数需要考虑的是：

1.神经网络层数

2.每层需要的神经元个数

3.每层所用的激活函数的种类

4.初始权重

5.优化器的选择

6.学习率

...

**微调隐藏层数量，神经元个数，优化器种类可以通过如下操作结合随机搜索(Randomsearch)得到最佳参数**：

```
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist
X_train, y_train = X_train_full[:-5000], y_train_full[:-5000] 
X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:] 


import keras_tuner as kt

#关键调优点：微调隐藏层数量，神经元个数，优化器种类
def build_model(hp):
    n_hidden = hp.Int("n_hidden", min_value=0, max_value=8, default=2)

    n_neurons = hp.Int("n_neurons", min_value=16, max_value=256)

    learning_rate = hp.Float("learning_rate", min_value=1e-4, max_value=1e-2,
                             sampling="log")

    optimizer = hp.Choice("optimizer", values=["sgd", "adam"])

    if optimizer == "sgd":
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
    else:
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)


    model = tf.keras.Sequential()

    model.add(tf.keras.layers.Flatten())

    for _ in range(n_hidden):
        model.add(tf.keras.layers.Dense(n_neurons, activation="relu"))

    model.add(tf.keras.layers.Dense(10, activation="softmax"))

    model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
                  metrics=["accuracy"])

    return model

#目标是得到最大的验证集准确率val_accuracy:
random_search_tuner = kt.RandomSearch(
    build_model, objective="val_accuracy", max_trials=5, overwrite=True,
    directory="my_fashion_mnist", project_name="my_rnd_search", seed=42) 

random_search_tuner.search(X_train, y_train, epochs=10,
                           validation_data=(X_valid, y_valid)) 


#得到最佳的3个模型：
top3_model=random_search_tuner.get_best_models(num_models=3)
best_model=top3_model[0]

#得到最佳三个模型的超参数：
top3_params=random_search_tuner.get_best_hyperparameters(num_trials=3)
top3_params[0].values#最佳超参数


#得到最佳实验：
best_trial=random_search_tuner.oracle.get_best_trials(num_trials=1)[0]
best_trial.summary()


test_loss,test_accuracy=best_model.evaluate(X_test,y_test)
```

 如果我们需要调整数据预处理的参数，如是否规范化，以及批量处理大小等超参数，则需要通过如下用类表示：(**HalvingRandomsearch类似，优点是速度快，但是精度低**)

它首先为多个模型进行少量 epoch 的训练，然后淘汰表现最差的模型，仅保留排名前 1 / factor 的模型（即本例中的前 1/3），并重复此筛选过程直至只剩下一个模型。max_epochs参数控制最佳模型将被训练的最大 epoch 数。整个过程在此情况下重复两次（hyperband_iterations=2）。所有模型在每次超带迭代中的总训练 epoch 数约为max_epochs * (log(max_epochs) / log(factor)) ** 2，因此本例中约为44 个 epoch。

```
class MyClassificationHyperModel(kt.HyperModel):
      def build(self,hp):
          return build_model(hp)
      def fit(self,hp,model,X,y,**kwargs):
          if hp.Boolean("normalize"):
             norm_layer=tf.keras.layers.Normalization()
             X=norm_layer(X)
          return model.fit(X,y,**kwargs)

hyperband_tuner=kt.Hyperband(MyClassificationHyperModel(),objective="val_accuracy",
                           seed=42,max_epochs=10,factor=3,hyperband_iterations=2,
                           overwrite=True,directory="my_fashion_mnist",project_name="hyperhand")
```

补充：**kt还提供了高斯贝叶斯优化方法寻找超参数**：

```
bayesian_opt_tuner = kt.BayesianOptimization(
    MyClassificationHyperModel(), objective="val_accuracy", seed=42,
    max_trials=10, alpha=1e-4, beta=2.6,
    overwrite=True, directory="my_fashion_mnist", project_name="bayesian_opt")
bayesian_opt_tuner.search(X_train, y_train, epochs=10,
                          validation_data=(X_valid, y_valid),
                          callbacks=[early_stopping_cb])
```

**关于优化隐藏层的数量和每层神经元数量的建议：**

1.隐藏层数量

简单问题：1-2个隐藏层

中等复杂问题：逐步增加层数直到开始过拟合

高复杂度任务：数十层，优先使用预训练模型（ResNet,BERT)(这里没懂...)

2.每层神经元数量：

渐进增加法：从较小网络开始（128，256），逐步增加神经元直到验证集性能下降

弹性裤策略：构建需求稍大的网络，使用正则化技术控制过拟合

```
model = Sequential([
    Dense(512, kernel_regularizer=l2(0.01)),
    Dropout(0.5),
    Dense(512, kernel_regularizer=l2(0.01)),
    Dropout(0.5)
])  
```

3.实践建议：

<img title="" src="file:///C:/Users/Lenovo/Downloads/deepseek_mermaid_20250712_7b2337.png" alt="" width="211" data-align="center">

**关于学习率，批量大小，优化器，批量大小，激活函数的建议：**

1.学习率：以非常低的学习率开始训练模型，并逐步将其增加到非常大的值，进行几百次迭代；一般来说，最优学习率约为最大学习率的一半，这个后续学习再讨论

2.优化器：之前上面的表格提供了相关建议

3.批量大小：尝试使用大批量大小并结合学习率预热（预热即以较小的学习率开始训练，然后逐步提高）；若训练不稳定或性能不好，则改用小批量大小

4.迭代次数：在多数情况下，训练迭代次数无需调整，只需用**早期停止策略**即可

```
checkpoint_cb=tf.keras.callbacks.ModelCheckpoint("my_checkpoints",
                                             save_best_only=True)
early_stopping_cb=tf.keras.callbacks.EarlyStopping(patience=10,
                                      restore_best_weights=True)

history=model.fit([...],callbacks=[checkpoint_cb,early_stopping_cb])
```

5.激活函数：一般来说，ReLU激活函数是所有隐藏层的良好默认选择，但对于输出层，具体取决于任务

**注：最优学习率取决于其他超参数——尤其是批量大小——因此，如果修改了任何超参数，请确保同时更新学习率。**
